{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Training.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Install the package"
   ],
   "metadata": {
    "id": "8mFcCCzayMTs"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pNLJK0Zmk853",
    "outputId": "99d9e556-9eac-4cf3-c1f4-806fbaa1d2ae"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting git+https://github.com/dolphin-in-a-coma/multi-task-cnn-eeg-emotion.git\n",
      "  Cloning https://github.com/dolphin-in-a-coma/multi-task-cnn-eeg-emotion.git to /tmp/pip-req-build-ei3lnpd4\n",
      "  Running command git clone -q https://github.com/dolphin-in-a-coma/multi-task-cnn-eeg-emotion.git /tmp/pip-req-build-ei3lnpd4\n",
      "Collecting setuptools~=52.0.0\n",
      "  Downloading setuptools-52.0.0-py3-none-any.whl (784 kB)\n",
      "\u001B[K     |████████████████████████████████| 784 kB 8.2 MB/s \n",
      "\u001B[?25hRequirement already satisfied: keras~=2.7.0 in /usr/local/lib/python3.7/dist-packages (from eegemotion==1.0) (2.7.0)\n",
      "Collecting numpy~=1.20.1\n",
      "  Downloading numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)\n",
      "\u001B[K     |████████████████████████████████| 15.3 MB 89.1 MB/s \n",
      "\u001B[?25hRequirement already satisfied: tensorflow~=2.7.0 in /usr/local/lib/python3.7/dist-packages (from eegemotion==1.0) (2.7.0)\n",
      "Collecting scikit-learn~=0.24.1\n",
      "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "\u001B[K     |████████████████████████████████| 22.3 MB 129.2 MB/s \n",
      "\u001B[?25hCollecting scipy~=1.6.2\n",
      "  Downloading scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
      "\u001B[K     |████████████████████████████████| 27.4 MB 56.8 MB/s \n",
      "\u001B[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn~=0.24.1->eegemotion==1.0) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn~=0.24.1->eegemotion==1.0) (3.0.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (0.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (3.17.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (3.1.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (12.0.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (2.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (1.42.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (2.7.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (1.15.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (0.22.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (3.10.0.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (1.13.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->eegemotion==1.0) (0.37.0)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.7.0->eegemotion==1.0) (1.5.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (3.3.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (1.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (1.35.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.7.0->eegemotion==1.0) (3.1.1)\n",
      "Building wheels for collected packages: eegemotion\n",
      "  Building wheel for eegemotion (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for eegemotion: filename=eegemotion-1.0-py3-none-any.whl size=8562 sha256=3f23afbeed66d67c2e49efd372fb7577fe90b60c67a537c73d985ddd2729a0ec\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ruwbq479/wheels/4b/55/5a/451c23785b0102aa988bf281121575019f779f155b12f77bec\n",
      "Successfully built eegemotion\n",
      "Installing collected packages: setuptools, numpy, scipy, scikit-learn, eegemotion\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 57.4.0\n",
      "    Uninstalling setuptools-57.4.0:\n",
      "      Successfully uninstalled setuptools-57.4.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.1\n",
      "    Uninstalling scikit-learn-1.0.1:\n",
      "      Successfully uninstalled scikit-learn-1.0.1\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.20.3 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001B[0m\n",
      "Successfully installed eegemotion-1.0 numpy-1.20.3 scikit-learn-0.24.2 scipy-1.6.3 setuptools-52.0.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy",
         "pkg_resources"
        ]
       }
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "!pip install git+https://github.com/dolphin-in-a-coma/multi-task-cnn-eeg-emotion.git"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connect to Google Drive\n",
    "Needed only if you work with Colab \n"
   ],
   "metadata": {
    "id": "b30VgF5NuzND"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys\n",
    "from google.colab import drive\n",
    "\n",
    "DRIVE_DATA_PATH = 'data/'\n",
    "\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "data_path = '/content/data'\n",
    "os.symlink(os.path.join('/content/drive/My Drive/', DRIVE_DATA_PATH), data_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_HhCPHQurTg",
    "outputId": "0bf71dbf-df4f-4135-c25d-5330abe9ecbe"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Specify arguments"
   ],
   "metadata": {
    "id": "VTb4zGkyybQz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# specify dataset and model dirs\n",
    "dataset_dir = data_path # path of the folder with PSD_s and DE_s files\n",
    "model_dir = data_path # path where the model and metrics will be stored\n",
    "metrics_dir = data_path # path where the model and metrics will be stored"
   ],
   "metadata": {
    "id": "1_QlO1yA1CzO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from eegemotion.data_load import load_data \n",
    "from eegemotion.train import train\n",
    "from eegemotion.utils import print_results\n",
    "\n",
    "img_size = img_rows, img_cols, num_chan = 8, 9, 8 # matrix shape of input data\n",
    "number_of_inputs = 1 # how many frames is taken into account during one pass\n",
    "\n",
    "features_type = 'multi' # 'PSD', 'DE' or 'multi' be carefull with num_chan\n",
    "num_classes = 2 # number of classes of input data\n",
    "frames_per_subject = 4800 # how many frames per one subject\n",
    "seed = 7 # random seed\n",
    "\n",
    "dropout_rate = .2\n",
    "model_name = 'MT_CNN' # will be a filename part\n",
    "lr_decay_factor = 0.5 # multiplixity factor of lr, where it's stucked in th plateu\n",
    "lr_decay_patience = 5 # how many epochs without prgress before lr_decay\n",
    "epochs_n = 200 # maximum number of epochs\n",
    "verbose = 0 # 0, 1 or 2\n",
    "\n",
    "subject_n = 32\n",
    "\n",
    "task = 'multi' # 'valence', 'arousal' or 'multi'\n",
    "fine_tuning = True # fine tune to all subjects specifically"
   ],
   "metadata": {
    "id": "gRrTuDkGlEqn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data and prepare for training"
   ],
   "metadata": {
    "id": "LgJnxjRV31Cu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "y_a_all_subject, y_v_all_subject, x_all_subject, all_subject_id =\\\n",
    "    load_data(dataset_dir, subject_n, img_size, number_of_inputs, features_type, num_classes, frames_per_subject, seed)"
   ],
   "metadata": {
    "id": "Q8YUZtXiqCvR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d51de25e-da47-4390-8b44-292e9788f5dd"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "processing:  01 ......\n",
      "\n",
      "processing:  02 ......\n",
      "\n",
      "processing:  03 ......\n",
      "\n",
      "processing:  04 ......\n",
      "\n",
      "processing:  05 ......\n",
      "\n",
      "processing:  06 ......\n",
      "\n",
      "processing:  07 ......\n",
      "\n",
      "processing:  08 ......\n",
      "\n",
      "processing:  09 ......\n",
      "\n",
      "processing:  10 ......\n",
      "\n",
      "processing:  11 ......\n",
      "\n",
      "processing:  12 ......\n",
      "\n",
      "processing:  13 ......\n",
      "\n",
      "processing:  14 ......\n",
      "\n",
      "processing:  15 ......\n",
      "\n",
      "processing:  16 ......\n",
      "\n",
      "processing:  17 ......\n",
      "\n",
      "processing:  18 ......\n",
      "\n",
      "processing:  19 ......\n",
      "\n",
      "processing:  20 ......\n",
      "\n",
      "processing:  21 ......\n",
      "\n",
      "processing:  22 ......\n",
      "\n",
      "processing:  23 ......\n",
      "\n",
      "processing:  24 ......\n",
      "\n",
      "processing:  25 ......\n",
      "\n",
      "processing:  26 ......\n",
      "\n",
      "processing:  27 ......\n",
      "\n",
      "processing:  28 ......\n",
      "\n",
      "processing:  29 ......\n",
      "\n",
      "processing:  30 ......\n",
      "\n",
      "processing:  31 ......\n",
      "\n",
      "processing:  32 ......\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the model"
   ],
   "metadata": {
    "id": "aq0xC7NO36gA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train(x_all_subject, y_a_all_subject, y_v_all_subject, all_subject_id, subject_n,\n",
    "      dropout_rate, number_of_inputs, model_dir, metrics_dir, model_name, img_size,\n",
    "      lr_decay_factor, lr_decay_patience, epochs_n, seed, verbose, task, fine_tuning)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLQg9rTmtseU",
    "outputId": "4a3edb38-0168-4db7-8a98-7a73a25ebbdc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 1.3131 - out_v_loss: 0.6719 - out_a_loss: 0.6412 - out_v_accuracy: 0.6314 - out_a_accuracy: 0.6569"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "320/320 [==============================] - 23s 38ms/step - loss: 1.3127 - out_v_loss: 0.6717 - out_a_loss: 0.6410 - out_v_accuracy: 0.6315 - out_a_accuracy: 0.6570 - val_loss: 1.1833 - val_out_v_loss: 0.5975 - val_out_a_loss: 0.5857 - val_out_v_accuracy: 0.6967 - val_out_a_accuracy: 0.7074 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 1.0257 - out_v_loss: 0.5161 - out_a_loss: 0.5097 - out_v_accuracy: 0.7396 - out_a_accuracy: 0.7497 - val_loss: 0.8706 - val_out_v_loss: 0.4298 - val_out_a_loss: 0.4408 - val_out_v_accuracy: 0.7932 - val_out_a_accuracy: 0.7889 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.8268 - out_v_loss: 0.4146 - out_a_loss: 0.4122 - out_v_accuracy: 0.8009 - out_a_accuracy: 0.8058 - val_loss: 0.7557 - val_out_v_loss: 0.3824 - val_out_a_loss: 0.3732 - val_out_v_accuracy: 0.8189 - val_out_a_accuracy: 0.8266 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.6796 - out_v_loss: 0.3439 - out_a_loss: 0.3357 - out_v_accuracy: 0.8437 - out_a_accuracy: 0.8481 - val_loss: 0.6164 - val_out_v_loss: 0.3144 - val_out_a_loss: 0.3020 - val_out_v_accuracy: 0.8555 - val_out_a_accuracy: 0.8703 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.5818 - out_v_loss: 0.2872 - out_a_loss: 0.2947 - out_v_accuracy: 0.8727 - out_a_accuracy: 0.8696 - val_loss: 0.5803 - val_out_v_loss: 0.2881 - val_out_a_loss: 0.2922 - val_out_v_accuracy: 0.8680 - val_out_a_accuracy: 0.8725 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.5003 - out_v_loss: 0.2519 - out_a_loss: 0.2484 - out_v_accuracy: 0.8882 - out_a_accuracy: 0.8902 - val_loss: 0.4773 - val_out_v_loss: 0.2348 - val_out_a_loss: 0.2424 - val_out_v_accuracy: 0.8945 - val_out_a_accuracy: 0.8973 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.4418 - out_v_loss: 0.2222 - out_a_loss: 0.2196 - out_v_accuracy: 0.9048 - out_a_accuracy: 0.9043 - val_loss: 0.4611 - val_out_v_loss: 0.2362 - val_out_a_loss: 0.2249 - val_out_v_accuracy: 0.8951 - val_out_a_accuracy: 0.9033 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3997 - out_v_loss: 0.2041 - out_a_loss: 0.1956 - out_v_accuracy: 0.9137 - out_a_accuracy: 0.9163 - val_loss: 0.4228 - val_out_v_loss: 0.2129 - val_out_a_loss: 0.2098 - val_out_v_accuracy: 0.9070 - val_out_a_accuracy: 0.9078 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3618 - out_v_loss: 0.1814 - out_a_loss: 0.1803 - out_v_accuracy: 0.9241 - out_a_accuracy: 0.9233 - val_loss: 0.4024 - val_out_v_loss: 0.2063 - val_out_a_loss: 0.1961 - val_out_v_accuracy: 0.9150 - val_out_a_accuracy: 0.9199 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3383 - out_v_loss: 0.1737 - out_a_loss: 0.1646 - out_v_accuracy: 0.9258 - out_a_accuracy: 0.9302 - val_loss: 0.3698 - val_out_v_loss: 0.1896 - val_out_a_loss: 0.1802 - val_out_v_accuracy: 0.9186 - val_out_a_accuracy: 0.9246 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3059 - out_v_loss: 0.1547 - out_a_loss: 0.1513 - out_v_accuracy: 0.9350 - out_a_accuracy: 0.9377 - val_loss: 0.3238 - val_out_v_loss: 0.1561 - val_out_a_loss: 0.1677 - val_out_v_accuracy: 0.9404 - val_out_a_accuracy: 0.9295 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2936 - out_v_loss: 0.1447 - out_a_loss: 0.1489 - out_v_accuracy: 0.9391 - out_a_accuracy: 0.9389 - val_loss: 0.3390 - val_out_v_loss: 0.1595 - val_out_a_loss: 0.1795 - val_out_v_accuracy: 0.9357 - val_out_a_accuracy: 0.9248 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2750 - out_v_loss: 0.1407 - out_a_loss: 0.1344 - out_v_accuracy: 0.9416 - out_a_accuracy: 0.9432 - val_loss: 0.3373 - val_out_v_loss: 0.1760 - val_out_a_loss: 0.1613 - val_out_v_accuracy: 0.9350 - val_out_a_accuracy: 0.9309 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2619 - out_v_loss: 0.1321 - out_a_loss: 0.1298 - out_v_accuracy: 0.9448 - out_a_accuracy: 0.9451 - val_loss: 0.3018 - val_out_v_loss: 0.1384 - val_out_a_loss: 0.1634 - val_out_v_accuracy: 0.9447 - val_out_a_accuracy: 0.9324 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2339 - out_v_loss: 0.1141 - out_a_loss: 0.1197 - out_v_accuracy: 0.9535 - out_a_accuracy: 0.9500 - val_loss: 0.2739 - val_out_v_loss: 0.1375 - val_out_a_loss: 0.1363 - val_out_v_accuracy: 0.9469 - val_out_a_accuracy: 0.9445 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2296 - out_v_loss: 0.1166 - out_a_loss: 0.1131 - out_v_accuracy: 0.9531 - out_a_accuracy: 0.9547 - val_loss: 0.3099 - val_out_v_loss: 0.1548 - val_out_a_loss: 0.1551 - val_out_v_accuracy: 0.9381 - val_out_a_accuracy: 0.9400 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2189 - out_v_loss: 0.1094 - out_a_loss: 0.1096 - out_v_accuracy: 0.9542 - out_a_accuracy: 0.9546 - val_loss: 0.3130 - val_out_v_loss: 0.1591 - val_out_a_loss: 0.1539 - val_out_v_accuracy: 0.9354 - val_out_a_accuracy: 0.9379 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2082 - out_v_loss: 0.1034 - out_a_loss: 0.1049 - out_v_accuracy: 0.9587 - out_a_accuracy: 0.9586 - val_loss: 0.2833 - val_out_v_loss: 0.1289 - val_out_a_loss: 0.1544 - val_out_v_accuracy: 0.9514 - val_out_a_accuracy: 0.9412 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2094 - out_v_loss: 0.1039 - out_a_loss: 0.1056 - out_v_accuracy: 0.9578 - out_a_accuracy: 0.9559 - val_loss: 0.2827 - val_out_v_loss: 0.1406 - val_out_a_loss: 0.1422 - val_out_v_accuracy: 0.9457 - val_out_a_accuracy: 0.9451 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1964 - out_v_loss: 0.0972 - out_a_loss: 0.0992 - out_v_accuracy: 0.9613 - out_a_accuracy: 0.9596 - val_loss: 0.2665 - val_out_v_loss: 0.1323 - val_out_a_loss: 0.1342 - val_out_v_accuracy: 0.9500 - val_out_a_accuracy: 0.9508 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1872 - out_v_loss: 0.0953 - out_a_loss: 0.0919 - out_v_accuracy: 0.9620 - out_a_accuracy: 0.9635 - val_loss: 0.2767 - val_out_v_loss: 0.1331 - val_out_a_loss: 0.1437 - val_out_v_accuracy: 0.9492 - val_out_a_accuracy: 0.9461 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1844 - out_v_loss: 0.0919 - out_a_loss: 0.0925 - out_v_accuracy: 0.9631 - out_a_accuracy: 0.9620 - val_loss: 0.2444 - val_out_v_loss: 0.1177 - val_out_a_loss: 0.1267 - val_out_v_accuracy: 0.9555 - val_out_a_accuracy: 0.9531 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1677 - out_v_loss: 0.0855 - out_a_loss: 0.0822 - out_v_accuracy: 0.9660 - out_a_accuracy: 0.9671 - val_loss: 0.2552 - val_out_v_loss: 0.1259 - val_out_a_loss: 0.1293 - val_out_v_accuracy: 0.9514 - val_out_a_accuracy: 0.9488 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1690 - out_v_loss: 0.0806 - out_a_loss: 0.0884 - out_v_accuracy: 0.9676 - out_a_accuracy: 0.9653 - val_loss: 0.2409 - val_out_v_loss: 0.1141 - val_out_a_loss: 0.1268 - val_out_v_accuracy: 0.9586 - val_out_a_accuracy: 0.9502 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1600 - out_v_loss: 0.0799 - out_a_loss: 0.0802 - out_v_accuracy: 0.9676 - out_a_accuracy: 0.9681 - val_loss: 0.2577 - val_out_v_loss: 0.1185 - val_out_a_loss: 0.1393 - val_out_v_accuracy: 0.9564 - val_out_a_accuracy: 0.9492 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1597 - out_v_loss: 0.0802 - out_a_loss: 0.0794 - out_v_accuracy: 0.9673 - out_a_accuracy: 0.9676 - val_loss: 0.2495 - val_out_v_loss: 0.1169 - val_out_a_loss: 0.1326 - val_out_v_accuracy: 0.9559 - val_out_a_accuracy: 0.9496 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1588 - out_v_loss: 0.0773 - out_a_loss: 0.0815 - out_v_accuracy: 0.9691 - out_a_accuracy: 0.9668 - val_loss: 0.2253 - val_out_v_loss: 0.1049 - val_out_a_loss: 0.1204 - val_out_v_accuracy: 0.9572 - val_out_a_accuracy: 0.9547 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1489 - out_v_loss: 0.0764 - out_a_loss: 0.0725 - out_v_accuracy: 0.9699 - out_a_accuracy: 0.9718 - val_loss: 0.2703 - val_out_v_loss: 0.1292 - val_out_a_loss: 0.1411 - val_out_v_accuracy: 0.9525 - val_out_a_accuracy: 0.9514 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1535 - out_v_loss: 0.0763 - out_a_loss: 0.0772 - out_v_accuracy: 0.9699 - out_a_accuracy: 0.9696 - val_loss: 0.2185 - val_out_v_loss: 0.1129 - val_out_a_loss: 0.1055 - val_out_v_accuracy: 0.9555 - val_out_a_accuracy: 0.9598 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1392 - out_v_loss: 0.0708 - out_a_loss: 0.0683 - out_v_accuracy: 0.9730 - out_a_accuracy: 0.9730 - val_loss: 0.2258 - val_out_v_loss: 0.1079 - val_out_a_loss: 0.1179 - val_out_v_accuracy: 0.9609 - val_out_a_accuracy: 0.9551 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1337 - out_v_loss: 0.0657 - out_a_loss: 0.0680 - out_v_accuracy: 0.9738 - out_a_accuracy: 0.9729 - val_loss: 0.1959 - val_out_v_loss: 0.0960 - val_out_a_loss: 0.0999 - val_out_v_accuracy: 0.9664 - val_out_a_accuracy: 0.9641 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1421 - out_v_loss: 0.0711 - out_a_loss: 0.0710 - out_v_accuracy: 0.9733 - out_a_accuracy: 0.9724 - val_loss: 0.2156 - val_out_v_loss: 0.1061 - val_out_a_loss: 0.1095 - val_out_v_accuracy: 0.9592 - val_out_a_accuracy: 0.9578 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1471 - out_v_loss: 0.0723 - out_a_loss: 0.0747 - out_v_accuracy: 0.9713 - out_a_accuracy: 0.9715 - val_loss: 0.2368 - val_out_v_loss: 0.1222 - val_out_a_loss: 0.1145 - val_out_v_accuracy: 0.9525 - val_out_a_accuracy: 0.9582 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1283 - out_v_loss: 0.0637 - out_a_loss: 0.0646 - out_v_accuracy: 0.9762 - out_a_accuracy: 0.9733 - val_loss: 0.2145 - val_out_v_loss: 0.1046 - val_out_a_loss: 0.1098 - val_out_v_accuracy: 0.9643 - val_out_a_accuracy: 0.9598 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1212 - out_v_loss: 0.0597 - out_a_loss: 0.0615 - out_v_accuracy: 0.9765 - out_a_accuracy: 0.9767 - val_loss: 0.2341 - val_out_v_loss: 0.1061 - val_out_a_loss: 0.1280 - val_out_v_accuracy: 0.9627 - val_out_a_accuracy: 0.9557 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.1311 - out_v_loss: 0.0650 - out_a_loss: 0.0661 - out_v_accuracy: 0.9750 - out_a_accuracy: 0.9744\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1312 - out_v_loss: 0.0652 - out_a_loss: 0.0660 - out_v_accuracy: 0.9749 - out_a_accuracy: 0.9745 - val_loss: 0.2135 - val_out_v_loss: 0.1065 - val_out_a_loss: 0.1070 - val_out_v_accuracy: 0.9643 - val_out_a_accuracy: 0.9605 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0865 - out_v_loss: 0.0450 - out_a_loss: 0.0416 - out_v_accuracy: 0.9827 - out_a_accuracy: 0.9840 - val_loss: 0.1730 - val_out_v_loss: 0.0891 - val_out_a_loss: 0.0839 - val_out_v_accuracy: 0.9688 - val_out_a_accuracy: 0.9699 - lr: 5.0000e-04\n",
      "Epoch 38/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0669 - out_v_loss: 0.0329 - out_a_loss: 0.0339 - out_v_accuracy: 0.9877 - out_a_accuracy: 0.9868 - val_loss: 0.1806 - val_out_v_loss: 0.0878 - val_out_a_loss: 0.0928 - val_out_v_accuracy: 0.9684 - val_out_a_accuracy: 0.9699 - lr: 5.0000e-04\n",
      "Epoch 39/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0624 - out_v_loss: 0.0290 - out_a_loss: 0.0334 - out_v_accuracy: 0.9892 - out_a_accuracy: 0.9872 - val_loss: 0.1885 - val_out_v_loss: 0.0866 - val_out_a_loss: 0.1019 - val_out_v_accuracy: 0.9693 - val_out_a_accuracy: 0.9666 - lr: 5.0000e-04\n",
      "Epoch 40/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0641 - out_v_loss: 0.0318 - out_a_loss: 0.0323 - out_v_accuracy: 0.9883 - out_a_accuracy: 0.9883 - val_loss: 0.1652 - val_out_v_loss: 0.0826 - val_out_a_loss: 0.0826 - val_out_v_accuracy: 0.9715 - val_out_a_accuracy: 0.9719 - lr: 5.0000e-04\n",
      "Epoch 41/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0655 - out_v_loss: 0.0313 - out_a_loss: 0.0342 - out_v_accuracy: 0.9881 - out_a_accuracy: 0.9858 - val_loss: 0.1800 - val_out_v_loss: 0.0862 - val_out_a_loss: 0.0939 - val_out_v_accuracy: 0.9709 - val_out_a_accuracy: 0.9699 - lr: 5.0000e-04\n",
      "Epoch 42/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0621 - out_v_loss: 0.0299 - out_a_loss: 0.0322 - out_v_accuracy: 0.9891 - out_a_accuracy: 0.9872 - val_loss: 0.1715 - val_out_v_loss: 0.0820 - val_out_a_loss: 0.0894 - val_out_v_accuracy: 0.9740 - val_out_a_accuracy: 0.9721 - lr: 5.0000e-04\n",
      "Epoch 43/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0616 - out_v_loss: 0.0307 - out_a_loss: 0.0308 - out_v_accuracy: 0.9876 - out_a_accuracy: 0.9890 - val_loss: 0.1539 - val_out_v_loss: 0.0733 - val_out_a_loss: 0.0807 - val_out_v_accuracy: 0.9736 - val_out_a_accuracy: 0.9738 - lr: 5.0000e-04\n",
      "Epoch 44/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0611 - out_v_loss: 0.0314 - out_a_loss: 0.0298 - out_v_accuracy: 0.9874 - out_a_accuracy: 0.9890 - val_loss: 0.1733 - val_out_v_loss: 0.0899 - val_out_a_loss: 0.0834 - val_out_v_accuracy: 0.9711 - val_out_a_accuracy: 0.9740 - lr: 5.0000e-04\n",
      "Epoch 45/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0574 - out_v_loss: 0.0303 - out_a_loss: 0.0272 - out_v_accuracy: 0.9884 - out_a_accuracy: 0.9891 - val_loss: 0.1874 - val_out_v_loss: 0.0946 - val_out_a_loss: 0.0928 - val_out_v_accuracy: 0.9713 - val_out_a_accuracy: 0.9693 - lr: 5.0000e-04\n",
      "Epoch 46/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0613 - out_v_loss: 0.0298 - out_a_loss: 0.0315 - out_v_accuracy: 0.9886 - out_a_accuracy: 0.9884 - val_loss: 0.1899 - val_out_v_loss: 0.0989 - val_out_a_loss: 0.0910 - val_out_v_accuracy: 0.9695 - val_out_a_accuracy: 0.9703 - lr: 5.0000e-04\n",
      "Epoch 47/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0614 - out_v_loss: 0.0322 - out_a_loss: 0.0293 - out_v_accuracy: 0.9876 - out_a_accuracy: 0.9883 - val_loss: 0.1639 - val_out_v_loss: 0.0848 - val_out_a_loss: 0.0792 - val_out_v_accuracy: 0.9732 - val_out_a_accuracy: 0.9770 - lr: 5.0000e-04\n",
      "Epoch 48/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0542 - out_v_loss: 0.0272 - out_a_loss: 0.0270 - out_v_accuracy: 0.9892 - out_a_accuracy: 0.9903\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0543 - out_v_loss: 0.0272 - out_a_loss: 0.0271 - out_v_accuracy: 0.9892 - out_a_accuracy: 0.9902 - val_loss: 0.2110 - val_out_v_loss: 0.0886 - val_out_a_loss: 0.1224 - val_out_v_accuracy: 0.9711 - val_out_a_accuracy: 0.9658 - lr: 5.0000e-04\n",
      "Epoch 49/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0466 - out_v_loss: 0.0231 - out_a_loss: 0.0235 - out_v_accuracy: 0.9913 - out_a_accuracy: 0.9917 - val_loss: 0.1572 - val_out_v_loss: 0.0711 - val_out_a_loss: 0.0861 - val_out_v_accuracy: 0.9760 - val_out_a_accuracy: 0.9746 - lr: 2.5000e-04\n",
      "Epoch 50/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0397 - out_v_loss: 0.0187 - out_a_loss: 0.0210 - out_v_accuracy: 0.9931 - out_a_accuracy: 0.9919 - val_loss: 0.1597 - val_out_v_loss: 0.0703 - val_out_a_loss: 0.0894 - val_out_v_accuracy: 0.9766 - val_out_a_accuracy: 0.9723 - lr: 2.5000e-04\n",
      "Epoch 51/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0372 - out_v_loss: 0.0176 - out_a_loss: 0.0195 - out_v_accuracy: 0.9932 - out_a_accuracy: 0.9934 - val_loss: 0.1529 - val_out_v_loss: 0.0676 - val_out_a_loss: 0.0853 - val_out_v_accuracy: 0.9783 - val_out_a_accuracy: 0.9734 - lr: 2.5000e-04\n",
      "Epoch 52/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0371 - out_v_loss: 0.0178 - out_a_loss: 0.0193 - out_v_accuracy: 0.9934 - out_a_accuracy: 0.9927 - val_loss: 0.1621 - val_out_v_loss: 0.0792 - val_out_a_loss: 0.0829 - val_out_v_accuracy: 0.9754 - val_out_a_accuracy: 0.9746 - lr: 2.5000e-04\n",
      "Epoch 53/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0351 - out_v_loss: 0.0168 - out_a_loss: 0.0183 - out_v_accuracy: 0.9936 - out_a_accuracy: 0.9934 - val_loss: 0.1635 - val_out_v_loss: 0.0764 - val_out_a_loss: 0.0871 - val_out_v_accuracy: 0.9766 - val_out_a_accuracy: 0.9721 - lr: 2.5000e-04\n",
      "Epoch 54/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0358 - out_v_loss: 0.0175 - out_a_loss: 0.0183 - out_v_accuracy: 0.9936 - out_a_accuracy: 0.9927 - val_loss: 0.1595 - val_out_v_loss: 0.0735 - val_out_a_loss: 0.0860 - val_out_v_accuracy: 0.9764 - val_out_a_accuracy: 0.9740 - lr: 2.5000e-04\n",
      "Epoch 55/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0325 - out_v_loss: 0.0142 - out_a_loss: 0.0183 - out_v_accuracy: 0.9947 - out_a_accuracy: 0.9932 - val_loss: 0.1568 - val_out_v_loss: 0.0712 - val_out_a_loss: 0.0856 - val_out_v_accuracy: 0.9768 - val_out_a_accuracy: 0.9738 - lr: 2.5000e-04\n",
      "Epoch 56/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0363 - out_v_loss: 0.0164 - out_a_loss: 0.0199 - out_v_accuracy: 0.9940 - out_a_accuracy: 0.9932\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0364 - out_v_loss: 0.0166 - out_a_loss: 0.0198 - out_v_accuracy: 0.9939 - out_a_accuracy: 0.9933 - val_loss: 0.1671 - val_out_v_loss: 0.0765 - val_out_a_loss: 0.0907 - val_out_v_accuracy: 0.9773 - val_out_a_accuracy: 0.9734 - lr: 2.5000e-04\n",
      "Epoch 57/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0314 - out_v_loss: 0.0157 - out_a_loss: 0.0157 - out_v_accuracy: 0.9941 - out_a_accuracy: 0.9940 - val_loss: 0.1490 - val_out_v_loss: 0.0710 - val_out_a_loss: 0.0780 - val_out_v_accuracy: 0.9795 - val_out_a_accuracy: 0.9752 - lr: 1.2500e-04\n",
      "Epoch 58/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0269 - out_v_loss: 0.0137 - out_a_loss: 0.0132 - out_v_accuracy: 0.9943 - out_a_accuracy: 0.9953 - val_loss: 0.1516 - val_out_v_loss: 0.0702 - val_out_a_loss: 0.0813 - val_out_v_accuracy: 0.9783 - val_out_a_accuracy: 0.9752 - lr: 1.2500e-04\n",
      "Epoch 59/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0264 - out_v_loss: 0.0117 - out_a_loss: 0.0147 - out_v_accuracy: 0.9955 - out_a_accuracy: 0.9948 - val_loss: 0.1525 - val_out_v_loss: 0.0729 - val_out_a_loss: 0.0796 - val_out_v_accuracy: 0.9775 - val_out_a_accuracy: 0.9768 - lr: 1.2500e-04\n",
      "Epoch 60/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0243 - out_v_loss: 0.0123 - out_a_loss: 0.0119 - out_v_accuracy: 0.9953 - out_a_accuracy: 0.9959 - val_loss: 0.1487 - val_out_v_loss: 0.0687 - val_out_a_loss: 0.0800 - val_out_v_accuracy: 0.9801 - val_out_a_accuracy: 0.9764 - lr: 1.2500e-04\n",
      "Epoch 61/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0243 - out_v_loss: 0.0124 - out_a_loss: 0.0119 - out_v_accuracy: 0.9952 - out_a_accuracy: 0.9957 - val_loss: 0.1513 - val_out_v_loss: 0.0700 - val_out_a_loss: 0.0813 - val_out_v_accuracy: 0.9787 - val_out_a_accuracy: 0.9766 - lr: 1.2500e-04\n",
      "Epoch 62/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0270 - out_v_loss: 0.0133 - out_a_loss: 0.0137 - out_v_accuracy: 0.9954 - out_a_accuracy: 0.9951 - val_loss: 0.1545 - val_out_v_loss: 0.0689 - val_out_a_loss: 0.0856 - val_out_v_accuracy: 0.9801 - val_out_a_accuracy: 0.9758 - lr: 1.2500e-04\n",
      "Epoch 63/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0216 - out_v_loss: 0.0113 - out_a_loss: 0.0103 - out_v_accuracy: 0.9958 - out_a_accuracy: 0.9964 - val_loss: 0.1534 - val_out_v_loss: 0.0684 - val_out_a_loss: 0.0850 - val_out_v_accuracy: 0.9787 - val_out_a_accuracy: 0.9750 - lr: 1.2500e-04\n",
      "Epoch 64/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0247 - out_v_loss: 0.0135 - out_a_loss: 0.0112 - out_v_accuracy: 0.9950 - out_a_accuracy: 0.9960 - val_loss: 0.1525 - val_out_v_loss: 0.0729 - val_out_a_loss: 0.0797 - val_out_v_accuracy: 0.9773 - val_out_a_accuracy: 0.9777 - lr: 1.2500e-04\n",
      "Epoch 65/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0214 - out_v_loss: 0.0107 - out_a_loss: 0.0107 - out_v_accuracy: 0.9960 - out_a_accuracy: 0.9962\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0213 - out_v_loss: 0.0107 - out_a_loss: 0.0107 - out_v_accuracy: 0.9960 - out_a_accuracy: 0.9962 - val_loss: 0.1518 - val_out_v_loss: 0.0693 - val_out_a_loss: 0.0825 - val_out_v_accuracy: 0.9797 - val_out_a_accuracy: 0.9762 - lr: 1.2500e-04\n",
      "Epoch 66/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0204 - out_v_loss: 0.0105 - out_a_loss: 0.0099 - out_v_accuracy: 0.9964 - out_a_accuracy: 0.9968 - val_loss: 0.1503 - val_out_v_loss: 0.0666 - val_out_a_loss: 0.0837 - val_out_v_accuracy: 0.9799 - val_out_a_accuracy: 0.9766 - lr: 6.2500e-05\n",
      "Epoch 67/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0221 - out_v_loss: 0.0108 - out_a_loss: 0.0113 - out_v_accuracy: 0.9962 - out_a_accuracy: 0.9958 - val_loss: 0.1519 - val_out_v_loss: 0.0663 - val_out_a_loss: 0.0856 - val_out_v_accuracy: 0.9783 - val_out_a_accuracy: 0.9762 - lr: 6.2500e-05\n",
      "Epoch 68/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0200 - out_v_loss: 0.0102 - out_a_loss: 0.0097 - out_v_accuracy: 0.9960 - out_a_accuracy: 0.9964 - val_loss: 0.1510 - val_out_v_loss: 0.0680 - val_out_a_loss: 0.0830 - val_out_v_accuracy: 0.9791 - val_out_a_accuracy: 0.9756 - lr: 6.2500e-05\n",
      "Epoch 69/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0202 - out_v_loss: 0.0102 - out_a_loss: 0.0099 - out_v_accuracy: 0.9966 - out_a_accuracy: 0.9963 - val_loss: 0.1431 - val_out_v_loss: 0.0641 - val_out_a_loss: 0.0790 - val_out_v_accuracy: 0.9801 - val_out_a_accuracy: 0.9764 - lr: 6.2500e-05\n",
      "Epoch 70/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0188 - out_v_loss: 0.0102 - out_a_loss: 0.0086 - out_v_accuracy: 0.9963 - out_a_accuracy: 0.9972 - val_loss: 0.1491 - val_out_v_loss: 0.0667 - val_out_a_loss: 0.0823 - val_out_v_accuracy: 0.9795 - val_out_a_accuracy: 0.9762 - lr: 6.2500e-05\n",
      "Epoch 71/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0174 - out_v_loss: 0.0082 - out_a_loss: 0.0092 - out_v_accuracy: 0.9974 - out_a_accuracy: 0.9968 - val_loss: 0.1534 - val_out_v_loss: 0.0679 - val_out_a_loss: 0.0854 - val_out_v_accuracy: 0.9809 - val_out_a_accuracy: 0.9752 - lr: 6.2500e-05\n",
      "Epoch 72/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0197 - out_v_loss: 0.0092 - out_a_loss: 0.0104 - out_v_accuracy: 0.9965 - out_a_accuracy: 0.9961 - val_loss: 0.1429 - val_out_v_loss: 0.0644 - val_out_a_loss: 0.0786 - val_out_v_accuracy: 0.9805 - val_out_a_accuracy: 0.9777 - lr: 6.2500e-05\n",
      "Epoch 73/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0186 - out_v_loss: 0.0095 - out_a_loss: 0.0091 - out_v_accuracy: 0.9971 - out_a_accuracy: 0.9969 - val_loss: 0.1423 - val_out_v_loss: 0.0642 - val_out_a_loss: 0.0781 - val_out_v_accuracy: 0.9803 - val_out_a_accuracy: 0.9779 - lr: 6.2500e-05\n",
      "Epoch 74/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0193 - out_v_loss: 0.0092 - out_a_loss: 0.0100 - out_v_accuracy: 0.9968 - out_a_accuracy: 0.9968 - val_loss: 0.1484 - val_out_v_loss: 0.0657 - val_out_a_loss: 0.0828 - val_out_v_accuracy: 0.9791 - val_out_a_accuracy: 0.9764 - lr: 6.2500e-05\n",
      "Epoch 75/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0184 - out_v_loss: 0.0100 - out_a_loss: 0.0084 - out_v_accuracy: 0.9965 - out_a_accuracy: 0.9972 - val_loss: 0.1565 - val_out_v_loss: 0.0695 - val_out_a_loss: 0.0871 - val_out_v_accuracy: 0.9805 - val_out_a_accuracy: 0.9764 - lr: 6.2500e-05\n",
      "Epoch 76/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0172 - out_v_loss: 0.0079 - out_a_loss: 0.0093 - out_v_accuracy: 0.9967 - out_a_accuracy: 0.9967 - val_loss: 0.1538 - val_out_v_loss: 0.0697 - val_out_a_loss: 0.0841 - val_out_v_accuracy: 0.9795 - val_out_a_accuracy: 0.9771 - lr: 6.2500e-05\n",
      "Epoch 77/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0178 - out_v_loss: 0.0101 - out_a_loss: 0.0077 - out_v_accuracy: 0.9968 - out_a_accuracy: 0.9975 - val_loss: 0.1516 - val_out_v_loss: 0.0669 - val_out_a_loss: 0.0847 - val_out_v_accuracy: 0.9803 - val_out_a_accuracy: 0.9760 - lr: 6.2500e-05\n",
      "Epoch 78/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0210 - out_v_loss: 0.0091 - out_a_loss: 0.0119 - out_v_accuracy: 0.9967 - out_a_accuracy: 0.9961\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0210 - out_v_loss: 0.0091 - out_a_loss: 0.0119 - out_v_accuracy: 0.9967 - out_a_accuracy: 0.9961 - val_loss: 0.1440 - val_out_v_loss: 0.0632 - val_out_a_loss: 0.0807 - val_out_v_accuracy: 0.9799 - val_out_a_accuracy: 0.9787 - lr: 6.2500e-05\n",
      "Epoch 79/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0172 - out_v_loss: 0.0087 - out_a_loss: 0.0085 - out_v_accuracy: 0.9966 - out_a_accuracy: 0.9974 - val_loss: 0.1439 - val_out_v_loss: 0.0638 - val_out_a_loss: 0.0801 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9781 - lr: 3.1250e-05\n",
      "Epoch 80/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0182 - out_v_loss: 0.0100 - out_a_loss: 0.0081 - out_v_accuracy: 0.9959 - out_a_accuracy: 0.9974 - val_loss: 0.1458 - val_out_v_loss: 0.0643 - val_out_a_loss: 0.0815 - val_out_v_accuracy: 0.9803 - val_out_a_accuracy: 0.9771 - lr: 3.1250e-05\n",
      "Epoch 81/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0164 - out_v_loss: 0.0092 - out_a_loss: 0.0072 - out_v_accuracy: 0.9971 - out_a_accuracy: 0.9973 - val_loss: 0.1439 - val_out_v_loss: 0.0637 - val_out_a_loss: 0.0802 - val_out_v_accuracy: 0.9805 - val_out_a_accuracy: 0.9785 - lr: 3.1250e-05\n",
      "Epoch 82/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0158 - out_v_loss: 0.0082 - out_a_loss: 0.0076 - out_v_accuracy: 0.9968 - out_a_accuracy: 0.9979 - val_loss: 0.1476 - val_out_v_loss: 0.0650 - val_out_a_loss: 0.0826 - val_out_v_accuracy: 0.9807 - val_out_a_accuracy: 0.9770 - lr: 3.1250e-05\n",
      "Epoch 83/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0164 - out_v_loss: 0.0079 - out_a_loss: 0.0085 - out_v_accuracy: 0.9969 - out_a_accuracy: 0.9968\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0164 - out_v_loss: 0.0079 - out_a_loss: 0.0085 - out_v_accuracy: 0.9969 - out_a_accuracy: 0.9968 - val_loss: 0.1438 - val_out_v_loss: 0.0641 - val_out_a_loss: 0.0798 - val_out_v_accuracy: 0.9816 - val_out_a_accuracy: 0.9781 - lr: 3.1250e-05\n",
      "Epoch 84/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0163 - out_v_loss: 0.0071 - out_a_loss: 0.0092 - out_v_accuracy: 0.9979 - out_a_accuracy: 0.9967 - val_loss: 0.1503 - val_out_v_loss: 0.0674 - val_out_a_loss: 0.0829 - val_out_v_accuracy: 0.9809 - val_out_a_accuracy: 0.9771 - lr: 1.5625e-05\n",
      "Epoch 85/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0144 - out_v_loss: 0.0066 - out_a_loss: 0.0078 - out_v_accuracy: 0.9976 - out_a_accuracy: 0.9972 - val_loss: 0.1451 - val_out_v_loss: 0.0634 - val_out_a_loss: 0.0817 - val_out_v_accuracy: 0.9816 - val_out_a_accuracy: 0.9771 - lr: 1.5625e-05\n",
      "Epoch 86/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0137 - out_v_loss: 0.0074 - out_a_loss: 0.0064 - out_v_accuracy: 0.9974 - out_a_accuracy: 0.9979 - val_loss: 0.1516 - val_out_v_loss: 0.0692 - val_out_a_loss: 0.0824 - val_out_v_accuracy: 0.9799 - val_out_a_accuracy: 0.9771 - lr: 1.5625e-05\n",
      "Epoch 87/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0145 - out_v_loss: 0.0075 - out_a_loss: 0.0070 - out_v_accuracy: 0.9971 - out_a_accuracy: 0.9979 - val_loss: 0.1449 - val_out_v_loss: 0.0645 - val_out_a_loss: 0.0804 - val_out_v_accuracy: 0.9807 - val_out_a_accuracy: 0.9775 - lr: 1.5625e-05\n",
      "Epoch 88/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0160 - out_v_loss: 0.0076 - out_a_loss: 0.0084 - out_v_accuracy: 0.9972 - out_a_accuracy: 0.9968\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0161 - out_v_loss: 0.0076 - out_a_loss: 0.0085 - out_v_accuracy: 0.9972 - out_a_accuracy: 0.9968 - val_loss: 0.1434 - val_out_v_loss: 0.0637 - val_out_a_loss: 0.0798 - val_out_v_accuracy: 0.9820 - val_out_a_accuracy: 0.9779 - lr: 1.5625e-05\n",
      "Epoch 89/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0160 - out_v_loss: 0.0076 - out_a_loss: 0.0084 - out_v_accuracy: 0.9972 - out_a_accuracy: 0.9974 - val_loss: 0.1440 - val_out_v_loss: 0.0642 - val_out_a_loss: 0.0798 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9779 - lr: 7.8125e-06\n",
      "Epoch 00089: early stopping\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.1440 - out_v_loss: 0.0642 - out_a_loss: 0.0798 - out_v_accuracy: 0.9814 - out_a_accuracy: 0.9779\n",
      "\n",
      "processing:  01 ......\n",
      "Before fine-tuning: [('loss', 0.056002), ('valence loss', 0.0385), ('arousal loss', 0.017502), ('valence acc', 0.981132), ('arousal acc', 0.987421)]\n",
      "Epoch 00030: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.028944), ('valence loss', 0.011051), ('arousal loss', 0.017893), ('valence acc', 0.993711), ('arousal acc', 0.993711)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.007514), ('valence loss', 0.007514), ('arousal loss', 0.045471), ('valence acc', 1.0), ('arousal acc', 0.974843)]\n",
      "Epoch 00040: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.004403), ('valence loss', 0.013766), ('arousal loss', 0.004403), ('valence acc', 0.987421), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  02 ......\n",
      "Before fine-tuning: [('loss', 0.161202), ('valence loss', 0.022407), ('arousal loss', 0.138795), ('valence acc', 0.987421), ('arousal acc', 0.968553)]\n",
      "Epoch 00019: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.126861), ('valence loss', 0.018271), ('arousal loss', 0.108589), ('valence acc', 0.987421), ('arousal acc', 0.968553)]\n",
      "Epoch 00048: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.017304), ('valence loss', 0.017304), ('arousal loss', 0.11724), ('valence acc', 0.993711), ('arousal acc', 0.968553)]\n",
      "Epoch 00049: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.108918), ('valence loss', 0.028777), ('arousal loss', 0.108918), ('valence acc', 0.993711), ('arousal acc', 0.974843)]\n",
      "\n",
      "processing:  03 ......\n",
      "Before fine-tuning: [('loss', 0.079892), ('valence loss', 0.028158), ('arousal loss', 0.051734), ('valence acc', 0.993506), ('arousal acc', 0.987013)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.079892), ('valence loss', 0.028158), ('arousal loss', 0.051734), ('valence acc', 0.993506), ('arousal acc', 0.987013)]\n",
      "Epoch 00036: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.017747), ('valence loss', 0.017747), ('arousal loss', 0.197194), ('valence acc', 0.993506), ('arousal acc', 0.935065)]\n",
      "Epoch 00022: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.027947), ('valence loss', 0.065899), ('arousal loss', 0.027947), ('valence acc', 0.980519), ('arousal acc', 0.987013)]\n",
      "\n",
      "processing:  04 ......\n",
      "Before fine-tuning: [('loss', 0.242255), ('valence loss', 0.068377), ('arousal loss', 0.173877), ('valence acc', 0.977012), ('arousal acc', 0.942529)]\n",
      "Epoch 00072: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.242255), ('valence loss', 0.068377), ('arousal loss', 0.173877), ('valence acc', 0.977012), ('arousal acc', 0.942529)]\n",
      "Epoch 00032: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.068377), ('valence loss', 0.068377), ('arousal loss', 0.173877), ('valence acc', 0.977012), ('arousal acc', 0.942529)]\n",
      "Epoch 00035: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.173877), ('valence loss', 0.068377), ('arousal loss', 0.173877), ('valence acc', 0.977012), ('arousal acc', 0.942529)]\n",
      "\n",
      "processing:  05 ......\n",
      "Before fine-tuning: [('loss', 0.233592), ('valence loss', 0.186622), ('arousal loss', 0.04697), ('valence acc', 0.941558), ('arousal acc', 0.980519)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.214752), ('valence loss', 0.175515), ('arousal loss', 0.039236), ('valence acc', 0.941558), ('arousal acc', 0.974026)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.156006), ('valence loss', 0.156006), ('arousal loss', 0.052132), ('valence acc', 0.954545), ('arousal acc', 0.974026)]\n",
      "Epoch 00068: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.043977), ('valence loss', 0.270582), ('arousal loss', 0.043977), ('valence acc', 0.928571), ('arousal acc', 0.974026)]\n",
      "\n",
      "processing:  06 ......\n",
      "Before fine-tuning: [('loss', 0.021516), ('valence loss', 0.00345), ('arousal loss', 0.018066), ('valence acc', 1.0), ('arousal acc', 0.993421)]\n",
      "Epoch 00041: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.021516), ('valence loss', 0.00345), ('arousal loss', 0.018066), ('valence acc', 1.0), ('arousal acc', 0.993421)]\n",
      "Epoch 00056: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.001564), ('valence loss', 0.001564), ('arousal loss', 0.098482), ('valence acc', 1.0), ('arousal acc', 0.973684)]\n",
      "Epoch 00022: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.018066), ('valence loss', 0.00345), ('arousal loss', 0.018066), ('valence acc', 1.0), ('arousal acc', 0.993421)]\n",
      "\n",
      "processing:  07 ......\n",
      "Before fine-tuning: [('loss', 0.015632), ('valence loss', 0.0113), ('arousal loss', 0.004332), ('valence acc', 0.993506), ('arousal acc', 1.0)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.014408), ('valence loss', 0.005702), ('arousal loss', 0.008706), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "Epoch 00041: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.006604), ('valence loss', 0.006604), ('arousal loss', 0.012796), ('valence acc', 1.0), ('arousal acc', 0.993506)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.001441), ('valence loss', 0.021428), ('arousal loss', 0.001441), ('valence acc', 0.993506), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  08 ......\n",
      "Before fine-tuning: [('loss', 0.044081), ('valence loss', 0.00899), ('arousal loss', 0.035091), ('valence acc', 0.994012), ('arousal acc', 0.988024)]\n",
      "Epoch 00121: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.023893), ('valence loss', 0.003925), ('arousal loss', 0.019968), ('valence acc', 1.0), ('arousal acc', 0.988024)]\n",
      "Epoch 00028: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.00168), ('valence loss', 0.00168), ('arousal loss', 0.03425), ('valence acc', 1.0), ('arousal acc', 0.982036)]\n",
      "Epoch 00073: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.017475), ('valence loss', 0.016529), ('arousal loss', 0.017475), ('valence acc', 0.994012), ('arousal acc', 0.994012)]\n",
      "\n",
      "processing:  09 ......\n",
      "Before fine-tuning: [('loss', 0.344213), ('valence loss', 0.034806), ('arousal loss', 0.309407), ('valence acc', 0.981481), ('arousal acc', 0.944444)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.320852), ('valence loss', 0.042541), ('arousal loss', 0.278311), ('valence acc', 0.969136), ('arousal acc', 0.95679)]\n",
      "Epoch 00040: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.034806), ('valence loss', 0.034806), ('arousal loss', 0.309407), ('valence acc', 0.981481), ('arousal acc', 0.944444)]\n",
      "Epoch 00019: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.266481), ('valence loss', 0.046724), ('arousal loss', 0.266481), ('valence acc', 0.975309), ('arousal acc', 0.962963)]\n",
      "\n",
      "processing:  10 ......\n",
      "Before fine-tuning: [('loss', 0.070451), ('valence loss', 0.005123), ('arousal loss', 0.065329), ('valence acc', 1.0), ('arousal acc', 0.993671)]\n",
      "Epoch 00052: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.038522), ('valence loss', 0.001531), ('arousal loss', 0.036991), ('valence acc', 1.0), ('arousal acc', 0.987342)]\n",
      "Epoch 00026: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.000538), ('valence loss', 0.000538), ('arousal loss', 0.034881), ('valence acc', 1.0), ('arousal acc', 0.993671)]\n",
      "Epoch 00064: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.044971), ('valence loss', 0.002085), ('arousal loss', 0.044971), ('valence acc', 1.0), ('arousal acc', 0.993671)]\n",
      "\n",
      "processing:  11 ......\n",
      "Before fine-tuning: [('loss', 0.211536), ('valence loss', 0.196265), ('arousal loss', 0.01527), ('valence acc', 0.938272), ('arousal acc', 0.993827)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.211536), ('valence loss', 0.196265), ('arousal loss', 0.01527), ('valence acc', 0.938272), ('arousal acc', 0.993827)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.162116), ('valence loss', 0.162116), ('arousal loss', 0.152141), ('valence acc', 0.925926), ('arousal acc', 0.950617)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.01527), ('valence loss', 0.196265), ('arousal loss', 0.01527), ('valence acc', 0.938272), ('arousal acc', 0.993827)]\n",
      "\n",
      "processing:  12 ......\n",
      "Before fine-tuning: [('loss', 0.07333), ('valence loss', 0.015083), ('arousal loss', 0.058247), ('valence acc', 0.986842), ('arousal acc', 0.993421)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.066396), ('valence loss', 0.013718), ('arousal loss', 0.052678), ('valence acc', 0.986842), ('arousal acc', 0.980263)]\n",
      "Epoch 00065: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.005032), ('valence loss', 0.005032), ('arousal loss', 0.202807), ('valence acc', 1.0), ('arousal acc', 0.927632)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.053619), ('valence loss', 0.014325), ('arousal loss', 0.053619), ('valence acc', 0.986842), ('arousal acc', 0.986842)]\n",
      "\n",
      "processing:  13 ......\n",
      "Before fine-tuning: [('loss', 0.098836), ('valence loss', 0.097641), ('arousal loss', 0.001196), ('valence acc', 0.972222), ('arousal acc', 1.0)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.06441), ('valence loss', 0.047595), ('arousal loss', 0.016815), ('valence acc', 0.979167), ('arousal acc', 0.993056)]\n",
      "Epoch 00047: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.048449), ('valence loss', 0.048449), ('arousal loss', 0.173393), ('valence acc', 0.965278), ('arousal acc', 0.9375)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.001196), ('valence loss', 0.097641), ('arousal loss', 0.001196), ('valence acc', 0.972222), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  14 ......\n",
      "Before fine-tuning: [('loss', 0.0432), ('valence loss', 0.034558), ('arousal loss', 0.008642), ('valence acc', 0.988235), ('arousal acc', 0.994118)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.033213), ('valence loss', 0.023547), ('arousal loss', 0.009667), ('valence acc', 0.988235), ('arousal acc', 0.994118)]\n",
      "Epoch 00021: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.014878), ('valence loss', 0.014878), ('arousal loss', 0.040905), ('valence acc', 0.988235), ('arousal acc', 0.988235)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.008642), ('valence loss', 0.034558), ('arousal loss', 0.008642), ('valence acc', 0.988235), ('arousal acc', 0.994118)]\n",
      "\n",
      "processing:  15 ......\n",
      "Before fine-tuning: [('loss', 0.206124), ('valence loss', 0.099179), ('arousal loss', 0.106945), ('valence acc', 0.987421), ('arousal acc', 0.981132)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.133983), ('valence loss', 0.056467), ('arousal loss', 0.077516), ('valence acc', 0.981132), ('arousal acc', 0.974843)]\n",
      "Epoch 00032: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.059616), ('valence loss', 0.059616), ('arousal loss', 0.099093), ('valence acc', 0.981132), ('arousal acc', 0.981132)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.057511), ('valence loss', 0.065369), ('arousal loss', 0.057511), ('valence acc', 0.987421), ('arousal acc', 0.987421)]\n",
      "\n",
      "processing:  16 ......\n",
      "Before fine-tuning: [('loss', 0.284516), ('valence loss', 0.099406), ('arousal loss', 0.18511), ('valence acc', 0.980263), ('arousal acc', 0.973684)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.281631), ('valence loss', 0.095137), ('arousal loss', 0.186494), ('valence acc', 0.993421), ('arousal acc', 0.967105)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.09705), ('valence loss', 0.09705), ('arousal loss', 0.182075), ('valence acc', 0.980263), ('arousal acc', 0.973684)]\n",
      "Epoch 00040: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.18511), ('valence loss', 0.099406), ('arousal loss', 0.18511), ('valence acc', 0.980263), ('arousal acc', 0.973684)]\n",
      "\n",
      "processing:  17 ......\n",
      "Before fine-tuning: [('loss', 0.293874), ('valence loss', 0.12276), ('arousal loss', 0.171113), ('valence acc', 0.965909), ('arousal acc', 0.960227)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.293874), ('valence loss', 0.12276), ('arousal loss', 0.171113), ('valence acc', 0.965909), ('arousal acc', 0.960227)]\n",
      "Epoch 00032: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.12276), ('valence loss', 0.12276), ('arousal loss', 0.171113), ('valence acc', 0.965909), ('arousal acc', 0.960227)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.171113), ('valence loss', 0.12276), ('arousal loss', 0.171113), ('valence acc', 0.965909), ('arousal acc', 0.960227)]\n",
      "\n",
      "processing:  18 ......\n",
      "Before fine-tuning: [('loss', 0.164385), ('valence loss', 0.102447), ('arousal loss', 0.061938), ('valence acc', 0.982759), ('arousal acc', 0.971264)]\n",
      "Epoch 00049: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.132582), ('valence loss', 0.063836), ('arousal loss', 0.068746), ('valence acc', 0.982759), ('arousal acc', 0.982759)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.05556), ('valence loss', 0.05556), ('arousal loss', 0.061311), ('valence acc', 0.982759), ('arousal acc', 0.982759)]\n",
      "Epoch 00034: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.046443), ('valence loss', 0.057724), ('arousal loss', 0.046443), ('valence acc', 0.977012), ('arousal acc', 0.977012)]\n",
      "\n",
      "processing:  19 ......\n",
      "Before fine-tuning: [('loss', 0.049242), ('valence loss', 0.020873), ('arousal loss', 0.028369), ('valence acc', 0.99359), ('arousal acc', 0.99359)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.038348), ('valence loss', 0.013942), ('arousal loss', 0.024406), ('valence acc', 0.99359), ('arousal acc', 0.99359)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.008119), ('valence loss', 0.008119), ('arousal loss', 0.017947), ('valence acc', 1.0), ('arousal acc', 0.99359)]\n",
      "Epoch 00020: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.009894), ('valence loss', 0.015756), ('arousal loss', 0.009894), ('valence acc', 0.99359), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  20 ......\n",
      "Before fine-tuning: [('loss', 0.044475), ('valence loss', 0.017331), ('arousal loss', 0.027143), ('valence acc', 0.993827), ('arousal acc', 0.987654)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.044475), ('valence loss', 0.017331), ('arousal loss', 0.027143), ('valence acc', 0.993827), ('arousal acc', 0.987654)]\n",
      "Epoch 00038: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.007843), ('valence loss', 0.007843), ('arousal loss', 0.131098), ('valence acc', 0.993827), ('arousal acc', 0.975309)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.027143), ('valence loss', 0.017331), ('arousal loss', 0.027143), ('valence acc', 0.993827), ('arousal acc', 0.987654)]\n",
      "\n",
      "processing:  21 ......\n",
      "Before fine-tuning: [('loss', 0.019264), ('valence loss', 0.019037), ('arousal loss', 0.000227), ('valence acc', 0.987952), ('arousal acc', 1.0)]\n",
      "Epoch 00028: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.019264), ('valence loss', 0.019037), ('arousal loss', 0.000227), ('valence acc', 0.987952), ('arousal acc', 1.0)]\n",
      "Epoch 00046: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.003089), ('valence loss', 0.003089), ('arousal loss', 0.035514), ('valence acc', 1.0), ('arousal acc', 0.981928)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.000227), ('valence loss', 0.019037), ('arousal loss', 0.000227), ('valence acc', 0.987952), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  22 ......\n",
      "Before fine-tuning: [('loss', 0.668135), ('valence loss', 0.353544), ('arousal loss', 0.314591), ('valence acc', 0.900553), ('arousal acc', 0.878453)]\n",
      "Epoch 00045: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.667104), ('valence loss', 0.343118), ('arousal loss', 0.323986), ('valence acc', 0.922652), ('arousal acc', 0.883978)]\n",
      "Epoch 00032: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.350954), ('valence loss', 0.350954), ('arousal loss', 0.329401), ('valence acc', 0.917127), ('arousal acc', 0.883978)]\n",
      "Epoch 00094: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.314591), ('valence loss', 0.353544), ('arousal loss', 0.314591), ('valence acc', 0.900553), ('arousal acc', 0.878453)]\n",
      "\n",
      "processing:  23 ......\n",
      "Before fine-tuning: [('loss', 0.013545), ('valence loss', 0.003747), ('arousal loss', 0.009798), ('valence acc', 1.0), ('arousal acc', 0.993333)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.013545), ('valence loss', 0.003747), ('arousal loss', 0.009798), ('valence acc', 1.0), ('arousal acc', 0.993333)]\n",
      "Epoch 00035: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.003747), ('valence loss', 0.003747), ('arousal loss', 0.009798), ('valence acc', 1.0), ('arousal acc', 0.993333)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.009798), ('valence loss', 0.003747), ('arousal loss', 0.009798), ('valence acc', 1.0), ('arousal acc', 0.993333)]\n",
      "\n",
      "processing:  24 ......\n",
      "Before fine-tuning: [('loss', 0.252643), ('valence loss', 0.103917), ('arousal loss', 0.148726), ('valence acc', 0.966443), ('arousal acc', 0.959732)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.237117), ('valence loss', 0.099781), ('arousal loss', 0.137335), ('valence acc', 0.966443), ('arousal acc', 0.95302)]\n",
      "Epoch 00031: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.101834), ('valence loss', 0.101834), ('arousal loss', 0.140424), ('valence acc', 0.959732), ('arousal acc', 0.95302)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.103168), ('valence loss', 0.094105), ('arousal loss', 0.103168), ('valence acc', 0.973154), ('arousal acc', 0.959732)]\n",
      "\n",
      "processing:  25 ......\n",
      "Before fine-tuning: [('loss', 0.086488), ('valence loss', 0.03478), ('arousal loss', 0.051708), ('valence acc', 0.982143), ('arousal acc', 0.988095)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.086488), ('valence loss', 0.03478), ('arousal loss', 0.051708), ('valence acc', 0.982143), ('arousal acc', 0.988095)]\n",
      "Epoch 00071: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.020212), ('valence loss', 0.020212), ('arousal loss', 0.11042), ('valence acc', 0.982143), ('arousal acc', 0.982143)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.046756), ('valence loss', 0.027472), ('arousal loss', 0.046756), ('valence acc', 0.994048), ('arousal acc', 0.994048)]\n",
      "\n",
      "processing:  26 ......\n",
      "Before fine-tuning: [('loss', 0.210129), ('valence loss', 0.10394), ('arousal loss', 0.106189), ('valence acc', 0.972222), ('arousal acc', 0.944444)]\n",
      "Epoch 00030: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.183707), ('valence loss', 0.101154), ('arousal loss', 0.082552), ('valence acc', 0.986111), ('arousal acc', 0.951389)]\n",
      "Epoch 00034: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.073329), ('valence loss', 0.073329), ('arousal loss', 0.138901), ('valence acc', 0.979167), ('arousal acc', 0.951389)]\n",
      "Epoch 00039: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.07757), ('valence loss', 0.173833), ('arousal loss', 0.07757), ('valence acc', 0.951389), ('arousal acc', 0.972222)]\n",
      "\n",
      "processing:  27 ......\n",
      "Before fine-tuning: [('loss', 0.042017), ('valence loss', 0.029055), ('arousal loss', 0.012963), ('valence acc', 0.993197), ('arousal acc', 0.993197)]\n",
      "Epoch 00034: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.010597), ('valence loss', 0.001403), ('arousal loss', 0.009194), ('valence acc', 1.0), ('arousal acc', 0.993197)]\n",
      "Epoch 00117: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.002262), ('valence loss', 0.002262), ('arousal loss', 0.028241), ('valence acc', 1.0), ('arousal acc', 0.986395)]\n",
      "Epoch 00045: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.005686), ('valence loss', 0.025117), ('arousal loss', 0.005686), ('valence acc', 0.986395), ('arousal acc', 0.993197)]\n",
      "\n",
      "processing:  28 ......\n",
      "Before fine-tuning: [('loss', 0.094761), ('valence loss', 0.015709), ('arousal loss', 0.079052), ('valence acc', 0.993333), ('arousal acc', 0.966667)]\n",
      "Epoch 00030: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.067296), ('valence loss', 0.013687), ('arousal loss', 0.053609), ('valence acc', 0.993333), ('arousal acc', 0.98)]\n",
      "Epoch 00073: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.005131), ('valence loss', 0.005131), ('arousal loss', 0.079275), ('valence acc', 1.0), ('arousal acc', 0.98)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.03963), ('valence loss', 0.023867), ('arousal loss', 0.03963), ('valence acc', 0.986667), ('arousal acc', 0.986667)]\n",
      "\n",
      "processing:  29 ......\n",
      "Before fine-tuning: [('loss', 0.10115), ('valence loss', 0.05852), ('arousal loss', 0.04263), ('valence acc', 0.981818), ('arousal acc', 0.987879)]\n",
      "Epoch 00027: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.058176), ('valence loss', 0.027392), ('arousal loss', 0.030785), ('valence acc', 0.987879), ('arousal acc', 0.975758)]\n",
      "Epoch 00031: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.029809), ('valence loss', 0.029809), ('arousal loss', 0.019033), ('valence acc', 0.981818), ('arousal acc', 0.987879)]\n",
      "Epoch 00041: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.021987), ('valence loss', 0.043668), ('arousal loss', 0.021987), ('valence acc', 0.981818), ('arousal acc', 0.993939)]\n",
      "\n",
      "processing:  30 ......\n",
      "Before fine-tuning: [('loss', 0.072874), ('valence loss', 0.018872), ('arousal loss', 0.054002), ('valence acc', 0.983607), ('arousal acc', 0.989071)]\n",
      "Epoch 00033: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.065984), ('valence loss', 0.017752), ('arousal loss', 0.048232), ('valence acc', 0.994536), ('arousal acc', 0.983607)]\n",
      "Epoch 00068: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.017021), ('valence loss', 0.017021), ('arousal loss', 0.063128), ('valence acc', 0.994536), ('arousal acc', 0.967213)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.029481), ('valence loss', 0.020624), ('arousal loss', 0.029481), ('valence acc', 0.994536), ('arousal acc', 0.989071)]\n",
      "\n",
      "processing:  31 ......\n",
      "Before fine-tuning: [('loss', 0.039752), ('valence loss', 0.01646), ('arousal loss', 0.023292), ('valence acc', 0.993827), ('arousal acc', 0.987654)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.039752), ('valence loss', 0.01646), ('arousal loss', 0.023292), ('valence acc', 0.993827), ('arousal acc', 0.987654)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.016167), ('valence loss', 0.016167), ('arousal loss', 0.03895), ('valence acc', 0.993827), ('arousal acc', 0.981481)]\n",
      "Epoch 00073: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.004014), ('valence loss', 0.023652), ('arousal loss', 0.004014), ('valence acc', 0.987654), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  32 ......\n",
      "Before fine-tuning: [('loss', 0.128288), ('valence loss', 0.04494), ('arousal loss', 0.083348), ('valence acc', 0.987097), ('arousal acc', 0.987097)]\n",
      "Epoch 00036: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.093611), ('valence loss', 0.041949), ('arousal loss', 0.051662), ('valence acc', 0.987097), ('arousal acc', 0.987097)]\n",
      "Epoch 00088: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.025726), ('valence loss', 0.025726), ('arousal loss', 0.082142), ('valence acc', 0.987097), ('arousal acc', 0.987097)]\n",
      "Epoch 00043: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.038104), ('valence loss', 0.050595), ('arousal loss', 0.038104), ('valence acc', 0.980645), ('arousal acc', 0.987097)]\n",
      "\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "320/320 [==============================] - 15s 36ms/step - loss: 1.3071 - out_v_loss: 0.6660 - out_a_loss: 0.6411 - out_v_accuracy: 0.6332 - out_a_accuracy: 0.6562 - val_loss: 1.1481 - val_out_v_loss: 0.5853 - val_out_a_loss: 0.5628 - val_out_v_accuracy: 0.6873 - val_out_a_accuracy: 0.7180 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 1.0574 - out_v_loss: 0.5356 - out_a_loss: 0.5218 - out_v_accuracy: 0.7270 - out_a_accuracy: 0.7383 - val_loss: 0.9018 - val_out_v_loss: 0.4668 - val_out_a_loss: 0.4351 - val_out_v_accuracy: 0.7750 - val_out_a_accuracy: 0.7881 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.8494 - out_v_loss: 0.4279 - out_a_loss: 0.4215 - out_v_accuracy: 0.7985 - out_a_accuracy: 0.8031 - val_loss: 0.7253 - val_out_v_loss: 0.3735 - val_out_a_loss: 0.3518 - val_out_v_accuracy: 0.8264 - val_out_a_accuracy: 0.8416 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.6981 - out_v_loss: 0.3545 - out_a_loss: 0.3436 - out_v_accuracy: 0.8372 - out_a_accuracy: 0.8441 - val_loss: 0.6261 - val_out_v_loss: 0.3075 - val_out_a_loss: 0.3186 - val_out_v_accuracy: 0.8615 - val_out_a_accuracy: 0.8531 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.5976 - out_v_loss: 0.2996 - out_a_loss: 0.2980 - out_v_accuracy: 0.8656 - out_a_accuracy: 0.8657 - val_loss: 0.5388 - val_out_v_loss: 0.2760 - val_out_a_loss: 0.2628 - val_out_v_accuracy: 0.8775 - val_out_a_accuracy: 0.8824 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.5094 - out_v_loss: 0.2593 - out_a_loss: 0.2502 - out_v_accuracy: 0.8858 - out_a_accuracy: 0.8893 - val_loss: 0.4894 - val_out_v_loss: 0.2425 - val_out_a_loss: 0.2469 - val_out_v_accuracy: 0.8990 - val_out_a_accuracy: 0.8918 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.4444 - out_v_loss: 0.2216 - out_a_loss: 0.2228 - out_v_accuracy: 0.9061 - out_a_accuracy: 0.9016 - val_loss: 0.4625 - val_out_v_loss: 0.2454 - val_out_a_loss: 0.2170 - val_out_v_accuracy: 0.8979 - val_out_a_accuracy: 0.9082 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.4096 - out_v_loss: 0.2055 - out_a_loss: 0.2041 - out_v_accuracy: 0.9104 - out_a_accuracy: 0.9104 - val_loss: 0.4458 - val_out_v_loss: 0.2059 - val_out_a_loss: 0.2400 - val_out_v_accuracy: 0.9135 - val_out_a_accuracy: 0.8990 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3687 - out_v_loss: 0.1836 - out_a_loss: 0.1851 - out_v_accuracy: 0.9220 - out_a_accuracy: 0.9202 - val_loss: 0.4153 - val_out_v_loss: 0.2252 - val_out_a_loss: 0.1901 - val_out_v_accuracy: 0.9105 - val_out_a_accuracy: 0.9197 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3467 - out_v_loss: 0.1715 - out_a_loss: 0.1752 - out_v_accuracy: 0.9263 - out_a_accuracy: 0.9245 - val_loss: 0.3598 - val_out_v_loss: 0.1820 - val_out_a_loss: 0.1777 - val_out_v_accuracy: 0.9260 - val_out_a_accuracy: 0.9256 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3233 - out_v_loss: 0.1610 - out_a_loss: 0.1623 - out_v_accuracy: 0.9341 - out_a_accuracy: 0.9316 - val_loss: 0.3546 - val_out_v_loss: 0.1717 - val_out_a_loss: 0.1829 - val_out_v_accuracy: 0.9318 - val_out_a_accuracy: 0.9236 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2934 - out_v_loss: 0.1459 - out_a_loss: 0.1475 - out_v_accuracy: 0.9385 - out_a_accuracy: 0.9390 - val_loss: 0.3723 - val_out_v_loss: 0.2103 - val_out_a_loss: 0.1620 - val_out_v_accuracy: 0.9195 - val_out_a_accuracy: 0.9301 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2706 - out_v_loss: 0.1314 - out_a_loss: 0.1392 - out_v_accuracy: 0.9481 - out_a_accuracy: 0.9420 - val_loss: 0.3320 - val_out_v_loss: 0.1764 - val_out_a_loss: 0.1556 - val_out_v_accuracy: 0.9312 - val_out_a_accuracy: 0.9318 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2532 - out_v_loss: 0.1284 - out_a_loss: 0.1248 - out_v_accuracy: 0.9478 - out_a_accuracy: 0.9470 - val_loss: 0.3588 - val_out_v_loss: 0.1811 - val_out_a_loss: 0.1776 - val_out_v_accuracy: 0.9283 - val_out_a_accuracy: 0.9309 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2596 - out_v_loss: 0.1273 - out_a_loss: 0.1323 - out_v_accuracy: 0.9472 - out_a_accuracy: 0.9448 - val_loss: 0.3194 - val_out_v_loss: 0.1705 - val_out_a_loss: 0.1488 - val_out_v_accuracy: 0.9348 - val_out_a_accuracy: 0.9410 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2285 - out_v_loss: 0.1142 - out_a_loss: 0.1143 - out_v_accuracy: 0.9524 - out_a_accuracy: 0.9525 - val_loss: 0.2981 - val_out_v_loss: 0.1463 - val_out_a_loss: 0.1518 - val_out_v_accuracy: 0.9451 - val_out_a_accuracy: 0.9406 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2384 - out_v_loss: 0.1192 - out_a_loss: 0.1192 - out_v_accuracy: 0.9522 - out_a_accuracy: 0.9506 - val_loss: 0.2870 - val_out_v_loss: 0.1470 - val_out_a_loss: 0.1399 - val_out_v_accuracy: 0.9416 - val_out_a_accuracy: 0.9451 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.2102 - out_v_loss: 0.1043 - out_a_loss: 0.1059 - out_v_accuracy: 0.9586 - out_a_accuracy: 0.9560 - val_loss: 0.2839 - val_out_v_loss: 0.1453 - val_out_a_loss: 0.1386 - val_out_v_accuracy: 0.9436 - val_out_a_accuracy: 0.9441 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.2027 - out_v_loss: 0.0982 - out_a_loss: 0.1044 - out_v_accuracy: 0.9600 - out_a_accuracy: 0.9578 - val_loss: 0.2734 - val_out_v_loss: 0.1481 - val_out_a_loss: 0.1254 - val_out_v_accuracy: 0.9469 - val_out_a_accuracy: 0.9539 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1922 - out_v_loss: 0.0951 - out_a_loss: 0.0971 - out_v_accuracy: 0.9613 - out_a_accuracy: 0.9618 - val_loss: 0.2747 - val_out_v_loss: 0.1372 - val_out_a_loss: 0.1375 - val_out_v_accuracy: 0.9531 - val_out_a_accuracy: 0.9480 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1929 - out_v_loss: 0.0951 - out_a_loss: 0.0978 - out_v_accuracy: 0.9617 - out_a_accuracy: 0.9601 - val_loss: 0.2485 - val_out_v_loss: 0.1330 - val_out_a_loss: 0.1155 - val_out_v_accuracy: 0.9520 - val_out_a_accuracy: 0.9512 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1728 - out_v_loss: 0.0873 - out_a_loss: 0.0855 - out_v_accuracy: 0.9647 - out_a_accuracy: 0.9640 - val_loss: 0.2520 - val_out_v_loss: 0.1310 - val_out_a_loss: 0.1210 - val_out_v_accuracy: 0.9498 - val_out_a_accuracy: 0.9498 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1712 - out_v_loss: 0.0858 - out_a_loss: 0.0854 - out_v_accuracy: 0.9676 - out_a_accuracy: 0.9665 - val_loss: 0.2460 - val_out_v_loss: 0.1141 - val_out_a_loss: 0.1319 - val_out_v_accuracy: 0.9596 - val_out_a_accuracy: 0.9484 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1713 - out_v_loss: 0.0849 - out_a_loss: 0.0863 - out_v_accuracy: 0.9658 - out_a_accuracy: 0.9639 - val_loss: 0.2711 - val_out_v_loss: 0.1385 - val_out_a_loss: 0.1325 - val_out_v_accuracy: 0.9523 - val_out_a_accuracy: 0.9514 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1650 - out_v_loss: 0.0819 - out_a_loss: 0.0830 - out_v_accuracy: 0.9677 - out_a_accuracy: 0.9685 - val_loss: 0.2672 - val_out_v_loss: 0.1349 - val_out_a_loss: 0.1323 - val_out_v_accuracy: 0.9537 - val_out_a_accuracy: 0.9500 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1490 - out_v_loss: 0.0757 - out_a_loss: 0.0733 - out_v_accuracy: 0.9694 - out_a_accuracy: 0.9708 - val_loss: 0.2546 - val_out_v_loss: 0.1255 - val_out_a_loss: 0.1291 - val_out_v_accuracy: 0.9570 - val_out_a_accuracy: 0.9535 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1535 - out_v_loss: 0.0777 - out_a_loss: 0.0758 - out_v_accuracy: 0.9688 - out_a_accuracy: 0.9712 - val_loss: 0.2763 - val_out_v_loss: 0.1446 - val_out_a_loss: 0.1317 - val_out_v_accuracy: 0.9488 - val_out_a_accuracy: 0.9527 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.1531 - out_v_loss: 0.0738 - out_a_loss: 0.0792 - out_v_accuracy: 0.9718 - out_a_accuracy: 0.9674\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1527 - out_v_loss: 0.0737 - out_a_loss: 0.0790 - out_v_accuracy: 0.9719 - out_a_accuracy: 0.9675 - val_loss: 0.2560 - val_out_v_loss: 0.1286 - val_out_a_loss: 0.1274 - val_out_v_accuracy: 0.9555 - val_out_a_accuracy: 0.9547 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1109 - out_v_loss: 0.0529 - out_a_loss: 0.0580 - out_v_accuracy: 0.9793 - out_a_accuracy: 0.9779 - val_loss: 0.1988 - val_out_v_loss: 0.1010 - val_out_a_loss: 0.0978 - val_out_v_accuracy: 0.9670 - val_out_a_accuracy: 0.9641 - lr: 5.0000e-04\n",
      "Epoch 30/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0814 - out_v_loss: 0.0393 - out_a_loss: 0.0421 - out_v_accuracy: 0.9842 - out_a_accuracy: 0.9831 - val_loss: 0.1913 - val_out_v_loss: 0.0932 - val_out_a_loss: 0.0981 - val_out_v_accuracy: 0.9699 - val_out_a_accuracy: 0.9654 - lr: 5.0000e-04\n",
      "Epoch 31/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0843 - out_v_loss: 0.0407 - out_a_loss: 0.0435 - out_v_accuracy: 0.9843 - out_a_accuracy: 0.9831 - val_loss: 0.2185 - val_out_v_loss: 0.1143 - val_out_a_loss: 0.1042 - val_out_v_accuracy: 0.9615 - val_out_a_accuracy: 0.9623 - lr: 5.0000e-04\n",
      "Epoch 32/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0767 - out_v_loss: 0.0387 - out_a_loss: 0.0380 - out_v_accuracy: 0.9849 - out_a_accuracy: 0.9845 - val_loss: 0.2061 - val_out_v_loss: 0.0978 - val_out_a_loss: 0.1083 - val_out_v_accuracy: 0.9668 - val_out_a_accuracy: 0.9621 - lr: 5.0000e-04\n",
      "Epoch 33/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0764 - out_v_loss: 0.0359 - out_a_loss: 0.0405 - out_v_accuracy: 0.9867 - out_a_accuracy: 0.9839 - val_loss: 0.2108 - val_out_v_loss: 0.0974 - val_out_a_loss: 0.1134 - val_out_v_accuracy: 0.9688 - val_out_a_accuracy: 0.9645 - lr: 5.0000e-04\n",
      "Epoch 34/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0750 - out_v_loss: 0.0372 - out_a_loss: 0.0378 - out_v_accuracy: 0.9853 - out_a_accuracy: 0.9851 - val_loss: 0.1873 - val_out_v_loss: 0.0885 - val_out_a_loss: 0.0989 - val_out_v_accuracy: 0.9691 - val_out_a_accuracy: 0.9656 - lr: 5.0000e-04\n",
      "Epoch 35/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0723 - out_v_loss: 0.0349 - out_a_loss: 0.0375 - out_v_accuracy: 0.9864 - out_a_accuracy: 0.9857 - val_loss: 0.1884 - val_out_v_loss: 0.0952 - val_out_a_loss: 0.0932 - val_out_v_accuracy: 0.9646 - val_out_a_accuracy: 0.9654 - lr: 5.0000e-04\n",
      "Epoch 36/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0783 - out_v_loss: 0.0368 - out_a_loss: 0.0415 - out_v_accuracy: 0.9851 - out_a_accuracy: 0.9835 - val_loss: 0.1952 - val_out_v_loss: 0.0999 - val_out_a_loss: 0.0953 - val_out_v_accuracy: 0.9674 - val_out_a_accuracy: 0.9684 - lr: 5.0000e-04\n",
      "Epoch 37/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0746 - out_v_loss: 0.0364 - out_a_loss: 0.0382 - out_v_accuracy: 0.9860 - out_a_accuracy: 0.9853 - val_loss: 0.1834 - val_out_v_loss: 0.0952 - val_out_a_loss: 0.0882 - val_out_v_accuracy: 0.9678 - val_out_a_accuracy: 0.9668 - lr: 5.0000e-04\n",
      "Epoch 38/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0679 - out_v_loss: 0.0320 - out_a_loss: 0.0359 - out_v_accuracy: 0.9879 - out_a_accuracy: 0.9861 - val_loss: 0.1828 - val_out_v_loss: 0.0968 - val_out_a_loss: 0.0860 - val_out_v_accuracy: 0.9689 - val_out_a_accuracy: 0.9688 - lr: 5.0000e-04\n",
      "Epoch 39/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0641 - out_v_loss: 0.0326 - out_a_loss: 0.0315 - out_v_accuracy: 0.9871 - out_a_accuracy: 0.9878 - val_loss: 0.1989 - val_out_v_loss: 0.1035 - val_out_a_loss: 0.0954 - val_out_v_accuracy: 0.9666 - val_out_a_accuracy: 0.9676 - lr: 5.0000e-04\n",
      "Epoch 40/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0760 - out_v_loss: 0.0355 - out_a_loss: 0.0405 - out_v_accuracy: 0.9865 - out_a_accuracy: 0.9846 - val_loss: 0.1927 - val_out_v_loss: 0.1046 - val_out_a_loss: 0.0881 - val_out_v_accuracy: 0.9654 - val_out_a_accuracy: 0.9697 - lr: 5.0000e-04\n",
      "Epoch 41/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0677 - out_v_loss: 0.0311 - out_a_loss: 0.0365 - out_v_accuracy: 0.9881 - out_a_accuracy: 0.9853 - val_loss: 0.1905 - val_out_v_loss: 0.0966 - val_out_a_loss: 0.0939 - val_out_v_accuracy: 0.9674 - val_out_a_accuracy: 0.9689 - lr: 5.0000e-04\n",
      "Epoch 42/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0730 - out_v_loss: 0.0358 - out_a_loss: 0.0371 - out_v_accuracy: 0.9860 - out_a_accuracy: 0.9866 - val_loss: 0.1970 - val_out_v_loss: 0.1049 - val_out_a_loss: 0.0921 - val_out_v_accuracy: 0.9674 - val_out_a_accuracy: 0.9670 - lr: 5.0000e-04\n",
      "Epoch 43/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0676 - out_v_loss: 0.0332 - out_a_loss: 0.0344 - out_v_accuracy: 0.9875 - out_a_accuracy: 0.9864\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0674 - out_v_loss: 0.0331 - out_a_loss: 0.0343 - out_v_accuracy: 0.9875 - out_a_accuracy: 0.9864 - val_loss: 0.1837 - val_out_v_loss: 0.0945 - val_out_a_loss: 0.0892 - val_out_v_accuracy: 0.9680 - val_out_a_accuracy: 0.9705 - lr: 5.0000e-04\n",
      "Epoch 44/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0537 - out_v_loss: 0.0279 - out_a_loss: 0.0257 - out_v_accuracy: 0.9899 - out_a_accuracy: 0.9898 - val_loss: 0.1825 - val_out_v_loss: 0.0965 - val_out_a_loss: 0.0860 - val_out_v_accuracy: 0.9688 - val_out_a_accuracy: 0.9717 - lr: 2.5000e-04\n",
      "Epoch 45/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0417 - out_v_loss: 0.0207 - out_a_loss: 0.0210 - out_v_accuracy: 0.9916 - out_a_accuracy: 0.9927 - val_loss: 0.1850 - val_out_v_loss: 0.0979 - val_out_a_loss: 0.0870 - val_out_v_accuracy: 0.9693 - val_out_a_accuracy: 0.9707 - lr: 2.5000e-04\n",
      "Epoch 46/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0417 - out_v_loss: 0.0208 - out_a_loss: 0.0209 - out_v_accuracy: 0.9925 - out_a_accuracy: 0.9914 - val_loss: 0.1928 - val_out_v_loss: 0.1000 - val_out_a_loss: 0.0928 - val_out_v_accuracy: 0.9689 - val_out_a_accuracy: 0.9697 - lr: 2.5000e-04\n",
      "Epoch 47/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0371 - out_v_loss: 0.0182 - out_a_loss: 0.0189 - out_v_accuracy: 0.9930 - out_a_accuracy: 0.9935 - val_loss: 0.1936 - val_out_v_loss: 0.1059 - val_out_a_loss: 0.0877 - val_out_v_accuracy: 0.9695 - val_out_a_accuracy: 0.9717 - lr: 2.5000e-04\n",
      "Epoch 48/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0372 - out_v_loss: 0.0177 - out_a_loss: 0.0195 - out_v_accuracy: 0.9934 - out_a_accuracy: 0.9930 - val_loss: 0.1975 - val_out_v_loss: 0.1088 - val_out_a_loss: 0.0887 - val_out_v_accuracy: 0.9676 - val_out_a_accuracy: 0.9709 - lr: 2.5000e-04\n",
      "Epoch 49/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0401 - out_v_loss: 0.0204 - out_a_loss: 0.0197 - out_v_accuracy: 0.9920 - out_a_accuracy: 0.9926 - val_loss: 0.1776 - val_out_v_loss: 0.0920 - val_out_a_loss: 0.0856 - val_out_v_accuracy: 0.9717 - val_out_a_accuracy: 0.9730 - lr: 2.5000e-04\n",
      "Epoch 50/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0362 - out_v_loss: 0.0189 - out_a_loss: 0.0173 - out_v_accuracy: 0.9925 - out_a_accuracy: 0.9936 - val_loss: 0.1883 - val_out_v_loss: 0.0961 - val_out_a_loss: 0.0922 - val_out_v_accuracy: 0.9723 - val_out_a_accuracy: 0.9727 - lr: 2.5000e-04\n",
      "Epoch 51/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0387 - out_v_loss: 0.0191 - out_a_loss: 0.0196 - out_v_accuracy: 0.9930 - out_a_accuracy: 0.9923 - val_loss: 0.1761 - val_out_v_loss: 0.0945 - val_out_a_loss: 0.0816 - val_out_v_accuracy: 0.9717 - val_out_a_accuracy: 0.9744 - lr: 2.5000e-04\n",
      "Epoch 52/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0391 - out_v_loss: 0.0200 - out_a_loss: 0.0191 - out_v_accuracy: 0.9921 - out_a_accuracy: 0.9934 - val_loss: 0.1801 - val_out_v_loss: 0.0966 - val_out_a_loss: 0.0835 - val_out_v_accuracy: 0.9703 - val_out_a_accuracy: 0.9734 - lr: 2.5000e-04\n",
      "Epoch 53/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0351 - out_v_loss: 0.0155 - out_a_loss: 0.0196 - out_v_accuracy: 0.9939 - out_a_accuracy: 0.9935 - val_loss: 0.1710 - val_out_v_loss: 0.0857 - val_out_a_loss: 0.0852 - val_out_v_accuracy: 0.9760 - val_out_a_accuracy: 0.9729 - lr: 2.5000e-04\n",
      "Epoch 54/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0342 - out_v_loss: 0.0166 - out_a_loss: 0.0176 - out_v_accuracy: 0.9935 - out_a_accuracy: 0.9934 - val_loss: 0.1674 - val_out_v_loss: 0.0880 - val_out_a_loss: 0.0794 - val_out_v_accuracy: 0.9732 - val_out_a_accuracy: 0.9717 - lr: 2.5000e-04\n",
      "Epoch 55/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0345 - out_v_loss: 0.0166 - out_a_loss: 0.0179 - out_v_accuracy: 0.9939 - out_a_accuracy: 0.9930 - val_loss: 0.1760 - val_out_v_loss: 0.0874 - val_out_a_loss: 0.0886 - val_out_v_accuracy: 0.9746 - val_out_a_accuracy: 0.9732 - lr: 2.5000e-04\n",
      "Epoch 56/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0351 - out_v_loss: 0.0165 - out_a_loss: 0.0186 - out_v_accuracy: 0.9937 - out_a_accuracy: 0.9929 - val_loss: 0.1649 - val_out_v_loss: 0.0890 - val_out_a_loss: 0.0760 - val_out_v_accuracy: 0.9730 - val_out_a_accuracy: 0.9758 - lr: 2.5000e-04\n",
      "Epoch 57/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0352 - out_v_loss: 0.0163 - out_a_loss: 0.0188 - out_v_accuracy: 0.9934 - out_a_accuracy: 0.9925 - val_loss: 0.1558 - val_out_v_loss: 0.0804 - val_out_a_loss: 0.0754 - val_out_v_accuracy: 0.9768 - val_out_a_accuracy: 0.9760 - lr: 2.5000e-04\n",
      "Epoch 58/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0334 - out_v_loss: 0.0162 - out_a_loss: 0.0172 - out_v_accuracy: 0.9940 - out_a_accuracy: 0.9932 - val_loss: 0.1628 - val_out_v_loss: 0.0810 - val_out_a_loss: 0.0818 - val_out_v_accuracy: 0.9756 - val_out_a_accuracy: 0.9750 - lr: 2.5000e-04\n",
      "Epoch 59/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0333 - out_v_loss: 0.0169 - out_a_loss: 0.0164 - out_v_accuracy: 0.9932 - out_a_accuracy: 0.9937 - val_loss: 0.1759 - val_out_v_loss: 0.0944 - val_out_a_loss: 0.0815 - val_out_v_accuracy: 0.9740 - val_out_a_accuracy: 0.9758 - lr: 2.5000e-04\n",
      "Epoch 60/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0333 - out_v_loss: 0.0175 - out_a_loss: 0.0158 - out_v_accuracy: 0.9939 - out_a_accuracy: 0.9938 - val_loss: 0.1883 - val_out_v_loss: 0.1024 - val_out_a_loss: 0.0859 - val_out_v_accuracy: 0.9723 - val_out_a_accuracy: 0.9738 - lr: 2.5000e-04\n",
      "Epoch 61/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0350 - out_v_loss: 0.0164 - out_a_loss: 0.0186 - out_v_accuracy: 0.9937 - out_a_accuracy: 0.9933 - val_loss: 0.1748 - val_out_v_loss: 0.0956 - val_out_a_loss: 0.0792 - val_out_v_accuracy: 0.9748 - val_out_a_accuracy: 0.9768 - lr: 2.5000e-04\n",
      "Epoch 62/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0315 - out_v_loss: 0.0158 - out_a_loss: 0.0158 - out_v_accuracy: 0.9940 - out_a_accuracy: 0.9940\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0316 - out_v_loss: 0.0158 - out_a_loss: 0.0158 - out_v_accuracy: 0.9939 - out_a_accuracy: 0.9940 - val_loss: 0.1729 - val_out_v_loss: 0.0940 - val_out_a_loss: 0.0789 - val_out_v_accuracy: 0.9736 - val_out_a_accuracy: 0.9752 - lr: 2.5000e-04\n",
      "Epoch 63/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0255 - out_v_loss: 0.0130 - out_a_loss: 0.0125 - out_v_accuracy: 0.9954 - out_a_accuracy: 0.9953 - val_loss: 0.1545 - val_out_v_loss: 0.0827 - val_out_a_loss: 0.0718 - val_out_v_accuracy: 0.9770 - val_out_a_accuracy: 0.9766 - lr: 1.2500e-04\n",
      "Epoch 64/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0274 - out_v_loss: 0.0134 - out_a_loss: 0.0140 - out_v_accuracy: 0.9947 - out_a_accuracy: 0.9956 - val_loss: 0.1588 - val_out_v_loss: 0.0861 - val_out_a_loss: 0.0727 - val_out_v_accuracy: 0.9750 - val_out_a_accuracy: 0.9775 - lr: 1.2500e-04\n",
      "Epoch 65/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0222 - out_v_loss: 0.0122 - out_a_loss: 0.0100 - out_v_accuracy: 0.9953 - out_a_accuracy: 0.9967 - val_loss: 0.1584 - val_out_v_loss: 0.0882 - val_out_a_loss: 0.0703 - val_out_v_accuracy: 0.9748 - val_out_a_accuracy: 0.9770 - lr: 1.2500e-04\n",
      "Epoch 66/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0228 - out_v_loss: 0.0127 - out_a_loss: 0.0101 - out_v_accuracy: 0.9947 - out_a_accuracy: 0.9965 - val_loss: 0.1646 - val_out_v_loss: 0.0892 - val_out_a_loss: 0.0754 - val_out_v_accuracy: 0.9738 - val_out_a_accuracy: 0.9764 - lr: 1.2500e-04\n",
      "Epoch 67/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0218 - out_v_loss: 0.0110 - out_a_loss: 0.0108 - out_v_accuracy: 0.9956 - out_a_accuracy: 0.9963 - val_loss: 0.1591 - val_out_v_loss: 0.0833 - val_out_a_loss: 0.0759 - val_out_v_accuracy: 0.9768 - val_out_a_accuracy: 0.9773 - lr: 1.2500e-04\n",
      "Epoch 68/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0247 - out_v_loss: 0.0132 - out_a_loss: 0.0115 - out_v_accuracy: 0.9952 - out_a_accuracy: 0.9956\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0247 - out_v_loss: 0.0132 - out_a_loss: 0.0115 - out_v_accuracy: 0.9952 - out_a_accuracy: 0.9956 - val_loss: 0.1597 - val_out_v_loss: 0.0839 - val_out_a_loss: 0.0758 - val_out_v_accuracy: 0.9783 - val_out_a_accuracy: 0.9768 - lr: 1.2500e-04\n",
      "Epoch 69/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0185 - out_v_loss: 0.0104 - out_a_loss: 0.0081 - out_v_accuracy: 0.9962 - out_a_accuracy: 0.9969 - val_loss: 0.1454 - val_out_v_loss: 0.0802 - val_out_a_loss: 0.0651 - val_out_v_accuracy: 0.9773 - val_out_a_accuracy: 0.9791 - lr: 6.2500e-05\n",
      "Epoch 70/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0198 - out_v_loss: 0.0101 - out_a_loss: 0.0097 - out_v_accuracy: 0.9963 - out_a_accuracy: 0.9967 - val_loss: 0.1562 - val_out_v_loss: 0.0842 - val_out_a_loss: 0.0719 - val_out_v_accuracy: 0.9756 - val_out_a_accuracy: 0.9770 - lr: 6.2500e-05\n",
      "Epoch 71/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0209 - out_v_loss: 0.0104 - out_a_loss: 0.0105 - out_v_accuracy: 0.9962 - out_a_accuracy: 0.9963 - val_loss: 0.1550 - val_out_v_loss: 0.0845 - val_out_a_loss: 0.0705 - val_out_v_accuracy: 0.9762 - val_out_a_accuracy: 0.9775 - lr: 6.2500e-05\n",
      "Epoch 72/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0191 - out_v_loss: 0.0093 - out_a_loss: 0.0098 - out_v_accuracy: 0.9967 - out_a_accuracy: 0.9966 - val_loss: 0.1558 - val_out_v_loss: 0.0856 - val_out_a_loss: 0.0703 - val_out_v_accuracy: 0.9770 - val_out_a_accuracy: 0.9785 - lr: 6.2500e-05\n",
      "Epoch 73/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0178 - out_v_loss: 0.0079 - out_a_loss: 0.0099 - out_v_accuracy: 0.9976 - out_a_accuracy: 0.9963 - val_loss: 0.1431 - val_out_v_loss: 0.0764 - val_out_a_loss: 0.0667 - val_out_v_accuracy: 0.9793 - val_out_a_accuracy: 0.9787 - lr: 6.2500e-05\n",
      "Epoch 74/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0201 - out_v_loss: 0.0107 - out_a_loss: 0.0094 - out_v_accuracy: 0.9957 - out_a_accuracy: 0.9965 - val_loss: 0.1418 - val_out_v_loss: 0.0774 - val_out_a_loss: 0.0644 - val_out_v_accuracy: 0.9791 - val_out_a_accuracy: 0.9791 - lr: 6.2500e-05\n",
      "Epoch 75/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0165 - out_v_loss: 0.0087 - out_a_loss: 0.0078 - out_v_accuracy: 0.9972 - out_a_accuracy: 0.9972 - val_loss: 0.1515 - val_out_v_loss: 0.0845 - val_out_a_loss: 0.0670 - val_out_v_accuracy: 0.9754 - val_out_a_accuracy: 0.9795 - lr: 6.2500e-05\n",
      "Epoch 76/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0179 - out_v_loss: 0.0097 - out_a_loss: 0.0082 - out_v_accuracy: 0.9964 - out_a_accuracy: 0.9965 - val_loss: 0.1546 - val_out_v_loss: 0.0853 - val_out_a_loss: 0.0694 - val_out_v_accuracy: 0.9754 - val_out_a_accuracy: 0.9787 - lr: 6.2500e-05\n",
      "Epoch 77/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0168 - out_v_loss: 0.0081 - out_a_loss: 0.0087 - out_v_accuracy: 0.9974 - out_a_accuracy: 0.9973 - val_loss: 0.1515 - val_out_v_loss: 0.0833 - val_out_a_loss: 0.0682 - val_out_v_accuracy: 0.9758 - val_out_a_accuracy: 0.9771 - lr: 6.2500e-05\n",
      "Epoch 78/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0162 - out_v_loss: 0.0076 - out_a_loss: 0.0086 - out_v_accuracy: 0.9970 - out_a_accuracy: 0.9971 - val_loss: 0.1499 - val_out_v_loss: 0.0787 - val_out_a_loss: 0.0712 - val_out_v_accuracy: 0.9775 - val_out_a_accuracy: 0.9787 - lr: 6.2500e-05\n",
      "Epoch 79/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0176 - out_v_loss: 0.0084 - out_a_loss: 0.0091 - out_v_accuracy: 0.9970 - out_a_accuracy: 0.9966\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0176 - out_v_loss: 0.0085 - out_a_loss: 0.0091 - out_v_accuracy: 0.9970 - out_a_accuracy: 0.9966 - val_loss: 0.1502 - val_out_v_loss: 0.0813 - val_out_a_loss: 0.0689 - val_out_v_accuracy: 0.9777 - val_out_a_accuracy: 0.9781 - lr: 6.2500e-05\n",
      "Epoch 80/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0168 - out_v_loss: 0.0085 - out_a_loss: 0.0083 - out_v_accuracy: 0.9970 - out_a_accuracy: 0.9969 - val_loss: 0.1491 - val_out_v_loss: 0.0789 - val_out_a_loss: 0.0702 - val_out_v_accuracy: 0.9779 - val_out_a_accuracy: 0.9781 - lr: 3.1250e-05\n",
      "Epoch 81/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0146 - out_v_loss: 0.0070 - out_a_loss: 0.0076 - out_v_accuracy: 0.9975 - out_a_accuracy: 0.9975 - val_loss: 0.1460 - val_out_v_loss: 0.0798 - val_out_a_loss: 0.0663 - val_out_v_accuracy: 0.9777 - val_out_a_accuracy: 0.9791 - lr: 3.1250e-05\n",
      "Epoch 82/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0136 - out_v_loss: 0.0067 - out_a_loss: 0.0069 - out_v_accuracy: 0.9978 - out_a_accuracy: 0.9979 - val_loss: 0.1464 - val_out_v_loss: 0.0802 - val_out_a_loss: 0.0662 - val_out_v_accuracy: 0.9785 - val_out_a_accuracy: 0.9812 - lr: 3.1250e-05\n",
      "Epoch 83/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0155 - out_v_loss: 0.0078 - out_a_loss: 0.0077 - out_v_accuracy: 0.9972 - out_a_accuracy: 0.9969 - val_loss: 0.1466 - val_out_v_loss: 0.0806 - val_out_a_loss: 0.0660 - val_out_v_accuracy: 0.9783 - val_out_a_accuracy: 0.9814 - lr: 3.1250e-05\n",
      "Epoch 84/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0140 - out_v_loss: 0.0070 - out_a_loss: 0.0071 - out_v_accuracy: 0.9972 - out_a_accuracy: 0.9975\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0140 - out_v_loss: 0.0069 - out_a_loss: 0.0071 - out_v_accuracy: 0.9972 - out_a_accuracy: 0.9975 - val_loss: 0.1516 - val_out_v_loss: 0.0852 - val_out_a_loss: 0.0664 - val_out_v_accuracy: 0.9766 - val_out_a_accuracy: 0.9809 - lr: 3.1250e-05\n",
      "Epoch 85/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0165 - out_v_loss: 0.0079 - out_a_loss: 0.0085 - out_v_accuracy: 0.9966 - out_a_accuracy: 0.9969 - val_loss: 0.1449 - val_out_v_loss: 0.0789 - val_out_a_loss: 0.0660 - val_out_v_accuracy: 0.9783 - val_out_a_accuracy: 0.9799 - lr: 1.5625e-05\n",
      "Epoch 86/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0140 - out_v_loss: 0.0073 - out_a_loss: 0.0067 - out_v_accuracy: 0.9977 - out_a_accuracy: 0.9976 - val_loss: 0.1490 - val_out_v_loss: 0.0819 - val_out_a_loss: 0.0671 - val_out_v_accuracy: 0.9771 - val_out_a_accuracy: 0.9797 - lr: 1.5625e-05\n",
      "Epoch 87/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0128 - out_v_loss: 0.0062 - out_a_loss: 0.0066 - out_v_accuracy: 0.9978 - out_a_accuracy: 0.9976 - val_loss: 0.1464 - val_out_v_loss: 0.0824 - val_out_a_loss: 0.0641 - val_out_v_accuracy: 0.9768 - val_out_a_accuracy: 0.9816 - lr: 1.5625e-05\n",
      "Epoch 88/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0158 - out_v_loss: 0.0092 - out_a_loss: 0.0066 - out_v_accuracy: 0.9969 - out_a_accuracy: 0.9982 - val_loss: 0.1535 - val_out_v_loss: 0.0871 - val_out_a_loss: 0.0663 - val_out_v_accuracy: 0.9764 - val_out_a_accuracy: 0.9801 - lr: 1.5625e-05\n",
      "Epoch 89/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0144 - out_v_loss: 0.0075 - out_a_loss: 0.0069 - out_v_accuracy: 0.9974 - out_a_accuracy: 0.9977\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0144 - out_v_loss: 0.0076 - out_a_loss: 0.0069 - out_v_accuracy: 0.9974 - out_a_accuracy: 0.9978 - val_loss: 0.1470 - val_out_v_loss: 0.0830 - val_out_a_loss: 0.0640 - val_out_v_accuracy: 0.9771 - val_out_a_accuracy: 0.9812 - lr: 1.5625e-05\n",
      "Epoch 90/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0170 - out_v_loss: 0.0081 - out_a_loss: 0.0089 - out_v_accuracy: 0.9968 - out_a_accuracy: 0.9967 - val_loss: 0.1470 - val_out_v_loss: 0.0831 - val_out_a_loss: 0.0639 - val_out_v_accuracy: 0.9766 - val_out_a_accuracy: 0.9811 - lr: 7.8125e-06\n",
      "Epoch 00090: early stopping\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.1470 - out_v_loss: 0.0831 - out_a_loss: 0.0639 - out_v_accuracy: 0.9766 - out_a_accuracy: 0.9811\n",
      "\n",
      "processing:  01 ......\n",
      "Before fine-tuning: [('loss', 0.021498), ('valence loss', 0.003498), ('arousal loss', 0.018), ('valence acc', 1.0), ('arousal acc', 0.99422)]\n",
      "Epoch 00064: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.013055), ('valence loss', 0.004872), ('arousal loss', 0.008183), ('valence acc', 0.99422), ('arousal acc', 0.99422)]\n",
      "Epoch 00026: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.002616), ('valence loss', 0.002616), ('arousal loss', 0.026694), ('valence acc', 1.0), ('arousal acc', 0.99422)]\n",
      "Epoch 00047: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.006931), ('valence loss', 0.005442), ('arousal loss', 0.006931), ('valence acc', 1.0), ('arousal acc', 0.99422)]\n",
      "\n",
      "processing:  02 ......\n",
      "Before fine-tuning: [('loss', 0.335984), ('valence loss', 0.174885), ('arousal loss', 0.161099), ('valence acc', 0.975155), ('arousal acc', 0.950311)]\n",
      "Epoch 00022: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.281168), ('valence loss', 0.175273), ('arousal loss', 0.105895), ('valence acc', 0.968944), ('arousal acc', 0.981366)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.150978), ('valence loss', 0.150978), ('arousal loss', 0.116443), ('valence acc', 0.981366), ('arousal acc', 0.975155)]\n",
      "Epoch 00026: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.101018), ('valence loss', 0.147369), ('arousal loss', 0.101018), ('valence acc', 0.968944), ('arousal acc', 0.968944)]\n",
      "\n",
      "processing:  03 ......\n",
      "Before fine-tuning: [('loss', 0.06312), ('valence loss', 0.06181), ('arousal loss', 0.001309), ('valence acc', 0.975758), ('arousal acc', 1.0)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.06312), ('valence loss', 0.06181), ('arousal loss', 0.001309), ('valence acc', 0.975758), ('arousal acc', 1.0)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.041093), ('valence loss', 0.041093), ('arousal loss', 0.122607), ('valence acc', 0.987879), ('arousal acc', 0.963636)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.001309), ('valence loss', 0.06181), ('arousal loss', 0.001309), ('valence acc', 0.975758), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  04 ......\n",
      "Before fine-tuning: [('loss', 0.218773), ('valence loss', 0.125578), ('arousal loss', 0.093195), ('valence acc', 0.955975), ('arousal acc', 0.968553)]\n",
      "Epoch 00042: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.218773), ('valence loss', 0.125578), ('arousal loss', 0.093195), ('valence acc', 0.955975), ('arousal acc', 0.968553)]\n",
      "Epoch 00020: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.10852), ('valence loss', 0.10852), ('arousal loss', 0.154558), ('valence acc', 0.962264), ('arousal acc', 0.943396)]\n",
      "Epoch 00036: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.087245), ('valence loss', 0.149138), ('arousal loss', 0.087245), ('valence acc', 0.955975), ('arousal acc', 0.968553)]\n",
      "\n",
      "processing:  05 ......\n",
      "Before fine-tuning: [('loss', 0.229968), ('valence loss', 0.171284), ('arousal loss', 0.058684), ('valence acc', 0.947368), ('arousal acc', 0.960526)]\n",
      "Epoch 00031: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.142293), ('valence loss', 0.099758), ('arousal loss', 0.042535), ('valence acc', 0.953947), ('arousal acc', 0.967105)]\n",
      "Epoch 00060: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.066286), ('valence loss', 0.066286), ('arousal loss', 0.100319), ('valence acc', 0.973684), ('arousal acc', 0.953947)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.028918), ('valence loss', 0.124754), ('arousal loss', 0.028918), ('valence acc', 0.960526), ('arousal acc', 0.986842)]\n",
      "\n",
      "processing:  06 ......\n",
      "Before fine-tuning: [('loss', 0.033158), ('valence loss', 0.000512), ('arousal loss', 0.032645), ('valence acc', 1.0), ('arousal acc', 0.986014)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.033158), ('valence loss', 0.000512), ('arousal loss', 0.032645), ('valence acc', 1.0), ('arousal acc', 0.986014)]\n",
      "Epoch 00043: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.000306), ('valence loss', 0.000306), ('arousal loss', 0.171915), ('valence acc', 1.0), ('arousal acc', 0.965035)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.032645), ('valence loss', 0.000512), ('arousal loss', 0.032645), ('valence acc', 1.0), ('arousal acc', 0.986014)]\n",
      "\n",
      "processing:  07 ......\n",
      "Before fine-tuning: [('loss', 0.027155), ('valence loss', 0.011694), ('arousal loss', 0.015461), ('valence acc', 0.993939), ('arousal acc', 0.993939)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.027155), ('valence loss', 0.011694), ('arousal loss', 0.015461), ('valence acc', 0.993939), ('arousal acc', 0.993939)]\n",
      "Epoch 00021: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.011694), ('valence loss', 0.011694), ('arousal loss', 0.015461), ('valence acc', 0.993939), ('arousal acc', 0.993939)]\n",
      "Epoch 00019: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.015461), ('valence loss', 0.011694), ('arousal loss', 0.015461), ('valence acc', 0.993939), ('arousal acc', 0.993939)]\n",
      "\n",
      "processing:  08 ......\n",
      "Before fine-tuning: [('loss', 0.016771), ('valence loss', 0.014205), ('arousal loss', 0.002566), ('valence acc', 0.992958), ('arousal acc', 1.0)]\n",
      "Epoch 00032: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.00677), ('valence loss', 0.004411), ('arousal loss', 0.002359), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "Epoch 00072: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.001797), ('valence loss', 0.001797), ('arousal loss', 0.00078), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.002227), ('valence loss', 0.021232), ('arousal loss', 0.002227), ('valence acc', 0.985915), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  09 ......\n",
      "Before fine-tuning: [('loss', 0.238765), ('valence loss', 0.0706), ('arousal loss', 0.168165), ('valence acc', 0.973333), ('arousal acc', 0.94)]\n",
      "Epoch 00019: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.231567), ('valence loss', 0.082007), ('arousal loss', 0.14956), ('valence acc', 0.966667), ('arousal acc', 0.946667)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.066553), ('valence loss', 0.066553), ('arousal loss', 0.134069), ('valence acc', 0.973333), ('arousal acc', 0.953333)]\n",
      "Epoch 00069: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.088878), ('valence loss', 0.080753), ('arousal loss', 0.088878), ('valence acc', 0.973333), ('arousal acc', 0.973333)]\n",
      "\n",
      "processing:  10 ......\n",
      "Before fine-tuning: [('loss', 0.129044), ('valence loss', 0.120737), ('arousal loss', 0.008307), ('valence acc', 0.974843), ('arousal acc', 0.993711)]\n",
      "Epoch 00032: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.09722), ('valence loss', 0.091962), ('arousal loss', 0.005258), ('valence acc', 0.987421), ('arousal acc', 1.0)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.100655), ('valence loss', 0.100655), ('arousal loss', 0.007101), ('valence acc', 0.981132), ('arousal acc', 0.993711)]\n",
      "Epoch 00042: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.000836), ('valence loss', 0.151285), ('arousal loss', 0.000836), ('valence acc', 0.974843), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  11 ......\n",
      "Before fine-tuning: [('loss', 0.136647), ('valence loss', 0.082791), ('arousal loss', 0.053856), ('valence acc', 0.97006), ('arousal acc', 0.976048)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.115349), ('valence loss', 0.082786), ('arousal loss', 0.032563), ('valence acc', 0.982036), ('arousal acc', 0.988024)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.081673), ('valence loss', 0.081673), ('arousal loss', 0.025556), ('valence acc', 0.97006), ('arousal acc', 0.994012)]\n",
      "Epoch 00038: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.024274), ('valence loss', 0.14853), ('arousal loss', 0.024274), ('valence acc', 0.952096), ('arousal acc', 0.988024)]\n",
      "\n",
      "processing:  12 ......\n",
      "Before fine-tuning: [('loss', 0.075929), ('valence loss', 0.045765), ('arousal loss', 0.030165), ('valence acc', 0.988372), ('arousal acc', 0.988372)]\n",
      "Epoch 00074: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.075929), ('valence loss', 0.045765), ('arousal loss', 0.030165), ('valence acc', 0.988372), ('arousal acc', 0.988372)]\n",
      "Epoch 00019: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.039228), ('valence loss', 0.039228), ('arousal loss', 0.099866), ('valence acc', 0.982558), ('arousal acc', 0.97093)]\n",
      "Epoch 00104: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.017252), ('valence loss', 0.073768), ('arousal loss', 0.017252), ('valence acc', 0.982558), ('arousal acc', 0.988372)]\n",
      "\n",
      "processing:  13 ......\n",
      "Before fine-tuning: [('loss', 0.109642), ('valence loss', 0.066059), ('arousal loss', 0.043583), ('valence acc', 0.983696), ('arousal acc', 0.994565)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.101819), ('valence loss', 0.068402), ('arousal loss', 0.033417), ('valence acc', 0.983696), ('arousal acc', 0.994565)]\n",
      "Epoch 00046: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.066059), ('valence loss', 0.066059), ('arousal loss', 0.043583), ('valence acc', 0.983696), ('arousal acc', 0.994565)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.035132), ('valence loss', 0.069995), ('arousal loss', 0.035132), ('valence acc', 0.983696), ('arousal acc', 0.98913)]\n",
      "\n",
      "processing:  14 ......\n",
      "Before fine-tuning: [('loss', 0.118274), ('valence loss', 0.04365), ('arousal loss', 0.074624), ('valence acc', 0.979866), ('arousal acc', 0.979866)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.104434), ('valence loss', 0.030051), ('arousal loss', 0.074383), ('valence acc', 0.986577), ('arousal acc', 0.986577)]\n",
      "Epoch 00064: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.0207), ('valence loss', 0.0207), ('arousal loss', 0.095893), ('valence acc', 0.986577), ('arousal acc', 0.986577)]\n",
      "Epoch 00066: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.070745), ('valence loss', 0.033252), ('arousal loss', 0.070745), ('valence acc', 0.986577), ('arousal acc', 0.973154)]\n",
      "\n",
      "processing:  15 ......\n",
      "Before fine-tuning: [('loss', 0.2804), ('valence loss', 0.192238), ('arousal loss', 0.088161), ('valence acc', 0.980519), ('arousal acc', 0.980519)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.276731), ('valence loss', 0.190815), ('arousal loss', 0.085915), ('valence acc', 0.980519), ('arousal acc', 0.980519)]\n",
      "Epoch 00043: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.166157), ('valence loss', 0.166157), ('arousal loss', 0.096416), ('valence acc', 0.980519), ('arousal acc', 0.967532)]\n",
      "Epoch 00067: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.052805), ('valence loss', 0.195356), ('arousal loss', 0.052805), ('valence acc', 0.980519), ('arousal acc', 0.980519)]\n",
      "\n",
      "processing:  16 ......\n",
      "Before fine-tuning: [('loss', 0.066884), ('valence loss', 0.063899), ('arousal loss', 0.002985), ('valence acc', 0.98773), ('arousal acc', 1.0)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.066884), ('valence loss', 0.063899), ('arousal loss', 0.002985), ('valence acc', 0.98773), ('arousal acc', 1.0)]\n",
      "Epoch 00021: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.063899), ('valence loss', 0.063899), ('arousal loss', 0.002985), ('valence acc', 0.98773), ('arousal acc', 1.0)]\n",
      "Epoch 00046: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.000983), ('valence loss', 0.091933), ('arousal loss', 0.000983), ('valence acc', 0.969325), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  17 ......\n",
      "Before fine-tuning: [('loss', 0.282316), ('valence loss', 0.109206), ('arousal loss', 0.17311), ('valence acc', 0.971429), ('arousal acc', 0.942857)]\n",
      "Epoch 00048: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.234528), ('valence loss', 0.085238), ('arousal loss', 0.14929), ('valence acc', 0.977143), ('arousal acc', 0.96)]\n",
      "Epoch 00036: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.07737), ('valence loss', 0.07737), ('arousal loss', 0.159726), ('valence acc', 0.982857), ('arousal acc', 0.96)]\n",
      "Epoch 00020: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.136428), ('valence loss', 0.1215), ('arousal loss', 0.136428), ('valence acc', 0.965714), ('arousal acc', 0.96)]\n",
      "\n",
      "processing:  18 ......\n",
      "Before fine-tuning: [('loss', 0.073228), ('valence loss', 0.044286), ('arousal loss', 0.028942), ('valence acc', 0.98773), ('arousal acc', 0.993865)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.069669), ('valence loss', 0.029611), ('arousal loss', 0.040058), ('valence acc', 0.981595), ('arousal acc', 0.98773)]\n",
      "Epoch 00038: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.017145), ('valence loss', 0.017145), ('arousal loss', 0.045752), ('valence acc', 0.993865), ('arousal acc', 0.98773)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.028942), ('valence loss', 0.044286), ('arousal loss', 0.028942), ('valence acc', 0.98773), ('arousal acc', 0.993865)]\n",
      "\n",
      "processing:  19 ......\n",
      "Before fine-tuning: [('loss', 0.171977), ('valence loss', 0.084317), ('arousal loss', 0.08766), ('valence acc', 0.981928), ('arousal acc', 0.975904)]\n",
      "Epoch 00044: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.108384), ('valence loss', 0.051432), ('arousal loss', 0.056952), ('valence acc', 0.981928), ('arousal acc', 0.981928)]\n",
      "Epoch 00019: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.049808), ('valence loss', 0.049808), ('arousal loss', 0.077177), ('valence acc', 0.981928), ('arousal acc', 0.981928)]\n",
      "Epoch 00075: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.043859), ('valence loss', 0.083237), ('arousal loss', 0.043859), ('valence acc', 0.981928), ('arousal acc', 0.987952)]\n",
      "\n",
      "processing:  20 ......\n",
      "Before fine-tuning: [('loss', 0.046976), ('valence loss', 0.010797), ('arousal loss', 0.036179), ('valence acc', 0.993421), ('arousal acc', 0.980263)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.045598), ('valence loss', 0.012603), ('arousal loss', 0.032994), ('valence acc', 0.993421), ('arousal acc', 0.986842)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.009797), ('valence loss', 0.009797), ('arousal loss', 0.03564), ('valence acc', 1.0), ('arousal acc', 0.980263)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.021632), ('valence loss', 0.011845), ('arousal loss', 0.021632), ('valence acc', 0.993421), ('arousal acc', 0.986842)]\n",
      "\n",
      "processing:  21 ......\n",
      "Before fine-tuning: [('loss', 0.068813), ('valence loss', 0.061468), ('arousal loss', 0.007346), ('valence acc', 0.980132), ('arousal acc', 1.0)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.068813), ('valence loss', 0.061468), ('arousal loss', 0.007346), ('valence acc', 0.980132), ('arousal acc', 1.0)]\n",
      "Epoch 00021: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.031908), ('valence loss', 0.031908), ('arousal loss', 0.150827), ('valence acc', 0.986755), ('arousal acc', 0.960265)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.007346), ('valence loss', 0.061468), ('arousal loss', 0.007346), ('valence acc', 0.980132), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  22 ......\n",
      "Before fine-tuning: [('loss', 0.454965), ('valence loss', 0.146738), ('arousal loss', 0.308227), ('valence acc', 0.938356), ('arousal acc', 0.90411)]\n",
      "Epoch 00041: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.399523), ('valence loss', 0.110903), ('arousal loss', 0.28862), ('valence acc', 0.952055), ('arousal acc', 0.917808)]\n",
      "Epoch 00074: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.112096), ('valence loss', 0.112096), ('arousal loss', 0.322507), ('valence acc', 0.938356), ('arousal acc', 0.924658)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.278479), ('valence loss', 0.107907), ('arousal loss', 0.278479), ('valence acc', 0.952055), ('arousal acc', 0.910959)]\n",
      "\n",
      "processing:  23 ......\n",
      "Before fine-tuning: [('loss', 0.115644), ('valence loss', 0.065588), ('arousal loss', 0.050056), ('valence acc', 0.988571), ('arousal acc', 0.982857)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.115644), ('valence loss', 0.065588), ('arousal loss', 0.050056), ('valence acc', 0.988571), ('arousal acc', 0.982857)]\n",
      "Epoch 00028: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.065588), ('valence loss', 0.065588), ('arousal loss', 0.050056), ('valence acc', 0.988571), ('arousal acc', 0.982857)]\n",
      "Epoch 00030: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.030892), ('valence loss', 0.194751), ('arousal loss', 0.030892), ('valence acc', 0.965714), ('arousal acc', 0.988571)]\n",
      "\n",
      "processing:  24 ......\n",
      "Before fine-tuning: [('loss', 0.096972), ('valence loss', 0.04397), ('arousal loss', 0.053002), ('valence acc', 0.993671), ('arousal acc', 0.981013)]\n",
      "Epoch 00028: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.095473), ('valence loss', 0.032871), ('arousal loss', 0.062602), ('valence acc', 0.993671), ('arousal acc', 0.981013)]\n",
      "Epoch 00030: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.03768), ('valence loss', 0.03768), ('arousal loss', 0.06948), ('valence acc', 0.993671), ('arousal acc', 0.981013)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.051907), ('valence loss', 0.05146), ('arousal loss', 0.051907), ('valence acc', 0.987342), ('arousal acc', 0.981013)]\n",
      "\n",
      "processing:  25 ......\n",
      "Before fine-tuning: [('loss', 0.109458), ('valence loss', 0.069406), ('arousal loss', 0.040052), ('valence acc', 0.986111), ('arousal acc', 0.993056)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.109458), ('valence loss', 0.069406), ('arousal loss', 0.040052), ('valence acc', 0.986111), ('arousal acc', 0.993056)]\n",
      "Epoch 00027: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.036804), ('valence loss', 0.036804), ('arousal loss', 0.082665), ('valence acc', 0.986111), ('arousal acc', 0.958333)]\n",
      "Epoch 00069: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.016731), ('valence loss', 0.089903), ('arousal loss', 0.016731), ('valence acc', 0.986111), ('arousal acc', 0.993056)]\n",
      "\n",
      "processing:  26 ......\n",
      "Before fine-tuning: [('loss', 0.205384), ('valence loss', 0.133801), ('arousal loss', 0.071583), ('valence acc', 0.967532), ('arousal acc', 0.967532)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.200055), ('valence loss', 0.106342), ('arousal loss', 0.093713), ('valence acc', 0.967532), ('arousal acc', 0.974026)]\n",
      "Epoch 00049: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.061397), ('valence loss', 0.061397), ('arousal loss', 0.257886), ('valence acc', 0.974026), ('arousal acc', 0.935065)]\n",
      "Epoch 00072: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.071583), ('valence loss', 0.133801), ('arousal loss', 0.071583), ('valence acc', 0.967532), ('arousal acc', 0.967532)]\n",
      "\n",
      "processing:  27 ......\n",
      "Before fine-tuning: [('loss', 0.071815), ('valence loss', 0.034764), ('arousal loss', 0.037051), ('valence acc', 0.988506), ('arousal acc', 0.982759)]\n",
      "Epoch 00033: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.071815), ('valence loss', 0.034764), ('arousal loss', 0.037051), ('valence acc', 0.988506), ('arousal acc', 0.982759)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.034764), ('valence loss', 0.034764), ('arousal loss', 0.037051), ('valence acc', 0.988506), ('arousal acc', 0.982759)]\n",
      "Epoch 00046: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.012653), ('valence loss', 0.157559), ('arousal loss', 0.012653), ('valence acc', 0.971264), ('arousal acc', 0.988506)]\n",
      "\n",
      "processing:  28 ......\n",
      "Before fine-tuning: [('loss', 0.2138), ('valence loss', 0.135871), ('arousal loss', 0.07793), ('valence acc', 0.97619), ('arousal acc', 0.97619)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.203156), ('valence loss', 0.138563), ('arousal loss', 0.064593), ('valence acc', 0.964286), ('arousal acc', 0.97619)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.113473), ('valence loss', 0.113473), ('arousal loss', 0.085143), ('valence acc', 0.982143), ('arousal acc', 0.970238)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.070173), ('valence loss', 0.136089), ('arousal loss', 0.070173), ('valence acc', 0.964286), ('arousal acc', 0.982143)]\n",
      "\n",
      "processing:  29 ......\n",
      "Before fine-tuning: [('loss', 0.070463), ('valence loss', 0.037228), ('arousal loss', 0.033235), ('valence acc', 0.973684), ('arousal acc', 0.993421)]\n",
      "Epoch 00046: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.046153), ('valence loss', 0.028079), ('arousal loss', 0.018075), ('valence acc', 0.993421), ('arousal acc', 0.993421)]\n",
      "Epoch 00041: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.031692), ('valence loss', 0.031692), ('arousal loss', 0.060732), ('valence acc', 0.980263), ('arousal acc', 0.973684)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.014677), ('valence loss', 0.050029), ('arousal loss', 0.014677), ('valence acc', 0.973684), ('arousal acc', 0.993421)]\n",
      "\n",
      "processing:  30 ......\n",
      "Before fine-tuning: [('loss', 0.058025), ('valence loss', 0.036302), ('arousal loss', 0.021722), ('valence acc', 0.987261), ('arousal acc', 1.0)]\n",
      "Epoch 00039: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.058025), ('valence loss', 0.036302), ('arousal loss', 0.021722), ('valence acc', 0.987261), ('arousal acc', 1.0)]\n",
      "Epoch 00036: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.036302), ('valence loss', 0.036302), ('arousal loss', 0.021722), ('valence acc', 0.987261), ('arousal acc', 1.0)]\n",
      "Epoch 00038: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.019437), ('valence loss', 0.046148), ('arousal loss', 0.019437), ('valence acc', 0.980892), ('arousal acc', 0.993631)]\n",
      "\n",
      "processing:  31 ......\n",
      "Before fine-tuning: [('loss', 0.164776), ('valence loss', 0.077185), ('arousal loss', 0.087591), ('valence acc', 0.97561), ('arousal acc', 0.969512)]\n",
      "Epoch 00057: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.164776), ('valence loss', 0.077185), ('arousal loss', 0.087591), ('valence acc', 0.97561), ('arousal acc', 0.969512)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.076849), ('valence loss', 0.076849), ('arousal loss', 0.087041), ('valence acc', 0.97561), ('arousal acc', 0.957317)]\n",
      "Epoch 00057: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.057979), ('valence loss', 0.133306), ('arousal loss', 0.057979), ('valence acc', 0.969512), ('arousal acc', 0.969512)]\n",
      "\n",
      "processing:  32 ......\n",
      "Before fine-tuning: [('loss', 0.245985), ('valence loss', 0.136915), ('arousal loss', 0.10907), ('valence acc', 0.957055), ('arousal acc', 0.97546)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.214975), ('valence loss', 0.108409), ('arousal loss', 0.106566), ('valence acc', 0.957055), ('arousal acc', 0.981595)]\n",
      "Epoch 00026: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.122913), ('valence loss', 0.122913), ('arousal loss', 0.151645), ('valence acc', 0.96319), ('arousal acc', 0.969325)]\n",
      "Epoch 00031: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.078024), ('valence loss', 0.143234), ('arousal loss', 0.078024), ('valence acc', 0.957055), ('arousal acc', 0.97546)]\n",
      "\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "320/320 [==============================] - 15s 36ms/step - loss: 1.3229 - out_v_loss: 0.6764 - out_a_loss: 0.6465 - out_v_accuracy: 0.6277 - out_a_accuracy: 0.6521 - val_loss: 1.1260 - val_out_v_loss: 0.5673 - val_out_a_loss: 0.5586 - val_out_v_accuracy: 0.6934 - val_out_a_accuracy: 0.7207 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 1.0443 - out_v_loss: 0.5354 - out_a_loss: 0.5089 - out_v_accuracy: 0.7292 - out_a_accuracy: 0.7487 - val_loss: 0.8751 - val_out_v_loss: 0.4465 - val_out_a_loss: 0.4287 - val_out_v_accuracy: 0.7869 - val_out_a_accuracy: 0.8027 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.8440 - out_v_loss: 0.4277 - out_a_loss: 0.4163 - out_v_accuracy: 0.7947 - out_a_accuracy: 0.8038 - val_loss: 0.7467 - val_out_v_loss: 0.3933 - val_out_a_loss: 0.3534 - val_out_v_accuracy: 0.8162 - val_out_a_accuracy: 0.8373 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.6991 - out_v_loss: 0.3544 - out_a_loss: 0.3447 - out_v_accuracy: 0.8372 - out_a_accuracy: 0.8427 - val_loss: 0.6063 - val_out_v_loss: 0.3022 - val_out_a_loss: 0.3041 - val_out_v_accuracy: 0.8689 - val_out_a_accuracy: 0.8609 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.5891 - out_v_loss: 0.2983 - out_a_loss: 0.2908 - out_v_accuracy: 0.8656 - out_a_accuracy: 0.8695 - val_loss: 0.5186 - val_out_v_loss: 0.2599 - val_out_a_loss: 0.2588 - val_out_v_accuracy: 0.8861 - val_out_a_accuracy: 0.8818 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.5101 - out_v_loss: 0.2559 - out_a_loss: 0.2543 - out_v_accuracy: 0.8864 - out_a_accuracy: 0.8832 - val_loss: 0.5102 - val_out_v_loss: 0.2632 - val_out_a_loss: 0.2470 - val_out_v_accuracy: 0.8895 - val_out_a_accuracy: 0.8955 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.4602 - out_v_loss: 0.2286 - out_a_loss: 0.2316 - out_v_accuracy: 0.9032 - out_a_accuracy: 0.9014 - val_loss: 0.4450 - val_out_v_loss: 0.2066 - val_out_a_loss: 0.2383 - val_out_v_accuracy: 0.9125 - val_out_a_accuracy: 0.8994 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.4078 - out_v_loss: 0.2035 - out_a_loss: 0.2043 - out_v_accuracy: 0.9128 - out_a_accuracy: 0.9122 - val_loss: 0.3939 - val_out_v_loss: 0.1906 - val_out_a_loss: 0.2033 - val_out_v_accuracy: 0.9162 - val_out_a_accuracy: 0.9082 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3672 - out_v_loss: 0.1858 - out_a_loss: 0.1814 - out_v_accuracy: 0.9242 - out_a_accuracy: 0.9229 - val_loss: 0.3648 - val_out_v_loss: 0.1864 - val_out_a_loss: 0.1784 - val_out_v_accuracy: 0.9199 - val_out_a_accuracy: 0.9252 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3447 - out_v_loss: 0.1721 - out_a_loss: 0.1726 - out_v_accuracy: 0.9266 - out_a_accuracy: 0.9277 - val_loss: 0.3644 - val_out_v_loss: 0.1945 - val_out_a_loss: 0.1699 - val_out_v_accuracy: 0.9229 - val_out_a_accuracy: 0.9277 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.3228 - out_v_loss: 0.1607 - out_a_loss: 0.1621 - out_v_accuracy: 0.9337 - out_a_accuracy: 0.9310 - val_loss: 0.3661 - val_out_v_loss: 0.1747 - val_out_a_loss: 0.1915 - val_out_v_accuracy: 0.9291 - val_out_a_accuracy: 0.9219 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3019 - out_v_loss: 0.1514 - out_a_loss: 0.1504 - out_v_accuracy: 0.9369 - out_a_accuracy: 0.9362 - val_loss: 0.3451 - val_out_v_loss: 0.1690 - val_out_a_loss: 0.1761 - val_out_v_accuracy: 0.9357 - val_out_a_accuracy: 0.9320 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2886 - out_v_loss: 0.1432 - out_a_loss: 0.1454 - out_v_accuracy: 0.9426 - out_a_accuracy: 0.9385 - val_loss: 0.3174 - val_out_v_loss: 0.1584 - val_out_a_loss: 0.1590 - val_out_v_accuracy: 0.9396 - val_out_a_accuracy: 0.9346 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2660 - out_v_loss: 0.1295 - out_a_loss: 0.1365 - out_v_accuracy: 0.9475 - out_a_accuracy: 0.9421 - val_loss: 0.2921 - val_out_v_loss: 0.1480 - val_out_a_loss: 0.1441 - val_out_v_accuracy: 0.9416 - val_out_a_accuracy: 0.9428 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2525 - out_v_loss: 0.1246 - out_a_loss: 0.1279 - out_v_accuracy: 0.9483 - out_a_accuracy: 0.9482 - val_loss: 0.2959 - val_out_v_loss: 0.1397 - val_out_a_loss: 0.1563 - val_out_v_accuracy: 0.9484 - val_out_a_accuracy: 0.9385 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2276 - out_v_loss: 0.1142 - out_a_loss: 0.1134 - out_v_accuracy: 0.9538 - out_a_accuracy: 0.9540 - val_loss: 0.2984 - val_out_v_loss: 0.1648 - val_out_a_loss: 0.1337 - val_out_v_accuracy: 0.9373 - val_out_a_accuracy: 0.9471 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2207 - out_v_loss: 0.1082 - out_a_loss: 0.1125 - out_v_accuracy: 0.9563 - out_a_accuracy: 0.9530 - val_loss: 0.2773 - val_out_v_loss: 0.1399 - val_out_a_loss: 0.1374 - val_out_v_accuracy: 0.9445 - val_out_a_accuracy: 0.9441 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2195 - out_v_loss: 0.1074 - out_a_loss: 0.1121 - out_v_accuracy: 0.9575 - out_a_accuracy: 0.9541 - val_loss: 0.2957 - val_out_v_loss: 0.1555 - val_out_a_loss: 0.1401 - val_out_v_accuracy: 0.9434 - val_out_a_accuracy: 0.9463 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2082 - out_v_loss: 0.1008 - out_a_loss: 0.1073 - out_v_accuracy: 0.9597 - out_a_accuracy: 0.9565 - val_loss: 0.2838 - val_out_v_loss: 0.1446 - val_out_a_loss: 0.1392 - val_out_v_accuracy: 0.9482 - val_out_a_accuracy: 0.9461 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2116 - out_v_loss: 0.1032 - out_a_loss: 0.1084 - out_v_accuracy: 0.9593 - out_a_accuracy: 0.9558 - val_loss: 0.2561 - val_out_v_loss: 0.1337 - val_out_a_loss: 0.1224 - val_out_v_accuracy: 0.9490 - val_out_a_accuracy: 0.9537 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1941 - out_v_loss: 0.0973 - out_a_loss: 0.0967 - out_v_accuracy: 0.9606 - out_a_accuracy: 0.9609 - val_loss: 0.2269 - val_out_v_loss: 0.1087 - val_out_a_loss: 0.1182 - val_out_v_accuracy: 0.9574 - val_out_a_accuracy: 0.9533 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1742 - out_v_loss: 0.0871 - out_a_loss: 0.0871 - out_v_accuracy: 0.9647 - out_a_accuracy: 0.9659 - val_loss: 0.2524 - val_out_v_loss: 0.1269 - val_out_a_loss: 0.1255 - val_out_v_accuracy: 0.9496 - val_out_a_accuracy: 0.9523 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1847 - out_v_loss: 0.0931 - out_a_loss: 0.0916 - out_v_accuracy: 0.9633 - out_a_accuracy: 0.9628 - val_loss: 0.2368 - val_out_v_loss: 0.1190 - val_out_a_loss: 0.1177 - val_out_v_accuracy: 0.9531 - val_out_a_accuracy: 0.9566 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1697 - out_v_loss: 0.0826 - out_a_loss: 0.0870 - out_v_accuracy: 0.9677 - out_a_accuracy: 0.9650 - val_loss: 0.2340 - val_out_v_loss: 0.1182 - val_out_a_loss: 0.1158 - val_out_v_accuracy: 0.9557 - val_out_a_accuracy: 0.9566 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1621 - out_v_loss: 0.0806 - out_a_loss: 0.0815 - out_v_accuracy: 0.9676 - out_a_accuracy: 0.9664 - val_loss: 0.2343 - val_out_v_loss: 0.1172 - val_out_a_loss: 0.1170 - val_out_v_accuracy: 0.9555 - val_out_a_accuracy: 0.9594 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.1633 - out_v_loss: 0.0816 - out_a_loss: 0.0817 - out_v_accuracy: 0.9675 - out_a_accuracy: 0.9679\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1635 - out_v_loss: 0.0818 - out_a_loss: 0.0817 - out_v_accuracy: 0.9673 - out_a_accuracy: 0.9679 - val_loss: 0.2546 - val_out_v_loss: 0.1204 - val_out_a_loss: 0.1343 - val_out_v_accuracy: 0.9576 - val_out_a_accuracy: 0.9541 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1140 - out_v_loss: 0.0535 - out_a_loss: 0.0604 - out_v_accuracy: 0.9801 - out_a_accuracy: 0.9763 - val_loss: 0.1896 - val_out_v_loss: 0.1022 - val_out_a_loss: 0.0874 - val_out_v_accuracy: 0.9643 - val_out_a_accuracy: 0.9674 - lr: 5.0000e-04\n",
      "Epoch 28/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0974 - out_v_loss: 0.0496 - out_a_loss: 0.0479 - out_v_accuracy: 0.9806 - out_a_accuracy: 0.9811 - val_loss: 0.1764 - val_out_v_loss: 0.0878 - val_out_a_loss: 0.0886 - val_out_v_accuracy: 0.9680 - val_out_a_accuracy: 0.9682 - lr: 5.0000e-04\n",
      "Epoch 29/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0880 - out_v_loss: 0.0404 - out_a_loss: 0.0477 - out_v_accuracy: 0.9845 - out_a_accuracy: 0.9814 - val_loss: 0.1751 - val_out_v_loss: 0.0869 - val_out_a_loss: 0.0882 - val_out_v_accuracy: 0.9697 - val_out_a_accuracy: 0.9688 - lr: 5.0000e-04\n",
      "Epoch 30/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0878 - out_v_loss: 0.0449 - out_a_loss: 0.0430 - out_v_accuracy: 0.9820 - out_a_accuracy: 0.9832 - val_loss: 0.1805 - val_out_v_loss: 0.0927 - val_out_a_loss: 0.0879 - val_out_v_accuracy: 0.9656 - val_out_a_accuracy: 0.9699 - lr: 5.0000e-04\n",
      "Epoch 31/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.0806 - out_v_loss: 0.0365 - out_a_loss: 0.0441 - out_v_accuracy: 0.9858 - out_a_accuracy: 0.9828 - val_loss: 0.1916 - val_out_v_loss: 0.1015 - val_out_a_loss: 0.0900 - val_out_v_accuracy: 0.9650 - val_out_a_accuracy: 0.9725 - lr: 5.0000e-04\n",
      "Epoch 32/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.0874 - out_v_loss: 0.0435 - out_a_loss: 0.0439 - out_v_accuracy: 0.9826 - out_a_accuracy: 0.9830 - val_loss: 0.1899 - val_out_v_loss: 0.1000 - val_out_a_loss: 0.0899 - val_out_v_accuracy: 0.9666 - val_out_a_accuracy: 0.9693 - lr: 5.0000e-04\n",
      "Epoch 33/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0771 - out_v_loss: 0.0379 - out_a_loss: 0.0391 - out_v_accuracy: 0.9854 - out_a_accuracy: 0.9850 - val_loss: 0.1855 - val_out_v_loss: 0.0936 - val_out_a_loss: 0.0919 - val_out_v_accuracy: 0.9688 - val_out_a_accuracy: 0.9699 - lr: 5.0000e-04\n",
      "Epoch 34/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0820 - out_v_loss: 0.0389 - out_a_loss: 0.0430 - out_v_accuracy: 0.9853 - out_a_accuracy: 0.9844\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0818 - out_v_loss: 0.0388 - out_a_loss: 0.0430 - out_v_accuracy: 0.9853 - out_a_accuracy: 0.9844 - val_loss: 0.1769 - val_out_v_loss: 0.0870 - val_out_a_loss: 0.0898 - val_out_v_accuracy: 0.9699 - val_out_a_accuracy: 0.9693 - lr: 5.0000e-04\n",
      "Epoch 35/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0585 - out_v_loss: 0.0276 - out_a_loss: 0.0309 - out_v_accuracy: 0.9891 - out_a_accuracy: 0.9877 - val_loss: 0.1605 - val_out_v_loss: 0.0824 - val_out_a_loss: 0.0781 - val_out_v_accuracy: 0.9727 - val_out_a_accuracy: 0.9744 - lr: 2.5000e-04\n",
      "Epoch 36/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0559 - out_v_loss: 0.0262 - out_a_loss: 0.0297 - out_v_accuracy: 0.9901 - out_a_accuracy: 0.9886 - val_loss: 0.1570 - val_out_v_loss: 0.0826 - val_out_a_loss: 0.0744 - val_out_v_accuracy: 0.9748 - val_out_a_accuracy: 0.9754 - lr: 2.5000e-04\n",
      "Epoch 37/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0551 - out_v_loss: 0.0279 - out_a_loss: 0.0272 - out_v_accuracy: 0.9896 - out_a_accuracy: 0.9893 - val_loss: 0.1475 - val_out_v_loss: 0.0729 - val_out_a_loss: 0.0746 - val_out_v_accuracy: 0.9760 - val_out_a_accuracy: 0.9744 - lr: 2.5000e-04\n",
      "Epoch 38/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0499 - out_v_loss: 0.0239 - out_a_loss: 0.0260 - out_v_accuracy: 0.9913 - out_a_accuracy: 0.9905 - val_loss: 0.1572 - val_out_v_loss: 0.0794 - val_out_a_loss: 0.0778 - val_out_v_accuracy: 0.9764 - val_out_a_accuracy: 0.9754 - lr: 2.5000e-04\n",
      "Epoch 39/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0465 - out_v_loss: 0.0203 - out_a_loss: 0.0262 - out_v_accuracy: 0.9925 - out_a_accuracy: 0.9895 - val_loss: 0.1629 - val_out_v_loss: 0.0832 - val_out_a_loss: 0.0797 - val_out_v_accuracy: 0.9734 - val_out_a_accuracy: 0.9748 - lr: 2.5000e-04\n",
      "Epoch 40/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0454 - out_v_loss: 0.0227 - out_a_loss: 0.0227 - out_v_accuracy: 0.9912 - out_a_accuracy: 0.9915 - val_loss: 0.1535 - val_out_v_loss: 0.0799 - val_out_a_loss: 0.0736 - val_out_v_accuracy: 0.9754 - val_out_a_accuracy: 0.9768 - lr: 2.5000e-04\n",
      "Epoch 41/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0471 - out_v_loss: 0.0234 - out_a_loss: 0.0237 - out_v_accuracy: 0.9910 - out_a_accuracy: 0.9911 - val_loss: 0.1558 - val_out_v_loss: 0.0794 - val_out_a_loss: 0.0765 - val_out_v_accuracy: 0.9756 - val_out_a_accuracy: 0.9758 - lr: 2.5000e-04\n",
      "Epoch 42/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0467 - out_v_loss: 0.0228 - out_a_loss: 0.0239 - out_v_accuracy: 0.9911 - out_a_accuracy: 0.9907\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0469 - out_v_loss: 0.0228 - out_a_loss: 0.0241 - out_v_accuracy: 0.9912 - out_a_accuracy: 0.9907 - val_loss: 0.1580 - val_out_v_loss: 0.0830 - val_out_a_loss: 0.0750 - val_out_v_accuracy: 0.9768 - val_out_a_accuracy: 0.9764 - lr: 2.5000e-04\n",
      "Epoch 43/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0396 - out_v_loss: 0.0190 - out_a_loss: 0.0206 - out_v_accuracy: 0.9929 - out_a_accuracy: 0.9924 - val_loss: 0.1521 - val_out_v_loss: 0.0785 - val_out_a_loss: 0.0736 - val_out_v_accuracy: 0.9758 - val_out_a_accuracy: 0.9766 - lr: 1.2500e-04\n",
      "Epoch 44/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0383 - out_v_loss: 0.0196 - out_a_loss: 0.0186 - out_v_accuracy: 0.9923 - out_a_accuracy: 0.9925 - val_loss: 0.1502 - val_out_v_loss: 0.0771 - val_out_a_loss: 0.0731 - val_out_v_accuracy: 0.9760 - val_out_a_accuracy: 0.9791 - lr: 1.2500e-04\n",
      "Epoch 45/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0352 - out_v_loss: 0.0175 - out_a_loss: 0.0176 - out_v_accuracy: 0.9937 - out_a_accuracy: 0.9933 - val_loss: 0.1532 - val_out_v_loss: 0.0760 - val_out_a_loss: 0.0772 - val_out_v_accuracy: 0.9764 - val_out_a_accuracy: 0.9762 - lr: 1.2500e-04\n",
      "Epoch 46/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0317 - out_v_loss: 0.0150 - out_a_loss: 0.0167 - out_v_accuracy: 0.9948 - out_a_accuracy: 0.9937 - val_loss: 0.1456 - val_out_v_loss: 0.0739 - val_out_a_loss: 0.0717 - val_out_v_accuracy: 0.9783 - val_out_a_accuracy: 0.9799 - lr: 1.2500e-04\n",
      "Epoch 47/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0353 - out_v_loss: 0.0184 - out_a_loss: 0.0169 - out_v_accuracy: 0.9931 - out_a_accuracy: 0.9938 - val_loss: 0.1489 - val_out_v_loss: 0.0765 - val_out_a_loss: 0.0724 - val_out_v_accuracy: 0.9766 - val_out_a_accuracy: 0.9789 - lr: 1.2500e-04\n",
      "Epoch 48/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0360 - out_v_loss: 0.0187 - out_a_loss: 0.0174 - out_v_accuracy: 0.9926 - out_a_accuracy: 0.9936 - val_loss: 0.1509 - val_out_v_loss: 0.0765 - val_out_a_loss: 0.0744 - val_out_v_accuracy: 0.9777 - val_out_a_accuracy: 0.9768 - lr: 1.2500e-04\n",
      "Epoch 49/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0304 - out_v_loss: 0.0147 - out_a_loss: 0.0158 - out_v_accuracy: 0.9937 - out_a_accuracy: 0.9944 - val_loss: 0.1474 - val_out_v_loss: 0.0743 - val_out_a_loss: 0.0731 - val_out_v_accuracy: 0.9785 - val_out_a_accuracy: 0.9803 - lr: 1.2500e-04\n",
      "Epoch 50/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0322 - out_v_loss: 0.0167 - out_a_loss: 0.0155 - out_v_accuracy: 0.9937 - out_a_accuracy: 0.9944 - val_loss: 0.1442 - val_out_v_loss: 0.0722 - val_out_a_loss: 0.0720 - val_out_v_accuracy: 0.9777 - val_out_a_accuracy: 0.9801 - lr: 1.2500e-04\n",
      "Epoch 51/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0306 - out_v_loss: 0.0145 - out_a_loss: 0.0161 - out_v_accuracy: 0.9942 - out_a_accuracy: 0.9941 - val_loss: 0.1492 - val_out_v_loss: 0.0746 - val_out_a_loss: 0.0746 - val_out_v_accuracy: 0.9773 - val_out_a_accuracy: 0.9801 - lr: 1.2500e-04\n",
      "Epoch 52/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0307 - out_v_loss: 0.0152 - out_a_loss: 0.0155 - out_v_accuracy: 0.9943 - out_a_accuracy: 0.9943 - val_loss: 0.1426 - val_out_v_loss: 0.0733 - val_out_a_loss: 0.0692 - val_out_v_accuracy: 0.9777 - val_out_a_accuracy: 0.9822 - lr: 1.2500e-04\n",
      "Epoch 53/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0294 - out_v_loss: 0.0155 - out_a_loss: 0.0139 - out_v_accuracy: 0.9941 - out_a_accuracy: 0.9948 - val_loss: 0.1422 - val_out_v_loss: 0.0706 - val_out_a_loss: 0.0716 - val_out_v_accuracy: 0.9793 - val_out_a_accuracy: 0.9816 - lr: 1.2500e-04\n",
      "Epoch 54/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0311 - out_v_loss: 0.0153 - out_a_loss: 0.0158 - out_v_accuracy: 0.9939 - out_a_accuracy: 0.9943 - val_loss: 0.1436 - val_out_v_loss: 0.0711 - val_out_a_loss: 0.0724 - val_out_v_accuracy: 0.9791 - val_out_a_accuracy: 0.9803 - lr: 1.2500e-04\n",
      "Epoch 55/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0293 - out_v_loss: 0.0136 - out_a_loss: 0.0157 - out_v_accuracy: 0.9943 - out_a_accuracy: 0.9942 - val_loss: 0.1455 - val_out_v_loss: 0.0727 - val_out_a_loss: 0.0727 - val_out_v_accuracy: 0.9773 - val_out_a_accuracy: 0.9789 - lr: 1.2500e-04\n",
      "Epoch 56/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0283 - out_v_loss: 0.0147 - out_a_loss: 0.0136 - out_v_accuracy: 0.9940 - out_a_accuracy: 0.9954 - val_loss: 0.1407 - val_out_v_loss: 0.0688 - val_out_a_loss: 0.0718 - val_out_v_accuracy: 0.9783 - val_out_a_accuracy: 0.9793 - lr: 1.2500e-04\n",
      "Epoch 57/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0271 - out_v_loss: 0.0128 - out_a_loss: 0.0143 - out_v_accuracy: 0.9953 - out_a_accuracy: 0.9954 - val_loss: 0.1373 - val_out_v_loss: 0.0670 - val_out_a_loss: 0.0703 - val_out_v_accuracy: 0.9793 - val_out_a_accuracy: 0.9793 - lr: 1.2500e-04\n",
      "Epoch 58/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0317 - out_v_loss: 0.0163 - out_a_loss: 0.0154 - out_v_accuracy: 0.9936 - out_a_accuracy: 0.9944 - val_loss: 0.1382 - val_out_v_loss: 0.0682 - val_out_a_loss: 0.0699 - val_out_v_accuracy: 0.9803 - val_out_a_accuracy: 0.9809 - lr: 1.2500e-04\n",
      "Epoch 59/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0296 - out_v_loss: 0.0129 - out_a_loss: 0.0167 - out_v_accuracy: 0.9952 - out_a_accuracy: 0.9939 - val_loss: 0.1506 - val_out_v_loss: 0.0781 - val_out_a_loss: 0.0725 - val_out_v_accuracy: 0.9764 - val_out_a_accuracy: 0.9809 - lr: 1.2500e-04\n",
      "Epoch 60/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0275 - out_v_loss: 0.0148 - out_a_loss: 0.0127 - out_v_accuracy: 0.9944 - out_a_accuracy: 0.9955 - val_loss: 0.1481 - val_out_v_loss: 0.0731 - val_out_a_loss: 0.0750 - val_out_v_accuracy: 0.9779 - val_out_a_accuracy: 0.9801 - lr: 1.2500e-04\n",
      "Epoch 61/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0282 - out_v_loss: 0.0143 - out_a_loss: 0.0139 - out_v_accuracy: 0.9944 - out_a_accuracy: 0.9950 - val_loss: 0.1440 - val_out_v_loss: 0.0728 - val_out_a_loss: 0.0712 - val_out_v_accuracy: 0.9777 - val_out_a_accuracy: 0.9814 - lr: 1.2500e-04\n",
      "Epoch 62/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0292 - out_v_loss: 0.0143 - out_a_loss: 0.0149 - out_v_accuracy: 0.9948 - out_a_accuracy: 0.9948\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0297 - out_v_loss: 0.0147 - out_a_loss: 0.0150 - out_v_accuracy: 0.9947 - out_a_accuracy: 0.9947 - val_loss: 0.1536 - val_out_v_loss: 0.0764 - val_out_a_loss: 0.0772 - val_out_v_accuracy: 0.9787 - val_out_a_accuracy: 0.9805 - lr: 1.2500e-04\n",
      "Epoch 63/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0236 - out_v_loss: 0.0119 - out_a_loss: 0.0117 - out_v_accuracy: 0.9958 - out_a_accuracy: 0.9956 - val_loss: 0.1507 - val_out_v_loss: 0.0782 - val_out_a_loss: 0.0725 - val_out_v_accuracy: 0.9787 - val_out_a_accuracy: 0.9799 - lr: 6.2500e-05\n",
      "Epoch 64/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0234 - out_v_loss: 0.0108 - out_a_loss: 0.0126 - out_v_accuracy: 0.9962 - out_a_accuracy: 0.9952 - val_loss: 0.1429 - val_out_v_loss: 0.0711 - val_out_a_loss: 0.0719 - val_out_v_accuracy: 0.9783 - val_out_a_accuracy: 0.9811 - lr: 6.2500e-05\n",
      "Epoch 65/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0234 - out_v_loss: 0.0115 - out_a_loss: 0.0118 - out_v_accuracy: 0.9955 - out_a_accuracy: 0.9955 - val_loss: 0.1403 - val_out_v_loss: 0.0715 - val_out_a_loss: 0.0688 - val_out_v_accuracy: 0.9799 - val_out_a_accuracy: 0.9814 - lr: 6.2500e-05\n",
      "Epoch 66/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0210 - out_v_loss: 0.0102 - out_a_loss: 0.0108 - out_v_accuracy: 0.9962 - out_a_accuracy: 0.9962 - val_loss: 0.1431 - val_out_v_loss: 0.0756 - val_out_a_loss: 0.0675 - val_out_v_accuracy: 0.9779 - val_out_a_accuracy: 0.9816 - lr: 6.2500e-05\n",
      "Epoch 67/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0224 - out_v_loss: 0.0109 - out_a_loss: 0.0115 - out_v_accuracy: 0.9961 - out_a_accuracy: 0.9955\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0224 - out_v_loss: 0.0109 - out_a_loss: 0.0115 - out_v_accuracy: 0.9961 - out_a_accuracy: 0.9955 - val_loss: 0.1481 - val_out_v_loss: 0.0770 - val_out_a_loss: 0.0711 - val_out_v_accuracy: 0.9795 - val_out_a_accuracy: 0.9795 - lr: 6.2500e-05\n",
      "Epoch 68/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0226 - out_v_loss: 0.0116 - out_a_loss: 0.0110 - out_v_accuracy: 0.9957 - out_a_accuracy: 0.9955 - val_loss: 0.1388 - val_out_v_loss: 0.0707 - val_out_a_loss: 0.0681 - val_out_v_accuracy: 0.9816 - val_out_a_accuracy: 0.9812 - lr: 3.1250e-05\n",
      "Epoch 69/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0204 - out_v_loss: 0.0097 - out_a_loss: 0.0107 - out_v_accuracy: 0.9962 - out_a_accuracy: 0.9959 - val_loss: 0.1389 - val_out_v_loss: 0.0707 - val_out_a_loss: 0.0683 - val_out_v_accuracy: 0.9809 - val_out_a_accuracy: 0.9826 - lr: 3.1250e-05\n",
      "Epoch 70/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0209 - out_v_loss: 0.0101 - out_a_loss: 0.0108 - out_v_accuracy: 0.9958 - out_a_accuracy: 0.9960 - val_loss: 0.1368 - val_out_v_loss: 0.0694 - val_out_a_loss: 0.0674 - val_out_v_accuracy: 0.9818 - val_out_a_accuracy: 0.9807 - lr: 3.1250e-05\n",
      "Epoch 71/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0195 - out_v_loss: 0.0090 - out_a_loss: 0.0105 - out_v_accuracy: 0.9972 - out_a_accuracy: 0.9956 - val_loss: 0.1385 - val_out_v_loss: 0.0688 - val_out_a_loss: 0.0696 - val_out_v_accuracy: 0.9820 - val_out_a_accuracy: 0.9811 - lr: 3.1250e-05\n",
      "Epoch 72/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0217 - out_v_loss: 0.0110 - out_a_loss: 0.0108 - out_v_accuracy: 0.9961 - out_a_accuracy: 0.9964 - val_loss: 0.1377 - val_out_v_loss: 0.0687 - val_out_a_loss: 0.0690 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9818 - lr: 3.1250e-05\n",
      "Epoch 73/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0202 - out_v_loss: 0.0097 - out_a_loss: 0.0105 - out_v_accuracy: 0.9966 - out_a_accuracy: 0.9962 - val_loss: 0.1408 - val_out_v_loss: 0.0715 - val_out_a_loss: 0.0693 - val_out_v_accuracy: 0.9809 - val_out_a_accuracy: 0.9820 - lr: 3.1250e-05\n",
      "Epoch 74/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0203 - out_v_loss: 0.0114 - out_a_loss: 0.0089 - out_v_accuracy: 0.9958 - out_a_accuracy: 0.9964 - val_loss: 0.1372 - val_out_v_loss: 0.0691 - val_out_a_loss: 0.0681 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9824 - lr: 3.1250e-05\n",
      "Epoch 75/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0196 - out_v_loss: 0.0099 - out_a_loss: 0.0097 - out_v_accuracy: 0.9964 - out_a_accuracy: 0.9965\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0196 - out_v_loss: 0.0099 - out_a_loss: 0.0096 - out_v_accuracy: 0.9964 - out_a_accuracy: 0.9965 - val_loss: 0.1373 - val_out_v_loss: 0.0683 - val_out_a_loss: 0.0690 - val_out_v_accuracy: 0.9803 - val_out_a_accuracy: 0.9828 - lr: 3.1250e-05\n",
      "Epoch 76/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0203 - out_v_loss: 0.0105 - out_a_loss: 0.0098 - out_v_accuracy: 0.9962 - out_a_accuracy: 0.9964 - val_loss: 0.1390 - val_out_v_loss: 0.0707 - val_out_a_loss: 0.0683 - val_out_v_accuracy: 0.9797 - val_out_a_accuracy: 0.9828 - lr: 1.5625e-05\n",
      "Epoch 77/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0192 - out_v_loss: 0.0100 - out_a_loss: 0.0092 - out_v_accuracy: 0.9961 - out_a_accuracy: 0.9966 - val_loss: 0.1349 - val_out_v_loss: 0.0683 - val_out_a_loss: 0.0666 - val_out_v_accuracy: 0.9809 - val_out_a_accuracy: 0.9834 - lr: 1.5625e-05\n",
      "Epoch 78/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0198 - out_v_loss: 0.0108 - out_a_loss: 0.0090 - out_v_accuracy: 0.9960 - out_a_accuracy: 0.9971 - val_loss: 0.1343 - val_out_v_loss: 0.0678 - val_out_a_loss: 0.0666 - val_out_v_accuracy: 0.9803 - val_out_a_accuracy: 0.9830 - lr: 1.5625e-05\n",
      "Epoch 79/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0192 - out_v_loss: 0.0091 - out_a_loss: 0.0100 - out_v_accuracy: 0.9966 - out_a_accuracy: 0.9960 - val_loss: 0.1359 - val_out_v_loss: 0.0683 - val_out_a_loss: 0.0676 - val_out_v_accuracy: 0.9811 - val_out_a_accuracy: 0.9822 - lr: 1.5625e-05\n",
      "Epoch 80/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0195 - out_v_loss: 0.0095 - out_a_loss: 0.0099 - out_v_accuracy: 0.9969 - out_a_accuracy: 0.9960 - val_loss: 0.1365 - val_out_v_loss: 0.0691 - val_out_a_loss: 0.0674 - val_out_v_accuracy: 0.9812 - val_out_a_accuracy: 0.9826 - lr: 1.5625e-05\n",
      "Epoch 81/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0183 - out_v_loss: 0.0096 - out_a_loss: 0.0088 - out_v_accuracy: 0.9970 - out_a_accuracy: 0.9969 - val_loss: 0.1349 - val_out_v_loss: 0.0670 - val_out_a_loss: 0.0679 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9820 - lr: 1.5625e-05\n",
      "Epoch 82/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0174 - out_v_loss: 0.0089 - out_a_loss: 0.0085 - out_v_accuracy: 0.9971 - out_a_accuracy: 0.9969 - val_loss: 0.1334 - val_out_v_loss: 0.0671 - val_out_a_loss: 0.0662 - val_out_v_accuracy: 0.9812 - val_out_a_accuracy: 0.9830 - lr: 1.5625e-05\n",
      "Epoch 83/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0169 - out_v_loss: 0.0090 - out_a_loss: 0.0079 - out_v_accuracy: 0.9962 - out_a_accuracy: 0.9969 - val_loss: 0.1365 - val_out_v_loss: 0.0687 - val_out_a_loss: 0.0678 - val_out_v_accuracy: 0.9812 - val_out_a_accuracy: 0.9832 - lr: 1.5625e-05\n",
      "Epoch 84/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0180 - out_v_loss: 0.0083 - out_a_loss: 0.0097 - out_v_accuracy: 0.9969 - out_a_accuracy: 0.9962 - val_loss: 0.1358 - val_out_v_loss: 0.0690 - val_out_a_loss: 0.0668 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9832 - lr: 1.5625e-05\n",
      "Epoch 85/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0174 - out_v_loss: 0.0083 - out_a_loss: 0.0091 - out_v_accuracy: 0.9965 - out_a_accuracy: 0.9967 - val_loss: 0.1343 - val_out_v_loss: 0.0672 - val_out_a_loss: 0.0672 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9834 - lr: 1.5625e-05\n",
      "Epoch 86/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0193 - out_v_loss: 0.0082 - out_a_loss: 0.0111 - out_v_accuracy: 0.9971 - out_a_accuracy: 0.9960 - val_loss: 0.1339 - val_out_v_loss: 0.0668 - val_out_a_loss: 0.0671 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9824 - lr: 1.5625e-05\n",
      "Epoch 87/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0180 - out_v_loss: 0.0088 - out_a_loss: 0.0093 - out_v_accuracy: 0.9968 - out_a_accuracy: 0.9968\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0180 - out_v_loss: 0.0088 - out_a_loss: 0.0092 - out_v_accuracy: 0.9967 - out_a_accuracy: 0.9968 - val_loss: 0.1375 - val_out_v_loss: 0.0688 - val_out_a_loss: 0.0688 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9824 - lr: 1.5625e-05\n",
      "Epoch 88/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0162 - out_v_loss: 0.0070 - out_a_loss: 0.0092 - out_v_accuracy: 0.9974 - out_a_accuracy: 0.9964 - val_loss: 0.1357 - val_out_v_loss: 0.0685 - val_out_a_loss: 0.0672 - val_out_v_accuracy: 0.9811 - val_out_a_accuracy: 0.9824 - lr: 7.8125e-06\n",
      "Epoch 89/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0156 - out_v_loss: 0.0070 - out_a_loss: 0.0086 - out_v_accuracy: 0.9976 - out_a_accuracy: 0.9968 - val_loss: 0.1358 - val_out_v_loss: 0.0687 - val_out_a_loss: 0.0671 - val_out_v_accuracy: 0.9812 - val_out_a_accuracy: 0.9828 - lr: 7.8125e-06\n",
      "Epoch 90/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0156 - out_v_loss: 0.0075 - out_a_loss: 0.0080 - out_v_accuracy: 0.9973 - out_a_accuracy: 0.9971 - val_loss: 0.1343 - val_out_v_loss: 0.0683 - val_out_a_loss: 0.0660 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9834 - lr: 7.8125e-06\n",
      "Epoch 91/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0194 - out_v_loss: 0.0093 - out_a_loss: 0.0101 - out_v_accuracy: 0.9967 - out_a_accuracy: 0.9970 - val_loss: 0.1355 - val_out_v_loss: 0.0689 - val_out_a_loss: 0.0666 - val_out_v_accuracy: 0.9805 - val_out_a_accuracy: 0.9828 - lr: 7.8125e-06\n",
      "Epoch 92/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0184 - out_v_loss: 0.0089 - out_a_loss: 0.0095 - out_v_accuracy: 0.9967 - out_a_accuracy: 0.9966\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0183 - out_v_loss: 0.0088 - out_a_loss: 0.0095 - out_v_accuracy: 0.9967 - out_a_accuracy: 0.9966 - val_loss: 0.1376 - val_out_v_loss: 0.0717 - val_out_a_loss: 0.0659 - val_out_v_accuracy: 0.9799 - val_out_a_accuracy: 0.9830 - lr: 7.8125e-06\n",
      "Epoch 93/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0187 - out_v_loss: 0.0094 - out_a_loss: 0.0093 - out_v_accuracy: 0.9961 - out_a_accuracy: 0.9968 - val_loss: 0.1342 - val_out_v_loss: 0.0682 - val_out_a_loss: 0.0660 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9834 - lr: 3.9063e-06\n",
      "Epoch 94/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0176 - out_v_loss: 0.0083 - out_a_loss: 0.0093 - out_v_accuracy: 0.9968 - out_a_accuracy: 0.9963 - val_loss: 0.1344 - val_out_v_loss: 0.0680 - val_out_a_loss: 0.0664 - val_out_v_accuracy: 0.9812 - val_out_a_accuracy: 0.9834 - lr: 3.9063e-06\n",
      "Epoch 95/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0160 - out_v_loss: 0.0074 - out_a_loss: 0.0086 - out_v_accuracy: 0.9976 - out_a_accuracy: 0.9967 - val_loss: 0.1363 - val_out_v_loss: 0.0700 - val_out_a_loss: 0.0663 - val_out_v_accuracy: 0.9805 - val_out_a_accuracy: 0.9838 - lr: 3.9063e-06\n",
      "Epoch 96/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0188 - out_v_loss: 0.0104 - out_a_loss: 0.0085 - out_v_accuracy: 0.9961 - out_a_accuracy: 0.9971 - val_loss: 0.1353 - val_out_v_loss: 0.0688 - val_out_a_loss: 0.0665 - val_out_v_accuracy: 0.9809 - val_out_a_accuracy: 0.9832 - lr: 3.9063e-06\n",
      "Epoch 97/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0181 - out_v_loss: 0.0080 - out_a_loss: 0.0100 - out_v_accuracy: 0.9971 - out_a_accuracy: 0.9962\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0182 - out_v_loss: 0.0080 - out_a_loss: 0.0101 - out_v_accuracy: 0.9971 - out_a_accuracy: 0.9961 - val_loss: 0.1356 - val_out_v_loss: 0.0691 - val_out_a_loss: 0.0666 - val_out_v_accuracy: 0.9807 - val_out_a_accuracy: 0.9830 - lr: 3.9063e-06\n",
      "Epoch 98/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0162 - out_v_loss: 0.0067 - out_a_loss: 0.0095 - out_v_accuracy: 0.9976 - out_a_accuracy: 0.9965 - val_loss: 0.1352 - val_out_v_loss: 0.0694 - val_out_a_loss: 0.0658 - val_out_v_accuracy: 0.9809 - val_out_a_accuracy: 0.9828 - lr: 1.9531e-06\n",
      "Epoch 00098: early stopping\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.1352 - out_v_loss: 0.0694 - out_a_loss: 0.0658 - out_v_accuracy: 0.9809 - out_a_accuracy: 0.9828\n",
      "\n",
      "processing:  01 ......\n",
      "Before fine-tuning: [('loss', 0.029981), ('valence loss', 0.013832), ('arousal loss', 0.016149), ('valence acc', 0.988024), ('arousal acc', 0.994012)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.029981), ('valence loss', 0.013832), ('arousal loss', 0.016149), ('valence acc', 0.988024), ('arousal acc', 0.994012)]\n",
      "Epoch 00055: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.007633), ('valence loss', 0.007633), ('arousal loss', 0.040372), ('valence acc', 0.994012), ('arousal acc', 0.988024)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.016149), ('valence loss', 0.013832), ('arousal loss', 0.016149), ('valence acc', 0.988024), ('arousal acc', 0.994012)]\n",
      "\n",
      "processing:  02 ......\n",
      "Before fine-tuning: [('loss', 0.418298), ('valence loss', 0.160454), ('arousal loss', 0.257844), ('valence acc', 0.966443), ('arousal acc', 0.939597)]\n",
      "Epoch 00038: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.352731), ('valence loss', 0.141922), ('arousal loss', 0.210809), ('valence acc', 0.966443), ('arousal acc', 0.926175)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.151031), ('valence loss', 0.151031), ('arousal loss', 0.228279), ('valence acc', 0.966443), ('arousal acc', 0.939597)]\n",
      "Epoch 00081: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.198205), ('valence loss', 0.158886), ('arousal loss', 0.198205), ('valence acc', 0.95302), ('arousal acc', 0.939597)]\n",
      "\n",
      "processing:  03 ......\n",
      "Before fine-tuning: [('loss', 0.115581), ('valence loss', 0.047154), ('arousal loss', 0.068427), ('valence acc', 0.988372), ('arousal acc', 0.97093)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.082744), ('valence loss', 0.034032), ('arousal loss', 0.048713), ('valence acc', 0.976744), ('arousal acc', 0.988372)]\n",
      "Epoch 00042: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.027057), ('valence loss', 0.027057), ('arousal loss', 0.36038), ('valence acc', 0.988372), ('arousal acc', 0.883721)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.035714), ('valence loss', 0.027972), ('arousal loss', 0.035714), ('valence acc', 0.982558), ('arousal acc', 0.988372)]\n",
      "\n",
      "processing:  04 ......\n",
      "Before fine-tuning: [('loss', 0.503483), ('valence loss', 0.246095), ('arousal loss', 0.257389), ('valence acc', 0.981366), ('arousal acc', 0.968944)]\n",
      "Epoch 00042: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.503483), ('valence loss', 0.246095), ('arousal loss', 0.257389), ('valence acc', 0.981366), ('arousal acc', 0.968944)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.216928), ('valence loss', 0.216928), ('arousal loss', 0.315156), ('valence acc', 0.987578), ('arousal acc', 0.962733)]\n",
      "Epoch 00056: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.257389), ('valence loss', 0.246095), ('arousal loss', 0.257389), ('valence acc', 0.981366), ('arousal acc', 0.968944)]\n",
      "\n",
      "processing:  05 ......\n",
      "Before fine-tuning: [('loss', 0.087399), ('valence loss', 0.036641), ('arousal loss', 0.050759), ('valence acc', 0.988439), ('arousal acc', 0.976879)]\n",
      "Epoch 00026: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.073502), ('valence loss', 0.043719), ('arousal loss', 0.029782), ('valence acc', 0.982659), ('arousal acc', 0.988439)]\n",
      "Epoch 00032: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.033229), ('valence loss', 0.033229), ('arousal loss', 0.070083), ('valence acc', 0.976879), ('arousal acc', 0.959538)]\n",
      "Epoch 00065: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.033569), ('valence loss', 0.055631), ('arousal loss', 0.033569), ('valence acc', 0.965318), ('arousal acc', 0.982659)]\n",
      "\n",
      "processing:  06 ......\n",
      "Before fine-tuning: [('loss', 0.046012), ('valence loss', 0.04246), ('arousal loss', 0.003552), ('valence acc', 0.988506), ('arousal acc', 1.0)]\n",
      "Epoch 00078: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.008089), ('valence loss', 0.005842), ('arousal loss', 0.002246), ('valence acc', 0.994253), ('arousal acc', 1.0)]\n",
      "Epoch 00026: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.005232), ('valence loss', 0.005232), ('arousal loss', 0.031981), ('valence acc', 1.0), ('arousal acc', 0.982759)]\n",
      "Epoch 00047: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.000243), ('valence loss', 0.058855), ('arousal loss', 0.000243), ('valence acc', 0.988506), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  07 ......\n",
      "Before fine-tuning: [('loss', 0.018695), ('valence loss', 0.007147), ('arousal loss', 0.011548), ('valence acc', 0.993289), ('arousal acc', 0.993289)]\n",
      "Epoch 00045: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.004486), ('valence loss', 0.001323), ('arousal loss', 0.003163), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.001594), ('valence loss', 0.001594), ('arousal loss', 0.009968), ('valence acc', 1.0), ('arousal acc', 0.993289)]\n",
      "Epoch 00069: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.000442), ('valence loss', 0.037197), ('arousal loss', 0.000442), ('valence acc', 0.986577), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  08 ......\n",
      "Before fine-tuning: [('loss', 0.167228), ('valence loss', 0.056597), ('arousal loss', 0.110631), ('valence acc', 0.988095), ('arousal acc', 0.970238)]\n",
      "Epoch 00052: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.118187), ('valence loss', 0.042169), ('arousal loss', 0.076018), ('valence acc', 0.988095), ('arousal acc', 0.970238)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.030844), ('valence loss', 0.030844), ('arousal loss', 0.098007), ('valence acc', 0.988095), ('arousal acc', 0.970238)]\n",
      "Epoch 00088: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.083837), ('valence loss', 0.049378), ('arousal loss', 0.083837), ('valence acc', 0.982143), ('arousal acc', 0.970238)]\n",
      "\n",
      "processing:  09 ......\n",
      "Before fine-tuning: [('loss', 0.323492), ('valence loss', 0.177039), ('arousal loss', 0.146453), ('valence acc', 0.969697), ('arousal acc', 0.957576)]\n",
      "Epoch 00019: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.310144), ('valence loss', 0.155383), ('arousal loss', 0.154761), ('valence acc', 0.963636), ('arousal acc', 0.957576)]\n",
      "Epoch 00080: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.148087), ('valence loss', 0.148087), ('arousal loss', 0.162187), ('valence acc', 0.981818), ('arousal acc', 0.957576)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.125421), ('valence loss', 0.172842), ('arousal loss', 0.125421), ('valence acc', 0.975758), ('arousal acc', 0.969697)]\n",
      "\n",
      "processing:  10 ......\n",
      "Before fine-tuning: [('loss', 0.032179), ('valence loss', 0.009462), ('arousal loss', 0.022716), ('valence acc', 0.993865), ('arousal acc', 0.993865)]\n",
      "Epoch 00021: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.025671), ('valence loss', 0.017333), ('arousal loss', 0.008338), ('valence acc', 0.993865), ('arousal acc', 0.993865)]\n",
      "Epoch 00022: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.004998), ('valence loss', 0.004998), ('arousal loss', 0.016839), ('valence acc', 1.0), ('arousal acc', 0.993865)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.002336), ('valence loss', 0.033964), ('arousal loss', 0.002336), ('valence acc', 0.98773), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  11 ......\n",
      "Before fine-tuning: [('loss', 0.281934), ('valence loss', 0.132583), ('arousal loss', 0.149351), ('valence acc', 0.952703), ('arousal acc', 0.972973)]\n",
      "Epoch 00042: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.214353), ('valence loss', 0.064254), ('arousal loss', 0.150099), ('valence acc', 0.966216), ('arousal acc', 0.966216)]\n",
      "Epoch 00022: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.051528), ('valence loss', 0.051528), ('arousal loss', 0.211262), ('valence acc', 0.966216), ('arousal acc', 0.932432)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.107113), ('valence loss', 0.087378), ('arousal loss', 0.107113), ('valence acc', 0.97973), ('arousal acc', 0.966216)]\n",
      "\n",
      "processing:  12 ......\n",
      "Before fine-tuning: [('loss', 0.005582), ('valence loss', 0.002639), ('arousal loss', 0.002942), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "Epoch 00052: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.005582), ('valence loss', 0.002639), ('arousal loss', 0.002942), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.002639), ('valence loss', 0.002639), ('arousal loss', 0.002942), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "Epoch 00033: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.002942), ('valence loss', 0.002639), ('arousal loss', 0.002942), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  13 ......\n",
      "Before fine-tuning: [('loss', 0.133507), ('valence loss', 0.110776), ('arousal loss', 0.022732), ('valence acc', 0.974843), ('arousal acc', 0.993711)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.116326), ('valence loss', 0.08352), ('arousal loss', 0.032805), ('valence acc', 0.962264), ('arousal acc', 0.987421)]\n",
      "Epoch 00032: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.079669), ('valence loss', 0.079669), ('arousal loss', 0.126967), ('valence acc', 0.968553), ('arousal acc', 0.955975)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.015021), ('valence loss', 0.08013), ('arousal loss', 0.015021), ('valence acc', 0.968553), ('arousal acc', 0.993711)]\n",
      "\n",
      "processing:  14 ......\n",
      "Before fine-tuning: [('loss', 0.077033), ('valence loss', 0.015049), ('arousal loss', 0.061984), ('valence acc', 0.993827), ('arousal acc', 0.981481)]\n",
      "Epoch 00032: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.050549), ('valence loss', 0.005836), ('arousal loss', 0.044713), ('valence acc', 1.0), ('arousal acc', 0.975309)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.002832), ('valence loss', 0.002832), ('arousal loss', 0.054993), ('valence acc', 1.0), ('arousal acc', 0.981481)]\n",
      "Epoch 00056: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.021717), ('valence loss', 0.003809), ('arousal loss', 0.021717), ('valence acc', 1.0), ('arousal acc', 0.993827)]\n",
      "\n",
      "processing:  15 ......\n",
      "Before fine-tuning: [('loss', 0.054237), ('valence loss', 0.023401), ('arousal loss', 0.030835), ('valence acc', 0.993506), ('arousal acc', 0.993506)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.054237), ('valence loss', 0.023401), ('arousal loss', 0.030835), ('valence acc', 0.993506), ('arousal acc', 0.993506)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.002404), ('valence loss', 0.002404), ('arousal loss', 0.060038), ('valence acc', 1.0), ('arousal acc', 0.980519)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.010383), ('valence loss', 0.070741), ('arousal loss', 0.010383), ('valence acc', 0.980519), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  16 ......\n",
      "Before fine-tuning: [('loss', 0.041975), ('valence loss', 0.040541), ('arousal loss', 0.001434), ('valence acc', 0.993103), ('arousal acc', 1.0)]\n",
      "Epoch 00071: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.007968), ('valence loss', 0.006472), ('arousal loss', 0.001496), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "Epoch 00044: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.010042), ('valence loss', 0.010042), ('arousal loss', 0.00542), ('valence acc', 0.993103), ('arousal acc', 1.0)]\n",
      "Epoch 00065: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.000287), ('valence loss', 0.044905), ('arousal loss', 0.000287), ('valence acc', 0.97931), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  17 ......\n",
      "Before fine-tuning: [('loss', 0.330845), ('valence loss', 0.090676), ('arousal loss', 0.240169), ('valence acc', 0.95302), ('arousal acc', 0.973154)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.321276), ('valence loss', 0.097417), ('arousal loss', 0.223859), ('valence acc', 0.95302), ('arousal acc', 0.966443)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.085513), ('valence loss', 0.085513), ('arousal loss', 0.221585), ('valence acc', 0.966443), ('arousal acc', 0.959732)]\n",
      "Epoch 00030: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.227396), ('valence loss', 0.106266), ('arousal loss', 0.227396), ('valence acc', 0.946309), ('arousal acc', 0.966443)]\n",
      "\n",
      "processing:  18 ......\n",
      "Before fine-tuning: [('loss', 0.053445), ('valence loss', 0.039041), ('arousal loss', 0.014404), ('valence acc', 0.987013), ('arousal acc', 0.993506)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.050131), ('valence loss', 0.043313), ('arousal loss', 0.006818), ('valence acc', 0.987013), ('arousal acc', 1.0)]\n",
      "Epoch 00056: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.022698), ('valence loss', 0.022698), ('arousal loss', 0.005399), ('valence acc', 0.987013), ('arousal acc', 1.0)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.003158), ('valence loss', 0.049114), ('arousal loss', 0.003158), ('valence acc', 0.987013), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  19 ......\n",
      "Before fine-tuning: [('loss', 0.099356), ('valence loss', 0.089972), ('arousal loss', 0.009384), ('valence acc', 0.968153), ('arousal acc', 1.0)]\n",
      "Epoch 00040: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.095952), ('valence loss', 0.075668), ('arousal loss', 0.020284), ('valence acc', 0.968153), ('arousal acc', 0.993631)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.060454), ('valence loss', 0.060454), ('arousal loss', 0.086048), ('valence acc', 0.974522), ('arousal acc', 0.968153)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.009384), ('valence loss', 0.089972), ('arousal loss', 0.009384), ('valence acc', 0.968153), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  20 ......\n",
      "Before fine-tuning: [('loss', 0.076342), ('valence loss', 0.006992), ('arousal loss', 0.06935), ('valence acc', 1.0), ('arousal acc', 0.986928)]\n",
      "Epoch 00063: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.01961), ('valence loss', 0.012958), ('arousal loss', 0.006652), ('valence acc', 0.986928), ('arousal acc', 1.0)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.006115), ('valence loss', 0.006115), ('arousal loss', 0.033597), ('valence acc', 1.0), ('arousal acc', 0.993464)]\n",
      "Epoch 00063: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.000947), ('valence loss', 0.023516), ('arousal loss', 0.000947), ('valence acc', 0.986928), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  21 ......\n",
      "Before fine-tuning: [('loss', 0.015722), ('valence loss', 0.014925), ('arousal loss', 0.000797), ('valence acc', 0.993711), ('arousal acc', 1.0)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.014636), ('valence loss', 0.011657), ('arousal loss', 0.00298), ('valence acc', 0.987421), ('arousal acc', 1.0)]\n",
      "Epoch 00039: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.003089), ('valence loss', 0.003089), ('arousal loss', 0.081417), ('valence acc', 1.0), ('arousal acc', 0.949686)]\n",
      "Epoch 00033: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.000797), ('valence loss', 0.014925), ('arousal loss', 0.000797), ('valence acc', 0.993711), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  22 ......\n",
      "Before fine-tuning: [('loss', 0.460431), ('valence loss', 0.239513), ('arousal loss', 0.220918), ('valence acc', 0.922619), ('arousal acc', 0.904762)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.460431), ('valence loss', 0.239513), ('arousal loss', 0.220918), ('valence acc', 0.922619), ('arousal acc', 0.904762)]\n",
      "Epoch 00034: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.239513), ('valence loss', 0.239513), ('arousal loss', 0.220918), ('valence acc', 0.922619), ('arousal acc', 0.904762)]\n",
      "Epoch 00028: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.21937), ('valence loss', 0.263965), ('arousal loss', 0.21937), ('valence acc', 0.910714), ('arousal acc', 0.916667)]\n",
      "\n",
      "processing:  23 ......\n",
      "Before fine-tuning: [('loss', 0.055932), ('valence loss', 0.05117), ('arousal loss', 0.004762), ('valence acc', 0.987097), ('arousal acc', 0.993548)]\n",
      "Epoch 00039: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.055932), ('valence loss', 0.05117), ('arousal loss', 0.004762), ('valence acc', 0.987097), ('arousal acc', 0.993548)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.044759), ('valence loss', 0.044759), ('arousal loss', 0.085369), ('valence acc', 0.987097), ('arousal acc', 0.980645)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.004762), ('valence loss', 0.05117), ('arousal loss', 0.004762), ('valence acc', 0.987097), ('arousal acc', 0.993548)]\n",
      "\n",
      "processing:  24 ......\n",
      "Before fine-tuning: [('loss', 0.200945), ('valence loss', 0.159415), ('arousal loss', 0.04153), ('valence acc', 0.945783), ('arousal acc', 0.987952)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.200945), ('valence loss', 0.159415), ('arousal loss', 0.04153), ('valence acc', 0.945783), ('arousal acc', 0.987952)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.158372), ('valence loss', 0.158372), ('arousal loss', 0.044754), ('valence acc', 0.939759), ('arousal acc', 0.975904)]\n",
      "Epoch 00059: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.02519), ('valence loss', 0.288176), ('arousal loss', 0.02519), ('valence acc', 0.921687), ('arousal acc', 0.987952)]\n",
      "\n",
      "processing:  25 ......\n",
      "Before fine-tuning: [('loss', 0.068543), ('valence loss', 0.060718), ('arousal loss', 0.007824), ('valence acc', 0.988889), ('arousal acc', 1.0)]\n",
      "Epoch 00033: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.061924), ('valence loss', 0.054854), ('arousal loss', 0.00707), ('valence acc', 0.988889), ('arousal acc', 1.0)]\n",
      "Epoch 00026: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.047922), ('valence loss', 0.047922), ('arousal loss', 0.037264), ('valence acc', 0.994444), ('arousal acc', 0.977778)]\n",
      "Epoch 00043: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.002942), ('valence loss', 0.031196), ('arousal loss', 0.002942), ('valence acc', 0.994444), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  26 ......\n",
      "Before fine-tuning: [('loss', 0.133076), ('valence loss', 0.061261), ('arousal loss', 0.071815), ('valence acc', 0.975904), ('arousal acc', 0.975904)]\n",
      "Epoch 00032: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.120089), ('valence loss', 0.042173), ('arousal loss', 0.077916), ('valence acc', 0.987952), ('arousal acc', 0.963855)]\n",
      "After fine-tuning on VALENCE [('loss', 0.023332), ('valence loss', 0.023332), ('arousal loss', 0.147041), ('valence acc', 0.987952), ('arousal acc', 0.933735)]\n",
      "Epoch 00037: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.051182), ('valence loss', 0.064963), ('arousal loss', 0.051182), ('valence acc', 0.981928), ('arousal acc', 0.981928)]\n",
      "\n",
      "processing:  27 ......\n",
      "Before fine-tuning: [('loss', 0.023607), ('valence loss', 0.018417), ('arousal loss', 0.00519), ('valence acc', 0.993421), ('arousal acc', 1.0)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.023607), ('valence loss', 0.018417), ('arousal loss', 0.00519), ('valence acc', 0.993421), ('arousal acc', 1.0)]\n",
      "Epoch 00037: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.015101), ('valence loss', 0.015101), ('arousal loss', 0.007406), ('valence acc', 0.993421), ('arousal acc', 0.993421)]\n",
      "Epoch 00040: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.00519), ('valence loss', 0.018417), ('arousal loss', 0.00519), ('valence acc', 0.993421), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  28 ......\n",
      "Before fine-tuning: [('loss', 0.084495), ('valence loss', 0.035094), ('arousal loss', 0.049401), ('valence acc', 0.984211), ('arousal acc', 0.989474)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.084495), ('valence loss', 0.035094), ('arousal loss', 0.049401), ('valence acc', 0.984211), ('arousal acc', 0.989474)]\n",
      "Epoch 00090: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.009922), ('valence loss', 0.009922), ('arousal loss', 0.09473), ('valence acc', 0.994737), ('arousal acc', 0.952632)]\n",
      "Epoch 00028: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.049401), ('valence loss', 0.035094), ('arousal loss', 0.049401), ('valence acc', 0.984211), ('arousal acc', 0.989474)]\n",
      "\n",
      "processing:  29 ......\n",
      "Before fine-tuning: [('loss', 0.021026), ('valence loss', 0.01806), ('arousal loss', 0.002966), ('valence acc', 0.986395), ('arousal acc', 1.0)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.016763), ('valence loss', 0.014195), ('arousal loss', 0.002569), ('valence acc', 0.986395), ('arousal acc', 1.0)]\n",
      "Epoch 00071: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.012811), ('valence loss', 0.012811), ('arousal loss', 0.002455), ('valence acc', 0.993197), ('arousal acc', 1.0)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.002703), ('valence loss', 0.013375), ('arousal loss', 0.002703), ('valence acc', 0.986395), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  30 ......\n",
      "Before fine-tuning: [('loss', 0.067554), ('valence loss', 0.035744), ('arousal loss', 0.03181), ('valence acc', 0.985816), ('arousal acc', 0.992908)]\n",
      "Epoch 00060: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.039939), ('valence loss', 0.02364), ('arousal loss', 0.0163), ('valence acc', 0.985816), ('arousal acc', 0.992908)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.019598), ('valence loss', 0.019598), ('arousal loss', 0.048046), ('valence acc', 0.985816), ('arousal acc', 0.971631)]\n",
      "Epoch 00033: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.014753), ('valence loss', 0.09254), ('arousal loss', 0.014753), ('valence acc', 0.957447), ('arousal acc', 0.992908)]\n",
      "\n",
      "processing:  31 ......\n",
      "Before fine-tuning: [('loss', 0.182829), ('valence loss', 0.075055), ('arousal loss', 0.107774), ('valence acc', 0.979866), ('arousal acc', 0.966443)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.16919), ('valence loss', 0.067538), ('arousal loss', 0.101652), ('valence acc', 0.979866), ('arousal acc', 0.973154)]\n",
      "Epoch 00033: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.075055), ('valence loss', 0.075055), ('arousal loss', 0.107774), ('valence acc', 0.979866), ('arousal acc', 0.966443)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.1041), ('valence loss', 0.090974), ('arousal loss', 0.1041), ('valence acc', 0.979866), ('arousal acc', 0.986577)]\n",
      "\n",
      "processing:  32 ......\n",
      "Before fine-tuning: [('loss', 0.062582), ('valence loss', 0.023567), ('arousal loss', 0.039015), ('valence acc', 0.994083), ('arousal acc', 0.988166)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.062336), ('valence loss', 0.023415), ('arousal loss', 0.03892), ('valence acc', 0.994083), ('arousal acc', 0.982249)]\n",
      "Epoch 00035: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.012034), ('valence loss', 0.012034), ('arousal loss', 0.088419), ('valence acc', 0.994083), ('arousal acc', 0.976331)]\n",
      "Epoch 00071: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.03155), ('valence loss', 0.021338), ('arousal loss', 0.03155), ('valence acc', 0.994083), ('arousal acc', 0.988166)]\n",
      "\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "320/320 [==============================] - 15s 37ms/step - loss: 1.3138 - out_v_loss: 0.6701 - out_a_loss: 0.6437 - out_v_accuracy: 0.6369 - out_a_accuracy: 0.6574 - val_loss: 1.1288 - val_out_v_loss: 0.5921 - val_out_a_loss: 0.5367 - val_out_v_accuracy: 0.6801 - val_out_a_accuracy: 0.7258 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 1.0251 - out_v_loss: 0.5209 - out_a_loss: 0.5042 - out_v_accuracy: 0.7314 - out_a_accuracy: 0.7479 - val_loss: 0.9005 - val_out_v_loss: 0.4643 - val_out_a_loss: 0.4362 - val_out_v_accuracy: 0.7750 - val_out_a_accuracy: 0.7873 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.8361 - out_v_loss: 0.4268 - out_a_loss: 0.4093 - out_v_accuracy: 0.7951 - out_a_accuracy: 0.8047 - val_loss: 0.7938 - val_out_v_loss: 0.4142 - val_out_a_loss: 0.3796 - val_out_v_accuracy: 0.8105 - val_out_a_accuracy: 0.8205 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.7048 - out_v_loss: 0.3571 - out_a_loss: 0.3477 - out_v_accuracy: 0.8350 - out_a_accuracy: 0.8419 - val_loss: 0.6187 - val_out_v_loss: 0.3326 - val_out_a_loss: 0.2861 - val_out_v_accuracy: 0.8506 - val_out_a_accuracy: 0.8723 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.6019 - out_v_loss: 0.3065 - out_a_loss: 0.2954 - out_v_accuracy: 0.8612 - out_a_accuracy: 0.8670 - val_loss: 0.5568 - val_out_v_loss: 0.2683 - val_out_a_loss: 0.2885 - val_out_v_accuracy: 0.8813 - val_out_a_accuracy: 0.8762 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.5264 - out_v_loss: 0.2655 - out_a_loss: 0.2608 - out_v_accuracy: 0.8817 - out_a_accuracy: 0.8838 - val_loss: 0.4763 - val_out_v_loss: 0.2427 - val_out_a_loss: 0.2337 - val_out_v_accuracy: 0.8951 - val_out_a_accuracy: 0.8967 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.4584 - out_v_loss: 0.2316 - out_a_loss: 0.2267 - out_v_accuracy: 0.9006 - out_a_accuracy: 0.9011 - val_loss: 0.4312 - val_out_v_loss: 0.2187 - val_out_a_loss: 0.2125 - val_out_v_accuracy: 0.9051 - val_out_a_accuracy: 0.9094 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.4272 - out_v_loss: 0.2164 - out_a_loss: 0.2108 - out_v_accuracy: 0.9064 - out_a_accuracy: 0.9083 - val_loss: 0.3880 - val_out_v_loss: 0.1996 - val_out_a_loss: 0.1884 - val_out_v_accuracy: 0.9137 - val_out_a_accuracy: 0.9195 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3819 - out_v_loss: 0.1921 - out_a_loss: 0.1898 - out_v_accuracy: 0.9187 - out_a_accuracy: 0.9177 - val_loss: 0.3809 - val_out_v_loss: 0.1883 - val_out_a_loss: 0.1926 - val_out_v_accuracy: 0.9199 - val_out_a_accuracy: 0.9176 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.3513 - out_v_loss: 0.1758 - out_a_loss: 0.1755 - out_v_accuracy: 0.9248 - out_a_accuracy: 0.9263 - val_loss: 0.3712 - val_out_v_loss: 0.1900 - val_out_a_loss: 0.1812 - val_out_v_accuracy: 0.9225 - val_out_a_accuracy: 0.9244 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.3220 - out_v_loss: 0.1647 - out_a_loss: 0.1573 - out_v_accuracy: 0.9318 - out_a_accuracy: 0.9325 - val_loss: 0.3690 - val_out_v_loss: 0.1696 - val_out_a_loss: 0.1995 - val_out_v_accuracy: 0.9289 - val_out_a_accuracy: 0.9230 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3058 - out_v_loss: 0.1531 - out_a_loss: 0.1527 - out_v_accuracy: 0.9358 - out_a_accuracy: 0.9360 - val_loss: 0.3246 - val_out_v_loss: 0.1702 - val_out_a_loss: 0.1543 - val_out_v_accuracy: 0.9322 - val_out_a_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2839 - out_v_loss: 0.1410 - out_a_loss: 0.1429 - out_v_accuracy: 0.9410 - out_a_accuracy: 0.9389 - val_loss: 0.3566 - val_out_v_loss: 0.1580 - val_out_a_loss: 0.1986 - val_out_v_accuracy: 0.9379 - val_out_a_accuracy: 0.9191 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2710 - out_v_loss: 0.1328 - out_a_loss: 0.1382 - out_v_accuracy: 0.9458 - out_a_accuracy: 0.9423 - val_loss: 0.3041 - val_out_v_loss: 0.1568 - val_out_a_loss: 0.1474 - val_out_v_accuracy: 0.9420 - val_out_a_accuracy: 0.9395 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2547 - out_v_loss: 0.1274 - out_a_loss: 0.1274 - out_v_accuracy: 0.9489 - out_a_accuracy: 0.9475 - val_loss: 0.2988 - val_out_v_loss: 0.1436 - val_out_a_loss: 0.1553 - val_out_v_accuracy: 0.9432 - val_out_a_accuracy: 0.9393 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2408 - out_v_loss: 0.1192 - out_a_loss: 0.1216 - out_v_accuracy: 0.9521 - out_a_accuracy: 0.9491 - val_loss: 0.2657 - val_out_v_loss: 0.1360 - val_out_a_loss: 0.1296 - val_out_v_accuracy: 0.9477 - val_out_a_accuracy: 0.9445 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.2295 - out_v_loss: 0.1133 - out_a_loss: 0.1162 - out_v_accuracy: 0.9521 - out_a_accuracy: 0.9532 - val_loss: 0.2689 - val_out_v_loss: 0.1311 - val_out_a_loss: 0.1378 - val_out_v_accuracy: 0.9484 - val_out_a_accuracy: 0.9430 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "320/320 [==============================] - 11s 36ms/step - loss: 0.2182 - out_v_loss: 0.1078 - out_a_loss: 0.1104 - out_v_accuracy: 0.9561 - out_a_accuracy: 0.9552 - val_loss: 0.2551 - val_out_v_loss: 0.1315 - val_out_a_loss: 0.1236 - val_out_v_accuracy: 0.9473 - val_out_a_accuracy: 0.9496 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2273 - out_v_loss: 0.1117 - out_a_loss: 0.1156 - out_v_accuracy: 0.9561 - out_a_accuracy: 0.9525 - val_loss: 0.2438 - val_out_v_loss: 0.1194 - val_out_a_loss: 0.1244 - val_out_v_accuracy: 0.9551 - val_out_a_accuracy: 0.9492 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1971 - out_v_loss: 0.0999 - out_a_loss: 0.0972 - out_v_accuracy: 0.9598 - out_a_accuracy: 0.9601 - val_loss: 0.2720 - val_out_v_loss: 0.1522 - val_out_a_loss: 0.1198 - val_out_v_accuracy: 0.9463 - val_out_a_accuracy: 0.9521 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1963 - out_v_loss: 0.0988 - out_a_loss: 0.0976 - out_v_accuracy: 0.9602 - out_a_accuracy: 0.9605 - val_loss: 0.2490 - val_out_v_loss: 0.1299 - val_out_a_loss: 0.1191 - val_out_v_accuracy: 0.9514 - val_out_a_accuracy: 0.9500 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1903 - out_v_loss: 0.0961 - out_a_loss: 0.0941 - out_v_accuracy: 0.9609 - out_a_accuracy: 0.9612 - val_loss: 0.2112 - val_out_v_loss: 0.1069 - val_out_a_loss: 0.1043 - val_out_v_accuracy: 0.9623 - val_out_a_accuracy: 0.9551 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1863 - out_v_loss: 0.0923 - out_a_loss: 0.0940 - out_v_accuracy: 0.9627 - out_a_accuracy: 0.9623 - val_loss: 0.2150 - val_out_v_loss: 0.1048 - val_out_a_loss: 0.1102 - val_out_v_accuracy: 0.9582 - val_out_a_accuracy: 0.9580 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1777 - out_v_loss: 0.0893 - out_a_loss: 0.0884 - out_v_accuracy: 0.9637 - out_a_accuracy: 0.9656 - val_loss: 0.2513 - val_out_v_loss: 0.1296 - val_out_a_loss: 0.1217 - val_out_v_accuracy: 0.9533 - val_out_a_accuracy: 0.9553 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1618 - out_v_loss: 0.0820 - out_a_loss: 0.0798 - out_v_accuracy: 0.9679 - out_a_accuracy: 0.9674 - val_loss: 0.2308 - val_out_v_loss: 0.1176 - val_out_a_loss: 0.1133 - val_out_v_accuracy: 0.9537 - val_out_a_accuracy: 0.9559 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1541 - out_v_loss: 0.0745 - out_a_loss: 0.0795 - out_v_accuracy: 0.9719 - out_a_accuracy: 0.9677 - val_loss: 0.2126 - val_out_v_loss: 0.1078 - val_out_a_loss: 0.1049 - val_out_v_accuracy: 0.9574 - val_out_a_accuracy: 0.9578 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "320/320 [==============================] - 11s 36ms/step - loss: 0.1566 - out_v_loss: 0.0808 - out_a_loss: 0.0759 - out_v_accuracy: 0.9683 - out_a_accuracy: 0.9694 - val_loss: 0.2070 - val_out_v_loss: 0.1046 - val_out_a_loss: 0.1024 - val_out_v_accuracy: 0.9596 - val_out_a_accuracy: 0.9607 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1520 - out_v_loss: 0.0758 - out_a_loss: 0.0763 - out_v_accuracy: 0.9688 - out_a_accuracy: 0.9703 - val_loss: 0.2394 - val_out_v_loss: 0.1201 - val_out_a_loss: 0.1193 - val_out_v_accuracy: 0.9563 - val_out_a_accuracy: 0.9521 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1651 - out_v_loss: 0.0801 - out_a_loss: 0.0850 - out_v_accuracy: 0.9695 - out_a_accuracy: 0.9663 - val_loss: 0.2094 - val_out_v_loss: 0.1010 - val_out_a_loss: 0.1083 - val_out_v_accuracy: 0.9602 - val_out_a_accuracy: 0.9570 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1511 - out_v_loss: 0.0760 - out_a_loss: 0.0751 - out_v_accuracy: 0.9692 - out_a_accuracy: 0.9699 - val_loss: 0.2437 - val_out_v_loss: 0.1247 - val_out_a_loss: 0.1190 - val_out_v_accuracy: 0.9539 - val_out_a_accuracy: 0.9578 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1440 - out_v_loss: 0.0695 - out_a_loss: 0.0745 - out_v_accuracy: 0.9720 - out_a_accuracy: 0.9699 - val_loss: 0.2204 - val_out_v_loss: 0.1106 - val_out_a_loss: 0.1098 - val_out_v_accuracy: 0.9613 - val_out_a_accuracy: 0.9600 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1420 - out_v_loss: 0.0699 - out_a_loss: 0.0721 - out_v_accuracy: 0.9733 - out_a_accuracy: 0.9705 - val_loss: 0.2041 - val_out_v_loss: 0.1031 - val_out_a_loss: 0.1010 - val_out_v_accuracy: 0.9609 - val_out_a_accuracy: 0.9623 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1387 - out_v_loss: 0.0705 - out_a_loss: 0.0682 - out_v_accuracy: 0.9725 - out_a_accuracy: 0.9738 - val_loss: 0.2157 - val_out_v_loss: 0.1214 - val_out_a_loss: 0.0943 - val_out_v_accuracy: 0.9559 - val_out_a_accuracy: 0.9645 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1396 - out_v_loss: 0.0676 - out_a_loss: 0.0721 - out_v_accuracy: 0.9734 - out_a_accuracy: 0.9710 - val_loss: 0.2313 - val_out_v_loss: 0.1216 - val_out_a_loss: 0.1097 - val_out_v_accuracy: 0.9570 - val_out_a_accuracy: 0.9586 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "320/320 [==============================] - 11s 36ms/step - loss: 0.1365 - out_v_loss: 0.0657 - out_a_loss: 0.0708 - out_v_accuracy: 0.9742 - out_a_accuracy: 0.9710 - val_loss: 0.1908 - val_out_v_loss: 0.0942 - val_out_a_loss: 0.0966 - val_out_v_accuracy: 0.9635 - val_out_a_accuracy: 0.9648 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1284 - out_v_loss: 0.0593 - out_a_loss: 0.0691 - out_v_accuracy: 0.9767 - out_a_accuracy: 0.9729 - val_loss: 0.2057 - val_out_v_loss: 0.1007 - val_out_a_loss: 0.1049 - val_out_v_accuracy: 0.9645 - val_out_a_accuracy: 0.9635 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1304 - out_v_loss: 0.0681 - out_a_loss: 0.0623 - out_v_accuracy: 0.9727 - out_a_accuracy: 0.9749 - val_loss: 0.1922 - val_out_v_loss: 0.1008 - val_out_a_loss: 0.0914 - val_out_v_accuracy: 0.9629 - val_out_a_accuracy: 0.9670 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1293 - out_v_loss: 0.0654 - out_a_loss: 0.0639 - out_v_accuracy: 0.9749 - out_a_accuracy: 0.9760 - val_loss: 0.2152 - val_out_v_loss: 0.1134 - val_out_a_loss: 0.1017 - val_out_v_accuracy: 0.9594 - val_out_a_accuracy: 0.9627 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1254 - out_v_loss: 0.0610 - out_a_loss: 0.0643 - out_v_accuracy: 0.9765 - out_a_accuracy: 0.9740 - val_loss: 0.1917 - val_out_v_loss: 0.0956 - val_out_a_loss: 0.0961 - val_out_v_accuracy: 0.9637 - val_out_a_accuracy: 0.9646 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1211 - out_v_loss: 0.0601 - out_a_loss: 0.0609 - out_v_accuracy: 0.9760 - out_a_accuracy: 0.9758 - val_loss: 0.1831 - val_out_v_loss: 0.0918 - val_out_a_loss: 0.0913 - val_out_v_accuracy: 0.9682 - val_out_a_accuracy: 0.9650 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1166 - out_v_loss: 0.0593 - out_a_loss: 0.0573 - out_v_accuracy: 0.9776 - out_a_accuracy: 0.9769 - val_loss: 0.1990 - val_out_v_loss: 0.0958 - val_out_a_loss: 0.1032 - val_out_v_accuracy: 0.9662 - val_out_a_accuracy: 0.9674 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1169 - out_v_loss: 0.0580 - out_a_loss: 0.0589 - out_v_accuracy: 0.9781 - out_a_accuracy: 0.9773 - val_loss: 0.1984 - val_out_v_loss: 0.1024 - val_out_a_loss: 0.0960 - val_out_v_accuracy: 0.9643 - val_out_a_accuracy: 0.9607 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "320/320 [==============================] - 11s 36ms/step - loss: 0.1227 - out_v_loss: 0.0601 - out_a_loss: 0.0626 - out_v_accuracy: 0.9761 - out_a_accuracy: 0.9743 - val_loss: 0.1763 - val_out_v_loss: 0.0965 - val_out_a_loss: 0.0798 - val_out_v_accuracy: 0.9646 - val_out_a_accuracy: 0.9707 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1176 - out_v_loss: 0.0558 - out_a_loss: 0.0618 - out_v_accuracy: 0.9772 - out_a_accuracy: 0.9758 - val_loss: 0.1975 - val_out_v_loss: 0.1032 - val_out_a_loss: 0.0943 - val_out_v_accuracy: 0.9627 - val_out_a_accuracy: 0.9619 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1131 - out_v_loss: 0.0537 - out_a_loss: 0.0594 - out_v_accuracy: 0.9797 - out_a_accuracy: 0.9766 - val_loss: 0.2017 - val_out_v_loss: 0.1078 - val_out_a_loss: 0.0939 - val_out_v_accuracy: 0.9652 - val_out_a_accuracy: 0.9670 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1002 - out_v_loss: 0.0497 - out_a_loss: 0.0505 - out_v_accuracy: 0.9804 - out_a_accuracy: 0.9797 - val_loss: 0.2013 - val_out_v_loss: 0.1089 - val_out_a_loss: 0.0924 - val_out_v_accuracy: 0.9637 - val_out_a_accuracy: 0.9678 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0958 - out_v_loss: 0.0463 - out_a_loss: 0.0495 - out_v_accuracy: 0.9822 - out_a_accuracy: 0.9807 - val_loss: 0.1813 - val_out_v_loss: 0.0974 - val_out_a_loss: 0.0839 - val_out_v_accuracy: 0.9650 - val_out_a_accuracy: 0.9707 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.1094 - out_v_loss: 0.0508 - out_a_loss: 0.0586 - out_v_accuracy: 0.9813 - out_a_accuracy: 0.9773\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1091 - out_v_loss: 0.0507 - out_a_loss: 0.0584 - out_v_accuracy: 0.9814 - out_a_accuracy: 0.9773 - val_loss: 0.1894 - val_out_v_loss: 0.1064 - val_out_a_loss: 0.0831 - val_out_v_accuracy: 0.9672 - val_out_a_accuracy: 0.9711 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0678 - out_v_loss: 0.0332 - out_a_loss: 0.0346 - out_v_accuracy: 0.9873 - out_a_accuracy: 0.9870 - val_loss: 0.1613 - val_out_v_loss: 0.0890 - val_out_a_loss: 0.0723 - val_out_v_accuracy: 0.9695 - val_out_a_accuracy: 0.9746 - lr: 5.0000e-04\n",
      "Epoch 50/200\n",
      "320/320 [==============================] - 12s 37ms/step - loss: 0.0640 - out_v_loss: 0.0324 - out_a_loss: 0.0316 - out_v_accuracy: 0.9875 - out_a_accuracy: 0.9878 - val_loss: 0.1468 - val_out_v_loss: 0.0797 - val_out_a_loss: 0.0671 - val_out_v_accuracy: 0.9742 - val_out_a_accuracy: 0.9777 - lr: 5.0000e-04\n",
      "Epoch 51/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.0537 - out_v_loss: 0.0259 - out_a_loss: 0.0278 - out_v_accuracy: 0.9899 - out_a_accuracy: 0.9889 - val_loss: 0.1513 - val_out_v_loss: 0.0803 - val_out_a_loss: 0.0710 - val_out_v_accuracy: 0.9727 - val_out_a_accuracy: 0.9758 - lr: 5.0000e-04\n",
      "Epoch 52/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0535 - out_v_loss: 0.0292 - out_a_loss: 0.0244 - out_v_accuracy: 0.9897 - out_a_accuracy: 0.9906 - val_loss: 0.1511 - val_out_v_loss: 0.0785 - val_out_a_loss: 0.0726 - val_out_v_accuracy: 0.9742 - val_out_a_accuracy: 0.9742 - lr: 5.0000e-04\n",
      "Epoch 53/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0561 - out_v_loss: 0.0288 - out_a_loss: 0.0274 - out_v_accuracy: 0.9896 - out_a_accuracy: 0.9889 - val_loss: 0.1546 - val_out_v_loss: 0.0851 - val_out_a_loss: 0.0695 - val_out_v_accuracy: 0.9725 - val_out_a_accuracy: 0.9758 - lr: 5.0000e-04\n",
      "Epoch 54/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0541 - out_v_loss: 0.0258 - out_a_loss: 0.0283 - out_v_accuracy: 0.9909 - out_a_accuracy: 0.9894 - val_loss: 0.1463 - val_out_v_loss: 0.0781 - val_out_a_loss: 0.0682 - val_out_v_accuracy: 0.9740 - val_out_a_accuracy: 0.9775 - lr: 5.0000e-04\n",
      "Epoch 55/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0564 - out_v_loss: 0.0287 - out_a_loss: 0.0277 - out_v_accuracy: 0.9894 - out_a_accuracy: 0.9897 - val_loss: 0.1549 - val_out_v_loss: 0.0867 - val_out_a_loss: 0.0682 - val_out_v_accuracy: 0.9699 - val_out_a_accuracy: 0.9775 - lr: 5.0000e-04\n",
      "Epoch 56/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0552 - out_v_loss: 0.0262 - out_a_loss: 0.0291 - out_v_accuracy: 0.9907 - out_a_accuracy: 0.9890 - val_loss: 0.1464 - val_out_v_loss: 0.0816 - val_out_a_loss: 0.0648 - val_out_v_accuracy: 0.9738 - val_out_a_accuracy: 0.9789 - lr: 5.0000e-04\n",
      "Epoch 57/200\n",
      "320/320 [==============================] - 11s 36ms/step - loss: 0.0561 - out_v_loss: 0.0293 - out_a_loss: 0.0269 - out_v_accuracy: 0.9896 - out_a_accuracy: 0.9899 - val_loss: 0.1456 - val_out_v_loss: 0.0831 - val_out_a_loss: 0.0625 - val_out_v_accuracy: 0.9738 - val_out_a_accuracy: 0.9781 - lr: 5.0000e-04\n",
      "Epoch 58/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0540 - out_v_loss: 0.0261 - out_a_loss: 0.0279 - out_v_accuracy: 0.9895 - out_a_accuracy: 0.9885 - val_loss: 0.1489 - val_out_v_loss: 0.0816 - val_out_a_loss: 0.0674 - val_out_v_accuracy: 0.9723 - val_out_a_accuracy: 0.9762 - lr: 5.0000e-04\n",
      "Epoch 59/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0485 - out_v_loss: 0.0222 - out_a_loss: 0.0263 - out_v_accuracy: 0.9917 - out_a_accuracy: 0.9895 - val_loss: 0.1472 - val_out_v_loss: 0.0834 - val_out_a_loss: 0.0638 - val_out_v_accuracy: 0.9742 - val_out_a_accuracy: 0.9795 - lr: 5.0000e-04\n",
      "Epoch 60/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0532 - out_v_loss: 0.0265 - out_a_loss: 0.0268 - out_v_accuracy: 0.9895 - out_a_accuracy: 0.9890 - val_loss: 0.1446 - val_out_v_loss: 0.0801 - val_out_a_loss: 0.0645 - val_out_v_accuracy: 0.9773 - val_out_a_accuracy: 0.9775 - lr: 5.0000e-04\n",
      "Epoch 61/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0472 - out_v_loss: 0.0221 - out_a_loss: 0.0252 - out_v_accuracy: 0.9923 - out_a_accuracy: 0.9906 - val_loss: 0.1565 - val_out_v_loss: 0.0834 - val_out_a_loss: 0.0731 - val_out_v_accuracy: 0.9744 - val_out_a_accuracy: 0.9770 - lr: 5.0000e-04\n",
      "Epoch 62/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0457 - out_v_loss: 0.0216 - out_a_loss: 0.0241 - out_v_accuracy: 0.9922 - out_a_accuracy: 0.9906 - val_loss: 0.1436 - val_out_v_loss: 0.0771 - val_out_a_loss: 0.0665 - val_out_v_accuracy: 0.9762 - val_out_a_accuracy: 0.9803 - lr: 5.0000e-04\n",
      "Epoch 63/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0515 - out_v_loss: 0.0255 - out_a_loss: 0.0260 - out_v_accuracy: 0.9899 - out_a_accuracy: 0.9897 - val_loss: 0.1423 - val_out_v_loss: 0.0810 - val_out_a_loss: 0.0614 - val_out_v_accuracy: 0.9748 - val_out_a_accuracy: 0.9785 - lr: 5.0000e-04\n",
      "Epoch 64/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0472 - out_v_loss: 0.0237 - out_a_loss: 0.0235 - out_v_accuracy: 0.9907 - out_a_accuracy: 0.9912 - val_loss: 0.1332 - val_out_v_loss: 0.0738 - val_out_a_loss: 0.0594 - val_out_v_accuracy: 0.9777 - val_out_a_accuracy: 0.9789 - lr: 5.0000e-04\n",
      "Epoch 65/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0444 - out_v_loss: 0.0223 - out_a_loss: 0.0222 - out_v_accuracy: 0.9912 - out_a_accuracy: 0.9918 - val_loss: 0.1633 - val_out_v_loss: 0.0826 - val_out_a_loss: 0.0807 - val_out_v_accuracy: 0.9730 - val_out_a_accuracy: 0.9742 - lr: 5.0000e-04\n",
      "Epoch 66/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0460 - out_v_loss: 0.0232 - out_a_loss: 0.0228 - out_v_accuracy: 0.9914 - out_a_accuracy: 0.9918 - val_loss: 0.1404 - val_out_v_loss: 0.0716 - val_out_a_loss: 0.0688 - val_out_v_accuracy: 0.9744 - val_out_a_accuracy: 0.9781 - lr: 5.0000e-04\n",
      "Epoch 67/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0441 - out_v_loss: 0.0207 - out_a_loss: 0.0235 - out_v_accuracy: 0.9919 - out_a_accuracy: 0.9911 - val_loss: 0.1536 - val_out_v_loss: 0.0831 - val_out_a_loss: 0.0705 - val_out_v_accuracy: 0.9732 - val_out_a_accuracy: 0.9758 - lr: 5.0000e-04\n",
      "Epoch 68/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0441 - out_v_loss: 0.0222 - out_a_loss: 0.0219 - out_v_accuracy: 0.9912 - out_a_accuracy: 0.9919 - val_loss: 0.1516 - val_out_v_loss: 0.0833 - val_out_a_loss: 0.0682 - val_out_v_accuracy: 0.9734 - val_out_a_accuracy: 0.9779 - lr: 5.0000e-04\n",
      "Epoch 69/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0435 - out_v_loss: 0.0242 - out_a_loss: 0.0192 - out_v_accuracy: 0.9904 - out_a_accuracy: 0.9932\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0434 - out_v_loss: 0.0242 - out_a_loss: 0.0192 - out_v_accuracy: 0.9905 - out_a_accuracy: 0.9932 - val_loss: 0.1475 - val_out_v_loss: 0.0826 - val_out_a_loss: 0.0649 - val_out_v_accuracy: 0.9748 - val_out_a_accuracy: 0.9791 - lr: 5.0000e-04\n",
      "Epoch 70/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0367 - out_v_loss: 0.0172 - out_a_loss: 0.0195 - out_v_accuracy: 0.9940 - out_a_accuracy: 0.9928 - val_loss: 0.1360 - val_out_v_loss: 0.0719 - val_out_a_loss: 0.0642 - val_out_v_accuracy: 0.9775 - val_out_a_accuracy: 0.9805 - lr: 2.5000e-04\n",
      "Epoch 71/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0295 - out_v_loss: 0.0144 - out_a_loss: 0.0151 - out_v_accuracy: 0.9946 - out_a_accuracy: 0.9944 - val_loss: 0.1314 - val_out_v_loss: 0.0681 - val_out_a_loss: 0.0633 - val_out_v_accuracy: 0.9787 - val_out_a_accuracy: 0.9816 - lr: 2.5000e-04\n",
      "Epoch 72/200\n",
      "320/320 [==============================] - 11s 36ms/step - loss: 0.0310 - out_v_loss: 0.0161 - out_a_loss: 0.0149 - out_v_accuracy: 0.9939 - out_a_accuracy: 0.9949 - val_loss: 0.1253 - val_out_v_loss: 0.0667 - val_out_a_loss: 0.0586 - val_out_v_accuracy: 0.9795 - val_out_a_accuracy: 0.9809 - lr: 2.5000e-04\n",
      "Epoch 73/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0288 - out_v_loss: 0.0121 - out_a_loss: 0.0167 - out_v_accuracy: 0.9957 - out_a_accuracy: 0.9936 - val_loss: 0.1321 - val_out_v_loss: 0.0640 - val_out_a_loss: 0.0680 - val_out_v_accuracy: 0.9803 - val_out_a_accuracy: 0.9787 - lr: 2.5000e-04\n",
      "Epoch 74/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0289 - out_v_loss: 0.0129 - out_a_loss: 0.0160 - out_v_accuracy: 0.9955 - out_a_accuracy: 0.9946 - val_loss: 0.1320 - val_out_v_loss: 0.0656 - val_out_a_loss: 0.0664 - val_out_v_accuracy: 0.9789 - val_out_a_accuracy: 0.9789 - lr: 2.5000e-04\n",
      "Epoch 75/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0288 - out_v_loss: 0.0130 - out_a_loss: 0.0158 - out_v_accuracy: 0.9948 - out_a_accuracy: 0.9942 - val_loss: 0.1366 - val_out_v_loss: 0.0701 - val_out_a_loss: 0.0664 - val_out_v_accuracy: 0.9775 - val_out_a_accuracy: 0.9795 - lr: 2.5000e-04\n",
      "Epoch 76/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0268 - out_v_loss: 0.0142 - out_a_loss: 0.0126 - out_v_accuracy: 0.9946 - out_a_accuracy: 0.9958 - val_loss: 0.1335 - val_out_v_loss: 0.0698 - val_out_a_loss: 0.0637 - val_out_v_accuracy: 0.9770 - val_out_a_accuracy: 0.9795 - lr: 2.5000e-04\n",
      "Epoch 77/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0263 - out_v_loss: 0.0147 - out_a_loss: 0.0115 - out_v_accuracy: 0.9947 - out_a_accuracy: 0.9956\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0262 - out_v_loss: 0.0147 - out_a_loss: 0.0115 - out_v_accuracy: 0.9947 - out_a_accuracy: 0.9956 - val_loss: 0.1337 - val_out_v_loss: 0.0713 - val_out_a_loss: 0.0624 - val_out_v_accuracy: 0.9771 - val_out_a_accuracy: 0.9793 - lr: 2.5000e-04\n",
      "Epoch 78/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0242 - out_v_loss: 0.0119 - out_a_loss: 0.0123 - out_v_accuracy: 0.9955 - out_a_accuracy: 0.9954 - val_loss: 0.1362 - val_out_v_loss: 0.0695 - val_out_a_loss: 0.0667 - val_out_v_accuracy: 0.9795 - val_out_a_accuracy: 0.9785 - lr: 1.2500e-04\n",
      "Epoch 79/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0218 - out_v_loss: 0.0113 - out_a_loss: 0.0105 - out_v_accuracy: 0.9954 - out_a_accuracy: 0.9963 - val_loss: 0.1242 - val_out_v_loss: 0.0632 - val_out_a_loss: 0.0610 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9816 - lr: 1.2500e-04\n",
      "Epoch 80/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0203 - out_v_loss: 0.0091 - out_a_loss: 0.0112 - out_v_accuracy: 0.9968 - out_a_accuracy: 0.9961 - val_loss: 0.1266 - val_out_v_loss: 0.0635 - val_out_a_loss: 0.0632 - val_out_v_accuracy: 0.9797 - val_out_a_accuracy: 0.9807 - lr: 1.2500e-04\n",
      "Epoch 81/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0225 - out_v_loss: 0.0104 - out_a_loss: 0.0121 - out_v_accuracy: 0.9966 - out_a_accuracy: 0.9956 - val_loss: 0.1251 - val_out_v_loss: 0.0631 - val_out_a_loss: 0.0620 - val_out_v_accuracy: 0.9793 - val_out_a_accuracy: 0.9809 - lr: 1.2500e-04\n",
      "Epoch 82/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0195 - out_v_loss: 0.0105 - out_a_loss: 0.0090 - out_v_accuracy: 0.9961 - out_a_accuracy: 0.9969 - val_loss: 0.1228 - val_out_v_loss: 0.0618 - val_out_a_loss: 0.0610 - val_out_v_accuracy: 0.9816 - val_out_a_accuracy: 0.9818 - lr: 1.2500e-04\n",
      "Epoch 83/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0189 - out_v_loss: 0.0096 - out_a_loss: 0.0094 - out_v_accuracy: 0.9960 - out_a_accuracy: 0.9965 - val_loss: 0.1240 - val_out_v_loss: 0.0630 - val_out_a_loss: 0.0610 - val_out_v_accuracy: 0.9797 - val_out_a_accuracy: 0.9809 - lr: 1.2500e-04\n",
      "Epoch 84/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0202 - out_v_loss: 0.0103 - out_a_loss: 0.0099 - out_v_accuracy: 0.9961 - out_a_accuracy: 0.9964 - val_loss: 0.1246 - val_out_v_loss: 0.0667 - val_out_a_loss: 0.0580 - val_out_v_accuracy: 0.9791 - val_out_a_accuracy: 0.9818 - lr: 1.2500e-04\n",
      "Epoch 85/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0165 - out_v_loss: 0.0079 - out_a_loss: 0.0085 - out_v_accuracy: 0.9970 - out_a_accuracy: 0.9970 - val_loss: 0.1241 - val_out_v_loss: 0.0639 - val_out_a_loss: 0.0602 - val_out_v_accuracy: 0.9805 - val_out_a_accuracy: 0.9801 - lr: 1.2500e-04\n",
      "Epoch 86/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0192 - out_v_loss: 0.0091 - out_a_loss: 0.0101 - out_v_accuracy: 0.9966 - out_a_accuracy: 0.9960 - val_loss: 0.1245 - val_out_v_loss: 0.0618 - val_out_a_loss: 0.0627 - val_out_v_accuracy: 0.9809 - val_out_a_accuracy: 0.9824 - lr: 1.2500e-04\n",
      "Epoch 87/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0178 - out_v_loss: 0.0094 - out_a_loss: 0.0084 - out_v_accuracy: 0.9969 - out_a_accuracy: 0.9969\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0179 - out_v_loss: 0.0094 - out_a_loss: 0.0085 - out_v_accuracy: 0.9969 - out_a_accuracy: 0.9968 - val_loss: 0.1239 - val_out_v_loss: 0.0620 - val_out_a_loss: 0.0619 - val_out_v_accuracy: 0.9795 - val_out_a_accuracy: 0.9811 - lr: 1.2500e-04\n",
      "Epoch 88/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0171 - out_v_loss: 0.0071 - out_a_loss: 0.0099 - out_v_accuracy: 0.9974 - out_a_accuracy: 0.9963 - val_loss: 0.1259 - val_out_v_loss: 0.0653 - val_out_a_loss: 0.0606 - val_out_v_accuracy: 0.9799 - val_out_a_accuracy: 0.9814 - lr: 6.2500e-05\n",
      "Epoch 89/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0147 - out_v_loss: 0.0065 - out_a_loss: 0.0081 - out_v_accuracy: 0.9976 - out_a_accuracy: 0.9974 - val_loss: 0.1204 - val_out_v_loss: 0.0612 - val_out_a_loss: 0.0592 - val_out_v_accuracy: 0.9803 - val_out_a_accuracy: 0.9809 - lr: 6.2500e-05\n",
      "Epoch 90/200\n",
      "320/320 [==============================] - 11s 36ms/step - loss: 0.0177 - out_v_loss: 0.0084 - out_a_loss: 0.0094 - out_v_accuracy: 0.9972 - out_a_accuracy: 0.9966 - val_loss: 0.1189 - val_out_v_loss: 0.0630 - val_out_a_loss: 0.0559 - val_out_v_accuracy: 0.9801 - val_out_a_accuracy: 0.9820 - lr: 6.2500e-05\n",
      "Epoch 91/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0164 - out_v_loss: 0.0078 - out_a_loss: 0.0086 - out_v_accuracy: 0.9975 - out_a_accuracy: 0.9968 - val_loss: 0.1197 - val_out_v_loss: 0.0613 - val_out_a_loss: 0.0584 - val_out_v_accuracy: 0.9816 - val_out_a_accuracy: 0.9820 - lr: 6.2500e-05\n",
      "Epoch 92/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0159 - out_v_loss: 0.0082 - out_a_loss: 0.0076 - out_v_accuracy: 0.9970 - out_a_accuracy: 0.9975 - val_loss: 0.1218 - val_out_v_loss: 0.0628 - val_out_a_loss: 0.0590 - val_out_v_accuracy: 0.9795 - val_out_a_accuracy: 0.9812 - lr: 6.2500e-05\n",
      "Epoch 93/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0134 - out_v_loss: 0.0064 - out_a_loss: 0.0070 - out_v_accuracy: 0.9979 - out_a_accuracy: 0.9975 - val_loss: 0.1205 - val_out_v_loss: 0.0622 - val_out_a_loss: 0.0583 - val_out_v_accuracy: 0.9807 - val_out_a_accuracy: 0.9811 - lr: 6.2500e-05\n",
      "Epoch 94/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0142 - out_v_loss: 0.0076 - out_a_loss: 0.0066 - out_v_accuracy: 0.9974 - out_a_accuracy: 0.9978 - val_loss: 0.1223 - val_out_v_loss: 0.0640 - val_out_a_loss: 0.0583 - val_out_v_accuracy: 0.9791 - val_out_a_accuracy: 0.9818 - lr: 6.2500e-05\n",
      "Epoch 95/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0158 - out_v_loss: 0.0073 - out_a_loss: 0.0085 - out_v_accuracy: 0.9975 - out_a_accuracy: 0.9968\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0158 - out_v_loss: 0.0073 - out_a_loss: 0.0085 - out_v_accuracy: 0.9975 - out_a_accuracy: 0.9968 - val_loss: 0.1232 - val_out_v_loss: 0.0607 - val_out_a_loss: 0.0626 - val_out_v_accuracy: 0.9809 - val_out_a_accuracy: 0.9830 - lr: 6.2500e-05\n",
      "Epoch 96/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0148 - out_v_loss: 0.0077 - out_a_loss: 0.0071 - out_v_accuracy: 0.9973 - out_a_accuracy: 0.9978 - val_loss: 0.1197 - val_out_v_loss: 0.0610 - val_out_a_loss: 0.0587 - val_out_v_accuracy: 0.9812 - val_out_a_accuracy: 0.9842 - lr: 3.1250e-05\n",
      "Epoch 97/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0152 - out_v_loss: 0.0069 - out_a_loss: 0.0084 - out_v_accuracy: 0.9978 - out_a_accuracy: 0.9972 - val_loss: 0.1186 - val_out_v_loss: 0.0620 - val_out_a_loss: 0.0565 - val_out_v_accuracy: 0.9807 - val_out_a_accuracy: 0.9820 - lr: 3.1250e-05\n",
      "Epoch 98/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0141 - out_v_loss: 0.0061 - out_a_loss: 0.0080 - out_v_accuracy: 0.9982 - out_a_accuracy: 0.9974 - val_loss: 0.1191 - val_out_v_loss: 0.0621 - val_out_a_loss: 0.0570 - val_out_v_accuracy: 0.9807 - val_out_a_accuracy: 0.9826 - lr: 3.1250e-05\n",
      "Epoch 99/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0133 - out_v_loss: 0.0066 - out_a_loss: 0.0067 - out_v_accuracy: 0.9976 - out_a_accuracy: 0.9976 - val_loss: 0.1232 - val_out_v_loss: 0.0659 - val_out_a_loss: 0.0573 - val_out_v_accuracy: 0.9793 - val_out_a_accuracy: 0.9840 - lr: 3.1250e-05\n",
      "Epoch 100/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0142 - out_v_loss: 0.0074 - out_a_loss: 0.0068 - out_v_accuracy: 0.9974 - out_a_accuracy: 0.9981 - val_loss: 0.1189 - val_out_v_loss: 0.0609 - val_out_a_loss: 0.0579 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9820 - lr: 3.1250e-05\n",
      "Epoch 101/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0136 - out_v_loss: 0.0059 - out_a_loss: 0.0077 - out_v_accuracy: 0.9979 - out_a_accuracy: 0.9970 - val_loss: 0.1191 - val_out_v_loss: 0.0609 - val_out_a_loss: 0.0582 - val_out_v_accuracy: 0.9816 - val_out_a_accuracy: 0.9822 - lr: 3.1250e-05\n",
      "Epoch 102/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0136 - out_v_loss: 0.0051 - out_a_loss: 0.0085 - out_v_accuracy: 0.9982 - out_a_accuracy: 0.9971 - val_loss: 0.1182 - val_out_v_loss: 0.0608 - val_out_a_loss: 0.0574 - val_out_v_accuracy: 0.9809 - val_out_a_accuracy: 0.9826 - lr: 3.1250e-05\n",
      "Epoch 103/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0152 - out_v_loss: 0.0076 - out_a_loss: 0.0076 - out_v_accuracy: 0.9968 - out_a_accuracy: 0.9975 - val_loss: 0.1199 - val_out_v_loss: 0.0630 - val_out_a_loss: 0.0569 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9826 - lr: 3.1250e-05\n",
      "Epoch 104/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0121 - out_v_loss: 0.0056 - out_a_loss: 0.0064 - out_v_accuracy: 0.9982 - out_a_accuracy: 0.9979 - val_loss: 0.1157 - val_out_v_loss: 0.0609 - val_out_a_loss: 0.0548 - val_out_v_accuracy: 0.9830 - val_out_a_accuracy: 0.9828 - lr: 3.1250e-05\n",
      "Epoch 105/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0142 - out_v_loss: 0.0075 - out_a_loss: 0.0067 - out_v_accuracy: 0.9974 - out_a_accuracy: 0.9973 - val_loss: 0.1167 - val_out_v_loss: 0.0620 - val_out_a_loss: 0.0547 - val_out_v_accuracy: 0.9820 - val_out_a_accuracy: 0.9838 - lr: 3.1250e-05\n",
      "Epoch 106/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0133 - out_v_loss: 0.0071 - out_a_loss: 0.0062 - out_v_accuracy: 0.9973 - out_a_accuracy: 0.9978 - val_loss: 0.1171 - val_out_v_loss: 0.0625 - val_out_a_loss: 0.0546 - val_out_v_accuracy: 0.9809 - val_out_a_accuracy: 0.9834 - lr: 3.1250e-05\n",
      "Epoch 107/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0138 - out_v_loss: 0.0067 - out_a_loss: 0.0072 - out_v_accuracy: 0.9975 - out_a_accuracy: 0.9974 - val_loss: 0.1219 - val_out_v_loss: 0.0647 - val_out_a_loss: 0.0572 - val_out_v_accuracy: 0.9797 - val_out_a_accuracy: 0.9838 - lr: 3.1250e-05\n",
      "Epoch 108/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0136 - out_v_loss: 0.0065 - out_a_loss: 0.0072 - out_v_accuracy: 0.9978 - out_a_accuracy: 0.9971 - val_loss: 0.1176 - val_out_v_loss: 0.0628 - val_out_a_loss: 0.0549 - val_out_v_accuracy: 0.9809 - val_out_a_accuracy: 0.9830 - lr: 3.1250e-05\n",
      "Epoch 109/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0142 - out_v_loss: 0.0066 - out_a_loss: 0.0076 - out_v_accuracy: 0.9978 - out_a_accuracy: 0.9969\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0142 - out_v_loss: 0.0066 - out_a_loss: 0.0076 - out_v_accuracy: 0.9978 - out_a_accuracy: 0.9969 - val_loss: 0.1192 - val_out_v_loss: 0.0627 - val_out_a_loss: 0.0565 - val_out_v_accuracy: 0.9807 - val_out_a_accuracy: 0.9832 - lr: 3.1250e-05\n",
      "Epoch 110/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0146 - out_v_loss: 0.0067 - out_a_loss: 0.0079 - out_v_accuracy: 0.9976 - out_a_accuracy: 0.9970 - val_loss: 0.1187 - val_out_v_loss: 0.0635 - val_out_a_loss: 0.0553 - val_out_v_accuracy: 0.9812 - val_out_a_accuracy: 0.9820 - lr: 1.5625e-05\n",
      "Epoch 111/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0154 - out_v_loss: 0.0077 - out_a_loss: 0.0077 - out_v_accuracy: 0.9972 - out_a_accuracy: 0.9968 - val_loss: 0.1202 - val_out_v_loss: 0.0652 - val_out_a_loss: 0.0550 - val_out_v_accuracy: 0.9803 - val_out_a_accuracy: 0.9834 - lr: 1.5625e-05\n",
      "Epoch 112/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0122 - out_v_loss: 0.0057 - out_a_loss: 0.0065 - out_v_accuracy: 0.9980 - out_a_accuracy: 0.9975 - val_loss: 0.1188 - val_out_v_loss: 0.0635 - val_out_a_loss: 0.0553 - val_out_v_accuracy: 0.9811 - val_out_a_accuracy: 0.9830 - lr: 1.5625e-05\n",
      "Epoch 113/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0110 - out_v_loss: 0.0051 - out_a_loss: 0.0059 - out_v_accuracy: 0.9984 - out_a_accuracy: 0.9977 - val_loss: 0.1215 - val_out_v_loss: 0.0637 - val_out_a_loss: 0.0578 - val_out_v_accuracy: 0.9805 - val_out_a_accuracy: 0.9840 - lr: 1.5625e-05\n",
      "Epoch 114/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0120 - out_v_loss: 0.0059 - out_a_loss: 0.0061 - out_v_accuracy: 0.9980 - out_a_accuracy: 0.9979\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0120 - out_v_loss: 0.0059 - out_a_loss: 0.0061 - out_v_accuracy: 0.9980 - out_a_accuracy: 0.9979 - val_loss: 0.1174 - val_out_v_loss: 0.0626 - val_out_a_loss: 0.0548 - val_out_v_accuracy: 0.9811 - val_out_a_accuracy: 0.9832 - lr: 1.5625e-05\n",
      "Epoch 115/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0112 - out_v_loss: 0.0058 - out_a_loss: 0.0054 - out_v_accuracy: 0.9983 - out_a_accuracy: 0.9983 - val_loss: 0.1214 - val_out_v_loss: 0.0647 - val_out_a_loss: 0.0567 - val_out_v_accuracy: 0.9811 - val_out_a_accuracy: 0.9840 - lr: 7.8125e-06\n",
      "Epoch 116/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0119 - out_v_loss: 0.0058 - out_a_loss: 0.0061 - out_v_accuracy: 0.9979 - out_a_accuracy: 0.9978 - val_loss: 0.1165 - val_out_v_loss: 0.0613 - val_out_a_loss: 0.0552 - val_out_v_accuracy: 0.9822 - val_out_a_accuracy: 0.9834 - lr: 7.8125e-06\n",
      "Epoch 117/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0109 - out_v_loss: 0.0056 - out_a_loss: 0.0053 - out_v_accuracy: 0.9982 - out_a_accuracy: 0.9982 - val_loss: 0.1167 - val_out_v_loss: 0.0624 - val_out_a_loss: 0.0543 - val_out_v_accuracy: 0.9814 - val_out_a_accuracy: 0.9828 - lr: 7.8125e-06\n",
      "Epoch 118/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0110 - out_v_loss: 0.0046 - out_a_loss: 0.0064 - out_v_accuracy: 0.9985 - out_a_accuracy: 0.9977 - val_loss: 0.1170 - val_out_v_loss: 0.0622 - val_out_a_loss: 0.0548 - val_out_v_accuracy: 0.9816 - val_out_a_accuracy: 0.9836 - lr: 7.8125e-06\n",
      "Epoch 119/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0112 - out_v_loss: 0.0061 - out_a_loss: 0.0052 - out_v_accuracy: 0.9975 - out_a_accuracy: 0.9985\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0112 - out_v_loss: 0.0061 - out_a_loss: 0.0052 - out_v_accuracy: 0.9975 - out_a_accuracy: 0.9985 - val_loss: 0.1161 - val_out_v_loss: 0.0615 - val_out_a_loss: 0.0546 - val_out_v_accuracy: 0.9820 - val_out_a_accuracy: 0.9830 - lr: 7.8125e-06\n",
      "Epoch 120/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0120 - out_v_loss: 0.0056 - out_a_loss: 0.0064 - out_v_accuracy: 0.9981 - out_a_accuracy: 0.9979 - val_loss: 0.1167 - val_out_v_loss: 0.0619 - val_out_a_loss: 0.0548 - val_out_v_accuracy: 0.9816 - val_out_a_accuracy: 0.9832 - lr: 3.9063e-06\n",
      "Epoch 00120: early stopping\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.1167 - out_v_loss: 0.0619 - out_a_loss: 0.0548 - out_v_accuracy: 0.9816 - out_a_accuracy: 0.9832\n",
      "\n",
      "processing:  01 ......\n",
      "Before fine-tuning: [('loss', 0.029365), ('valence loss', 0.028361), ('arousal loss', 0.001004), ('valence acc', 0.986111), ('arousal acc', 1.0)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.029365), ('valence loss', 0.028361), ('arousal loss', 0.001004), ('valence acc', 0.986111), ('arousal acc', 1.0)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.028361), ('valence loss', 0.028361), ('arousal loss', 0.001004), ('valence acc', 0.986111), ('arousal acc', 1.0)]\n",
      "Epoch 00083: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.000634), ('valence loss', 0.053091), ('arousal loss', 0.000634), ('valence acc', 0.986111), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  02 ......\n",
      "Before fine-tuning: [('loss', 0.350777), ('valence loss', 0.093033), ('arousal loss', 0.257743), ('valence acc', 0.968944), ('arousal acc', 0.944099)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.343259), ('valence loss', 0.082608), ('arousal loss', 0.260651), ('valence acc', 0.956522), ('arousal acc', 0.944099)]\n",
      "Epoch 00038: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.090912), ('valence loss', 0.090912), ('arousal loss', 0.328354), ('valence acc', 0.981366), ('arousal acc', 0.937888)]\n",
      "Epoch 00035: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.256394), ('valence loss', 0.082415), ('arousal loss', 0.256394), ('valence acc', 0.956522), ('arousal acc', 0.944099)]\n",
      "\n",
      "processing:  03 ......\n",
      "Before fine-tuning: [('loss', 0.02718), ('valence loss', 0.027137), ('arousal loss', 4.3e-05), ('valence acc', 0.986928), ('arousal acc', 1.0)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.02718), ('valence loss', 0.027137), ('arousal loss', 4.3e-05), ('valence acc', 0.986928), ('arousal acc', 1.0)]\n",
      "Epoch 00020: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.016963), ('valence loss', 0.016963), ('arousal loss', 0.097196), ('valence acc', 0.993464), ('arousal acc', 0.941176)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 4.3e-05), ('valence loss', 0.027137), ('arousal loss', 4.3e-05), ('valence acc', 0.986928), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  04 ......\n",
      "Before fine-tuning: [('loss', 0.149395), ('valence loss', 0.131689), ('arousal loss', 0.017706), ('valence acc', 0.972603), ('arousal acc', 0.986301)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.149395), ('valence loss', 0.131689), ('arousal loss', 0.017706), ('valence acc', 0.972603), ('arousal acc', 0.986301)]\n",
      "Epoch 00037: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.05068), ('valence loss', 0.05068), ('arousal loss', 0.130465), ('valence acc', 0.986301), ('arousal acc', 0.952055)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.008099), ('valence loss', 0.041268), ('arousal loss', 0.008099), ('valence acc', 0.979452), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  05 ......\n",
      "Before fine-tuning: [('loss', 0.208189), ('valence loss', 0.153108), ('arousal loss', 0.055081), ('valence acc', 0.963636), ('arousal acc', 0.975758)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.203581), ('valence loss', 0.131316), ('arousal loss', 0.072265), ('valence acc', 0.957576), ('arousal acc', 0.969697)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.094414), ('valence loss', 0.094414), ('arousal loss', 0.145054), ('valence acc', 0.963636), ('arousal acc', 0.963636)]\n",
      "Epoch 00089: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.046043), ('valence loss', 0.128036), ('arousal loss', 0.046043), ('valence acc', 0.963636), ('arousal acc', 0.987879)]\n",
      "\n",
      "processing:  06 ......\n",
      "Before fine-tuning: [('loss', 0.064196), ('valence loss', 0.005804), ('arousal loss', 0.058392), ('valence acc', 1.0), ('arousal acc', 0.980892)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.064196), ('valence loss', 0.005804), ('arousal loss', 0.058392), ('valence acc', 1.0), ('arousal acc', 0.980892)]\n",
      "Epoch 00034: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.003227), ('valence loss', 0.003227), ('arousal loss', 0.139398), ('valence acc', 1.0), ('arousal acc', 0.968153)]\n",
      "Epoch 00096: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.03084), ('valence loss', 0.078364), ('arousal loss', 0.03084), ('valence acc', 0.980892), ('arousal acc', 0.987261)]\n",
      "\n",
      "processing:  07 ......\n",
      "Before fine-tuning: [('loss', 0.031987), ('valence loss', 0.031715), ('arousal loss', 0.000272), ('valence acc', 0.987654), ('arousal acc', 1.0)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.021378), ('valence loss', 0.020443), ('arousal loss', 0.000935), ('valence acc', 0.993827), ('arousal acc', 1.0)]\n",
      "Epoch 00037: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.026953), ('valence loss', 0.026953), ('arousal loss', 0.002443), ('valence acc', 0.993827), ('arousal acc', 1.0)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.000272), ('valence loss', 0.031715), ('arousal loss', 0.000272), ('valence acc', 0.987654), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  08 ......\n",
      "Before fine-tuning: [('loss', 0.068788), ('valence loss', 0.006035), ('arousal loss', 0.062753), ('valence acc', 0.993827), ('arousal acc', 0.987654)]\n",
      "Epoch 00031: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.054972), ('valence loss', 0.004407), ('arousal loss', 0.050565), ('valence acc', 1.0), ('arousal acc', 0.987654)]\n",
      "Epoch 00051: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.002019), ('valence loss', 0.002019), ('arousal loss', 0.061976), ('valence acc', 1.0), ('arousal acc', 0.981481)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.044204), ('valence loss', 0.018435), ('arousal loss', 0.044204), ('valence acc', 0.993827), ('arousal acc', 0.993827)]\n",
      "\n",
      "processing:  09 ......\n",
      "Before fine-tuning: [('loss', 0.268668), ('valence loss', 0.173672), ('arousal loss', 0.094996), ('valence acc', 0.974684), ('arousal acc', 0.968354)]\n",
      "Epoch 00030: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.244079), ('valence loss', 0.135972), ('arousal loss', 0.108107), ('valence acc', 0.968354), ('arousal acc', 0.955696)]\n",
      "Epoch 00037: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.15394), ('valence loss', 0.15394), ('arousal loss', 0.146421), ('valence acc', 0.974684), ('arousal acc', 0.949367)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.094996), ('valence loss', 0.173672), ('arousal loss', 0.094996), ('valence acc', 0.974684), ('arousal acc', 0.968354)]\n",
      "\n",
      "processing:  10 ......\n",
      "Before fine-tuning: [('loss', 0.058179), ('valence loss', 0.004166), ('arousal loss', 0.054013), ('valence acc', 1.0), ('arousal acc', 0.994012)]\n",
      "Epoch 00034: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.058179), ('valence loss', 0.004166), ('arousal loss', 0.054013), ('valence acc', 1.0), ('arousal acc', 0.994012)]\n",
      "Epoch 00019: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.001885), ('valence loss', 0.001885), ('arousal loss', 0.081993), ('valence acc', 1.0), ('arousal acc', 0.982036)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.023177), ('valence loss', 0.005519), ('arousal loss', 0.023177), ('valence acc', 1.0), ('arousal acc', 0.988024)]\n",
      "\n",
      "processing:  11 ......\n",
      "Before fine-tuning: [('loss', 0.117773), ('valence loss', 0.064804), ('arousal loss', 0.052969), ('valence acc', 0.980263), ('arousal acc', 0.986842)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.117773), ('valence loss', 0.064804), ('arousal loss', 0.052969), ('valence acc', 0.980263), ('arousal acc', 0.986842)]\n",
      "Epoch 00021: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.064804), ('valence loss', 0.064804), ('arousal loss', 0.052969), ('valence acc', 0.980263), ('arousal acc', 0.986842)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.050861), ('valence loss', 0.085926), ('arousal loss', 0.050861), ('valence acc', 0.973684), ('arousal acc', 0.973684)]\n",
      "\n",
      "processing:  12 ......\n",
      "Before fine-tuning: [('loss', 0.027629), ('valence loss', 0.018395), ('arousal loss', 0.009234), ('valence acc', 0.988166), ('arousal acc', 0.994083)]\n",
      "Epoch 00045: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.027629), ('valence loss', 0.018395), ('arousal loss', 0.009234), ('valence acc', 0.988166), ('arousal acc', 0.994083)]\n",
      "Epoch 00047: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.00126), ('valence loss', 0.00126), ('arousal loss', 0.184713), ('valence acc', 1.0), ('arousal acc', 0.940828)]\n",
      "Epoch 00031: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.004786), ('valence loss', 0.006509), ('arousal loss', 0.004786), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  13 ......\n",
      "Before fine-tuning: [('loss', 0.049651), ('valence loss', 0.047661), ('arousal loss', 0.00199), ('valence acc', 0.988304), ('arousal acc', 1.0)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.049651), ('valence loss', 0.047661), ('arousal loss', 0.00199), ('valence acc', 0.988304), ('arousal acc', 1.0)]\n",
      "Epoch 00037: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.027027), ('valence loss', 0.027027), ('arousal loss', 0.254457), ('valence acc', 0.988304), ('arousal acc', 0.894737)]\n",
      "Epoch 00040: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.00199), ('valence loss', 0.047661), ('arousal loss', 0.00199), ('valence acc', 0.988304), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  14 ......\n",
      "Before fine-tuning: [('loss', 0.063055), ('valence loss', 0.030253), ('arousal loss', 0.032802), ('valence acc', 0.993464), ('arousal acc', 0.980392)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.057741), ('valence loss', 0.019495), ('arousal loss', 0.038246), ('valence acc', 0.993464), ('arousal acc', 0.980392)]\n",
      "Epoch 00047: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.02062), ('valence loss', 0.02062), ('arousal loss', 0.066499), ('valence acc', 0.986928), ('arousal acc', 0.980392)]\n",
      "Epoch 00021: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.032802), ('valence loss', 0.030253), ('arousal loss', 0.032802), ('valence acc', 0.993464), ('arousal acc', 0.980392)]\n",
      "\n",
      "processing:  15 ......\n",
      "Before fine-tuning: [('loss', 0.053202), ('valence loss', 0.002352), ('arousal loss', 0.050849), ('valence acc', 1.0), ('arousal acc', 0.98773)]\n",
      "Epoch 00030: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.041623), ('valence loss', 0.00556), ('arousal loss', 0.036063), ('valence acc', 1.0), ('arousal acc', 0.993865)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.002251), ('valence loss', 0.002251), ('arousal loss', 0.044201), ('valence acc', 1.0), ('arousal acc', 0.98773)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.023344), ('valence loss', 0.004291), ('arousal loss', 0.023344), ('valence acc', 1.0), ('arousal acc', 0.98773)]\n",
      "\n",
      "processing:  16 ......\n",
      "Before fine-tuning: [('loss', 0.0393), ('valence loss', 0.004837), ('arousal loss', 0.034462), ('valence acc', 1.0), ('arousal acc', 0.982143)]\n",
      "Epoch 00052: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.02367), ('valence loss', 0.005894), ('arousal loss', 0.017776), ('valence acc', 1.0), ('arousal acc', 0.988095)]\n",
      "Epoch 00035: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.004837), ('valence loss', 0.004837), ('arousal loss', 0.034462), ('valence acc', 1.0), ('arousal acc', 0.982143)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.009749), ('valence loss', 0.029526), ('arousal loss', 0.009749), ('valence acc', 0.982143), ('arousal acc', 0.994048)]\n",
      "\n",
      "processing:  17 ......\n",
      "Before fine-tuning: [('loss', 0.277109), ('valence loss', 0.105037), ('arousal loss', 0.172072), ('valence acc', 0.972973), ('arousal acc', 0.945946)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.277109), ('valence loss', 0.105037), ('arousal loss', 0.172072), ('valence acc', 0.972973), ('arousal acc', 0.945946)]\n",
      "Epoch 00028: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.105037), ('valence loss', 0.105037), ('arousal loss', 0.172072), ('valence acc', 0.972973), ('arousal acc', 0.945946)]\n",
      "Epoch 00044: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.152393), ('valence loss', 0.160192), ('arousal loss', 0.152393), ('valence acc', 0.945946), ('arousal acc', 0.939189)]\n",
      "\n",
      "processing:  18 ......\n",
      "Before fine-tuning: [('loss', 0.142548), ('valence loss', 0.14245), ('arousal loss', 9.9e-05), ('valence acc', 0.985507), ('arousal acc', 1.0)]\n",
      "Epoch 00033: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.141073), ('valence loss', 0.140856), ('arousal loss', 0.000217), ('valence acc', 0.992754), ('arousal acc', 1.0)]\n",
      "Epoch 00059: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.123989), ('valence loss', 0.123989), ('arousal loss', 0.000837), ('valence acc', 0.992754), ('arousal acc', 1.0)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 9.9e-05), ('valence loss', 0.14245), ('arousal loss', 9.9e-05), ('valence acc', 0.985507), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  19 ......\n",
      "Before fine-tuning: [('loss', 0.013624), ('valence loss', 0.008297), ('arousal loss', 0.005327), ('valence acc', 0.994152), ('arousal acc', 1.0)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.013624), ('valence loss', 0.008297), ('arousal loss', 0.005327), ('valence acc', 0.994152), ('arousal acc', 1.0)]\n",
      "Epoch 00044: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.00343), ('valence loss', 0.00343), ('arousal loss', 0.004689), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "Epoch 00037: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.00284), ('valence loss', 0.021598), ('arousal loss', 0.00284), ('valence acc', 0.994152), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  20 ......\n",
      "Before fine-tuning: [('loss', 0.056598), ('valence loss', 0.005071), ('arousal loss', 0.051528), ('valence acc', 1.0), ('arousal acc', 0.988506)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.029608), ('valence loss', 0.006119), ('arousal loss', 0.023489), ('valence acc', 1.0), ('arousal acc', 0.994253)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.00432), ('valence loss', 0.00432), ('arousal loss', 0.031639), ('valence acc', 1.0), ('arousal acc', 0.982759)]\n",
      "Epoch 00028: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.020861), ('valence loss', 0.007228), ('arousal loss', 0.020861), ('valence acc', 1.0), ('arousal acc', 0.988506)]\n",
      "\n",
      "processing:  21 ......\n",
      "Before fine-tuning: [('loss', 0.038623), ('valence loss', 0.009733), ('arousal loss', 0.02889), ('valence acc', 0.993902), ('arousal acc', 0.993902)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.028872), ('valence loss', 0.017367), ('arousal loss', 0.011505), ('valence acc', 0.987805), ('arousal acc', 0.993902)]\n",
      "Epoch 00020: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.004137), ('valence loss', 0.004137), ('arousal loss', 0.035053), ('valence acc', 1.0), ('arousal acc', 0.987805)]\n",
      "Epoch 00061: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.007037), ('valence loss', 0.021693), ('arousal loss', 0.007037), ('valence acc', 0.981707), ('arousal acc', 0.993902)]\n",
      "\n",
      "processing:  22 ......\n",
      "Before fine-tuning: [('loss', 0.594659), ('valence loss', 0.340838), ('arousal loss', 0.25382), ('valence acc', 0.888158), ('arousal acc', 0.888158)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.594659), ('valence loss', 0.340838), ('arousal loss', 0.25382), ('valence acc', 0.888158), ('arousal acc', 0.888158)]\n",
      "Epoch 00019: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.287045), ('valence loss', 0.287045), ('arousal loss', 0.451832), ('valence acc', 0.894737), ('arousal acc', 0.901316)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.25382), ('valence loss', 0.340838), ('arousal loss', 0.25382), ('valence acc', 0.888158), ('arousal acc', 0.888158)]\n",
      "\n",
      "processing:  23 ......\n",
      "Before fine-tuning: [('loss', 0.05343), ('valence loss', 0.053206), ('arousal loss', 0.000225), ('valence acc', 0.987654), ('arousal acc', 1.0)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.052025), ('valence loss', 0.045101), ('arousal loss', 0.006925), ('valence acc', 0.981481), ('arousal acc', 1.0)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.038318), ('valence loss', 0.038318), ('arousal loss', 0.046461), ('valence acc', 0.981481), ('arousal acc', 0.975309)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.000225), ('valence loss', 0.053206), ('arousal loss', 0.000225), ('valence acc', 0.987654), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  24 ......\n",
      "Before fine-tuning: [('loss', 0.157912), ('valence loss', 0.129321), ('arousal loss', 0.02859), ('valence acc', 0.95625), ('arousal acc', 0.98125)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.157912), ('valence loss', 0.129321), ('arousal loss', 0.02859), ('valence acc', 0.95625), ('arousal acc', 0.98125)]\n",
      "Epoch 00039: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.083168), ('valence loss', 0.083168), ('arousal loss', 0.312125), ('valence acc', 0.96875), ('arousal acc', 0.91875)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.02859), ('valence loss', 0.129321), ('arousal loss', 0.02859), ('valence acc', 0.95625), ('arousal acc', 0.98125)]\n",
      "\n",
      "processing:  25 ......\n",
      "Before fine-tuning: [('loss', 0.047756), ('valence loss', 0.00742), ('arousal loss', 0.040336), ('valence acc', 0.99359), ('arousal acc', 0.987179)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.041999), ('valence loss', 0.002307), ('arousal loss', 0.039692), ('valence acc', 1.0), ('arousal acc', 0.99359)]\n",
      "Epoch 00026: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.000178), ('valence loss', 0.000178), ('arousal loss', 0.079399), ('valence acc', 1.0), ('arousal acc', 0.974359)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.034757), ('valence loss', 0.003507), ('arousal loss', 0.034757), ('valence acc', 1.0), ('arousal acc', 0.99359)]\n",
      "\n",
      "processing:  26 ......\n",
      "Before fine-tuning: [('loss', 0.180437), ('valence loss', 0.06022), ('arousal loss', 0.120218), ('valence acc', 0.97006), ('arousal acc', 0.964072)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.170071), ('valence loss', 0.061238), ('arousal loss', 0.108833), ('valence acc', 0.97006), ('arousal acc', 0.958084)]\n",
      "Epoch 00039: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.03839), ('valence loss', 0.03839), ('arousal loss', 0.177214), ('valence acc', 0.97006), ('arousal acc', 0.946108)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.114726), ('valence loss', 0.065664), ('arousal loss', 0.114726), ('valence acc', 0.97006), ('arousal acc', 0.958084)]\n",
      "\n",
      "processing:  27 ......\n",
      "Before fine-tuning: [('loss', 0.102277), ('valence loss', 0.071834), ('arousal loss', 0.030444), ('valence acc', 0.99375), ('arousal acc', 0.99375)]\n",
      "Epoch 00038: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.056295), ('valence loss', 0.045187), ('arousal loss', 0.011108), ('valence acc', 0.99375), ('arousal acc', 0.99375)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.054718), ('valence loss', 0.054718), ('arousal loss', 0.018333), ('valence acc', 0.9875), ('arousal acc', 0.99375)]\n",
      "Epoch 00065: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.011819), ('valence loss', 0.086955), ('arousal loss', 0.011819), ('valence acc', 0.98125), ('arousal acc', 0.99375)]\n",
      "\n",
      "processing:  28 ......\n",
      "Before fine-tuning: [('loss', 0.144749), ('valence loss', 0.096737), ('arousal loss', 0.048012), ('valence acc', 0.974522), ('arousal acc', 0.987261)]\n",
      "Epoch 00034: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.137466), ('valence loss', 0.097449), ('arousal loss', 0.040017), ('valence acc', 0.968153), ('arousal acc', 0.980892)]\n",
      "Epoch 00019: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.083576), ('valence loss', 0.083576), ('arousal loss', 0.045188), ('valence acc', 0.974522), ('arousal acc', 0.980892)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.042834), ('valence loss', 0.106883), ('arousal loss', 0.042834), ('valence acc', 0.968153), ('arousal acc', 0.974522)]\n",
      "\n",
      "processing:  29 ......\n",
      "Before fine-tuning: [('loss', 0.147313), ('valence loss', 0.039679), ('arousal loss', 0.107634), ('valence acc', 0.988506), ('arousal acc', 0.971264)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.143881), ('valence loss', 0.038287), ('arousal loss', 0.105594), ('valence acc', 0.982759), ('arousal acc', 0.977012)]\n",
      "Epoch 00030: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.035682), ('valence loss', 0.035682), ('arousal loss', 0.107059), ('valence acc', 0.982759), ('arousal acc', 0.977012)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.09735), ('valence loss', 0.042622), ('arousal loss', 0.09735), ('valence acc', 0.982759), ('arousal acc', 0.971264)]\n",
      "\n",
      "processing:  30 ......\n",
      "Before fine-tuning: [('loss', 0.094614), ('valence loss', 0.02539), ('arousal loss', 0.069224), ('valence acc', 0.987805), ('arousal acc', 0.981707)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.094614), ('valence loss', 0.02539), ('arousal loss', 0.069224), ('valence acc', 0.987805), ('arousal acc', 0.981707)]\n",
      "Epoch 00031: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.02539), ('valence loss', 0.02539), ('arousal loss', 0.069224), ('valence acc', 0.987805), ('arousal acc', 0.981707)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.069224), ('valence loss', 0.02539), ('arousal loss', 0.069224), ('valence acc', 0.987805), ('arousal acc', 0.981707)]\n",
      "\n",
      "processing:  31 ......\n",
      "Before fine-tuning: [('loss', 0.035363), ('valence loss', 0.032199), ('arousal loss', 0.003164), ('valence acc', 0.988304), ('arousal acc', 1.0)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.035363), ('valence loss', 0.032199), ('arousal loss', 0.003164), ('valence acc', 0.988304), ('arousal acc', 1.0)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.032199), ('valence loss', 0.032199), ('arousal loss', 0.003164), ('valence acc', 0.988304), ('arousal acc', 1.0)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.003164), ('valence loss', 0.032199), ('arousal loss', 0.003164), ('valence acc', 0.988304), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  32 ......\n",
      "Before fine-tuning: [('loss', 0.06168), ('valence loss', 0.047407), ('arousal loss', 0.014273), ('valence acc', 0.986755), ('arousal acc', 0.993378)]\n",
      "Epoch 00044: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.050562), ('valence loss', 0.034461), ('arousal loss', 0.016101), ('valence acc', 0.986755), ('arousal acc', 0.993378)]\n",
      "Epoch 00019: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.04125), ('valence loss', 0.04125), ('arousal loss', 0.020883), ('valence acc', 0.980132), ('arousal acc', 0.986755)]\n",
      "Epoch 00048: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.014112), ('valence loss', 0.030507), ('arousal loss', 0.014112), ('valence acc', 0.986755), ('arousal acc', 0.993378)]\n",
      "\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "320/320 [==============================] - 15s 36ms/step - loss: 1.3224 - out_v_loss: 0.6705 - out_a_loss: 0.6519 - out_v_accuracy: 0.6238 - out_a_accuracy: 0.6476 - val_loss: 1.1410 - val_out_v_loss: 0.6107 - val_out_a_loss: 0.5303 - val_out_v_accuracy: 0.6742 - val_out_a_accuracy: 0.7371 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 1.0424 - out_v_loss: 0.5256 - out_a_loss: 0.5168 - out_v_accuracy: 0.7343 - out_a_accuracy: 0.7404 - val_loss: 0.9483 - val_out_v_loss: 0.4701 - val_out_a_loss: 0.4782 - val_out_v_accuracy: 0.7736 - val_out_a_accuracy: 0.7602 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.8475 - out_v_loss: 0.4273 - out_a_loss: 0.4202 - out_v_accuracy: 0.7953 - out_a_accuracy: 0.7996 - val_loss: 0.7730 - val_out_v_loss: 0.3873 - val_out_a_loss: 0.3857 - val_out_v_accuracy: 0.8340 - val_out_a_accuracy: 0.8111 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.6989 - out_v_loss: 0.3511 - out_a_loss: 0.3478 - out_v_accuracy: 0.8407 - out_a_accuracy: 0.8398 - val_loss: 0.6356 - val_out_v_loss: 0.3115 - val_out_a_loss: 0.3242 - val_out_v_accuracy: 0.8580 - val_out_a_accuracy: 0.8566 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.5995 - out_v_loss: 0.3017 - out_a_loss: 0.2978 - out_v_accuracy: 0.8646 - out_a_accuracy: 0.8668 - val_loss: 0.5188 - val_out_v_loss: 0.2602 - val_out_a_loss: 0.2586 - val_out_v_accuracy: 0.8848 - val_out_a_accuracy: 0.8885 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.5075 - out_v_loss: 0.2530 - out_a_loss: 0.2544 - out_v_accuracy: 0.8883 - out_a_accuracy: 0.8871 - val_loss: 0.4659 - val_out_v_loss: 0.2337 - val_out_a_loss: 0.2322 - val_out_v_accuracy: 0.8961 - val_out_a_accuracy: 0.8967 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.4512 - out_v_loss: 0.2250 - out_a_loss: 0.2262 - out_v_accuracy: 0.9027 - out_a_accuracy: 0.9016 - val_loss: 0.4516 - val_out_v_loss: 0.2350 - val_out_a_loss: 0.2166 - val_out_v_accuracy: 0.9023 - val_out_a_accuracy: 0.9066 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.4079 - out_v_loss: 0.2022 - out_a_loss: 0.2058 - out_v_accuracy: 0.9136 - out_a_accuracy: 0.9077 - val_loss: 0.4440 - val_out_v_loss: 0.2262 - val_out_a_loss: 0.2178 - val_out_v_accuracy: 0.9037 - val_out_a_accuracy: 0.9104 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3701 - out_v_loss: 0.1822 - out_a_loss: 0.1879 - out_v_accuracy: 0.9249 - out_a_accuracy: 0.9181 - val_loss: 0.3890 - val_out_v_loss: 0.1943 - val_out_a_loss: 0.1947 - val_out_v_accuracy: 0.9240 - val_out_a_accuracy: 0.9207 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.3393 - out_v_loss: 0.1697 - out_a_loss: 0.1696 - out_v_accuracy: 0.9303 - out_a_accuracy: 0.9267 - val_loss: 0.4076 - val_out_v_loss: 0.2051 - val_out_a_loss: 0.2025 - val_out_v_accuracy: 0.9215 - val_out_a_accuracy: 0.9160 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.3090 - out_v_loss: 0.1477 - out_a_loss: 0.1613 - out_v_accuracy: 0.9390 - out_a_accuracy: 0.9313 - val_loss: 0.3659 - val_out_v_loss: 0.1843 - val_out_a_loss: 0.1816 - val_out_v_accuracy: 0.9283 - val_out_a_accuracy: 0.9229 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2928 - out_v_loss: 0.1481 - out_a_loss: 0.1446 - out_v_accuracy: 0.9393 - out_a_accuracy: 0.9384 - val_loss: 0.3600 - val_out_v_loss: 0.1775 - val_out_a_loss: 0.1825 - val_out_v_accuracy: 0.9307 - val_out_a_accuracy: 0.9268 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2738 - out_v_loss: 0.1323 - out_a_loss: 0.1416 - out_v_accuracy: 0.9443 - out_a_accuracy: 0.9408 - val_loss: 0.4237 - val_out_v_loss: 0.1997 - val_out_a_loss: 0.2240 - val_out_v_accuracy: 0.9186 - val_out_a_accuracy: 0.9187 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2685 - out_v_loss: 0.1338 - out_a_loss: 0.1347 - out_v_accuracy: 0.9447 - out_a_accuracy: 0.9442 - val_loss: 0.3384 - val_out_v_loss: 0.1731 - val_out_a_loss: 0.1653 - val_out_v_accuracy: 0.9352 - val_out_a_accuracy: 0.9367 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2323 - out_v_loss: 0.1166 - out_a_loss: 0.1157 - out_v_accuracy: 0.9521 - out_a_accuracy: 0.9523 - val_loss: 0.3164 - val_out_v_loss: 0.1754 - val_out_a_loss: 0.1410 - val_out_v_accuracy: 0.9350 - val_out_a_accuracy: 0.9473 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2321 - out_v_loss: 0.1154 - out_a_loss: 0.1167 - out_v_accuracy: 0.9536 - out_a_accuracy: 0.9533 - val_loss: 0.3255 - val_out_v_loss: 0.1391 - val_out_a_loss: 0.1864 - val_out_v_accuracy: 0.9479 - val_out_a_accuracy: 0.9320 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2235 - out_v_loss: 0.1086 - out_a_loss: 0.1149 - out_v_accuracy: 0.9558 - out_a_accuracy: 0.9515 - val_loss: 0.3224 - val_out_v_loss: 0.1502 - val_out_a_loss: 0.1722 - val_out_v_accuracy: 0.9438 - val_out_a_accuracy: 0.9346 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.2124 - out_v_loss: 0.1030 - out_a_loss: 0.1094 - out_v_accuracy: 0.9582 - out_a_accuracy: 0.9552 - val_loss: 0.2928 - val_out_v_loss: 0.1413 - val_out_a_loss: 0.1515 - val_out_v_accuracy: 0.9475 - val_out_a_accuracy: 0.9434 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1955 - out_v_loss: 0.0975 - out_a_loss: 0.0980 - out_v_accuracy: 0.9604 - out_a_accuracy: 0.9606 - val_loss: 0.3025 - val_out_v_loss: 0.1459 - val_out_a_loss: 0.1566 - val_out_v_accuracy: 0.9459 - val_out_a_accuracy: 0.9410 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.2042 - out_v_loss: 0.1019 - out_a_loss: 0.1023 - out_v_accuracy: 0.9594 - out_a_accuracy: 0.9587 - val_loss: 0.3159 - val_out_v_loss: 0.1382 - val_out_a_loss: 0.1777 - val_out_v_accuracy: 0.9457 - val_out_a_accuracy: 0.9367 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1864 - out_v_loss: 0.0886 - out_a_loss: 0.0978 - out_v_accuracy: 0.9656 - out_a_accuracy: 0.9612 - val_loss: 0.2641 - val_out_v_loss: 0.1172 - val_out_a_loss: 0.1469 - val_out_v_accuracy: 0.9568 - val_out_a_accuracy: 0.9447 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1776 - out_v_loss: 0.0872 - out_a_loss: 0.0904 - out_v_accuracy: 0.9652 - out_a_accuracy: 0.9645 - val_loss: 0.2572 - val_out_v_loss: 0.1341 - val_out_a_loss: 0.1231 - val_out_v_accuracy: 0.9498 - val_out_a_accuracy: 0.9545 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1630 - out_v_loss: 0.0822 - out_a_loss: 0.0808 - out_v_accuracy: 0.9679 - out_a_accuracy: 0.9670 - val_loss: 0.3012 - val_out_v_loss: 0.1347 - val_out_a_loss: 0.1665 - val_out_v_accuracy: 0.9525 - val_out_a_accuracy: 0.9393 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.1831 - out_v_loss: 0.0902 - out_a_loss: 0.0928 - out_v_accuracy: 0.9644 - out_a_accuracy: 0.9617 - val_loss: 0.2677 - val_out_v_loss: 0.1345 - val_out_a_loss: 0.1332 - val_out_v_accuracy: 0.9527 - val_out_a_accuracy: 0.9516 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.1584 - out_v_loss: 0.0735 - out_a_loss: 0.0850 - out_v_accuracy: 0.9711 - out_a_accuracy: 0.9647 - val_loss: 0.2524 - val_out_v_loss: 0.1236 - val_out_a_loss: 0.1288 - val_out_v_accuracy: 0.9572 - val_out_a_accuracy: 0.9557 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1559 - out_v_loss: 0.0752 - out_a_loss: 0.0808 - out_v_accuracy: 0.9708 - out_a_accuracy: 0.9684 - val_loss: 0.2582 - val_out_v_loss: 0.1140 - val_out_a_loss: 0.1442 - val_out_v_accuracy: 0.9576 - val_out_a_accuracy: 0.9506 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1559 - out_v_loss: 0.0764 - out_a_loss: 0.0795 - out_v_accuracy: 0.9698 - out_a_accuracy: 0.9687 - val_loss: 0.2608 - val_out_v_loss: 0.1251 - val_out_a_loss: 0.1357 - val_out_v_accuracy: 0.9537 - val_out_a_accuracy: 0.9533 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "320/320 [==============================] - 12s 36ms/step - loss: 0.1618 - out_v_loss: 0.0790 - out_a_loss: 0.0828 - out_v_accuracy: 0.9688 - out_a_accuracy: 0.9671 - val_loss: 0.2440 - val_out_v_loss: 0.1255 - val_out_a_loss: 0.1185 - val_out_v_accuracy: 0.9561 - val_out_a_accuracy: 0.9557 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1466 - out_v_loss: 0.0679 - out_a_loss: 0.0786 - out_v_accuracy: 0.9741 - out_a_accuracy: 0.9685 - val_loss: 0.2674 - val_out_v_loss: 0.1389 - val_out_a_loss: 0.1285 - val_out_v_accuracy: 0.9520 - val_out_a_accuracy: 0.9553 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "320/320 [==============================] - 12s 36ms/step - loss: 0.1558 - out_v_loss: 0.0812 - out_a_loss: 0.0746 - out_v_accuracy: 0.9691 - out_a_accuracy: 0.9708 - val_loss: 0.2436 - val_out_v_loss: 0.1121 - val_out_a_loss: 0.1316 - val_out_v_accuracy: 0.9609 - val_out_a_accuracy: 0.9533 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1401 - out_v_loss: 0.0699 - out_a_loss: 0.0702 - out_v_accuracy: 0.9721 - out_a_accuracy: 0.9726 - val_loss: 0.2506 - val_out_v_loss: 0.1162 - val_out_a_loss: 0.1345 - val_out_v_accuracy: 0.9623 - val_out_a_accuracy: 0.9551 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1405 - out_v_loss: 0.0686 - out_a_loss: 0.0719 - out_v_accuracy: 0.9733 - out_a_accuracy: 0.9707 - val_loss: 0.2671 - val_out_v_loss: 0.1251 - val_out_a_loss: 0.1420 - val_out_v_accuracy: 0.9555 - val_out_a_accuracy: 0.9527 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1330 - out_v_loss: 0.0650 - out_a_loss: 0.0681 - out_v_accuracy: 0.9739 - out_a_accuracy: 0.9725 - val_loss: 0.2557 - val_out_v_loss: 0.1137 - val_out_a_loss: 0.1419 - val_out_v_accuracy: 0.9576 - val_out_a_accuracy: 0.9529 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "320/320 [==============================] - 12s 36ms/step - loss: 0.1303 - out_v_loss: 0.0632 - out_a_loss: 0.0671 - out_v_accuracy: 0.9753 - out_a_accuracy: 0.9749 - val_loss: 0.2233 - val_out_v_loss: 0.1076 - val_out_a_loss: 0.1157 - val_out_v_accuracy: 0.9629 - val_out_a_accuracy: 0.9604 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1239 - out_v_loss: 0.0611 - out_a_loss: 0.0628 - out_v_accuracy: 0.9765 - out_a_accuracy: 0.9754 - val_loss: 0.2319 - val_out_v_loss: 0.1170 - val_out_a_loss: 0.1149 - val_out_v_accuracy: 0.9598 - val_out_a_accuracy: 0.9580 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "320/320 [==============================] - 12s 36ms/step - loss: 0.1260 - out_v_loss: 0.0610 - out_a_loss: 0.0649 - out_v_accuracy: 0.9778 - out_a_accuracy: 0.9742 - val_loss: 0.2126 - val_out_v_loss: 0.1039 - val_out_a_loss: 0.1087 - val_out_v_accuracy: 0.9662 - val_out_a_accuracy: 0.9588 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1235 - out_v_loss: 0.0631 - out_a_loss: 0.0604 - out_v_accuracy: 0.9749 - out_a_accuracy: 0.9770 - val_loss: 0.2373 - val_out_v_loss: 0.1207 - val_out_a_loss: 0.1166 - val_out_v_accuracy: 0.9574 - val_out_a_accuracy: 0.9574 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1149 - out_v_loss: 0.0583 - out_a_loss: 0.0566 - out_v_accuracy: 0.9784 - out_a_accuracy: 0.9771 - val_loss: 0.2695 - val_out_v_loss: 0.1342 - val_out_a_loss: 0.1353 - val_out_v_accuracy: 0.9557 - val_out_a_accuracy: 0.9557 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1154 - out_v_loss: 0.0567 - out_a_loss: 0.0587 - out_v_accuracy: 0.9772 - out_a_accuracy: 0.9771 - val_loss: 0.2327 - val_out_v_loss: 0.1084 - val_out_a_loss: 0.1243 - val_out_v_accuracy: 0.9635 - val_out_a_accuracy: 0.9582 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1180 - out_v_loss: 0.0568 - out_a_loss: 0.0611 - out_v_accuracy: 0.9777 - out_a_accuracy: 0.9756 - val_loss: 0.2273 - val_out_v_loss: 0.1097 - val_out_a_loss: 0.1176 - val_out_v_accuracy: 0.9631 - val_out_a_accuracy: 0.9604 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.1189 - out_v_loss: 0.0623 - out_a_loss: 0.0566 - out_v_accuracy: 0.9762 - out_a_accuracy: 0.9774\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.1191 - out_v_loss: 0.0623 - out_a_loss: 0.0567 - out_v_accuracy: 0.9763 - out_a_accuracy: 0.9773 - val_loss: 0.2354 - val_out_v_loss: 0.1113 - val_out_a_loss: 0.1241 - val_out_v_accuracy: 0.9631 - val_out_a_accuracy: 0.9551 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "320/320 [==============================] - 11s 36ms/step - loss: 0.0792 - out_v_loss: 0.0368 - out_a_loss: 0.0423 - out_v_accuracy: 0.9850 - out_a_accuracy: 0.9832 - val_loss: 0.1897 - val_out_v_loss: 0.0920 - val_out_a_loss: 0.0977 - val_out_v_accuracy: 0.9693 - val_out_a_accuracy: 0.9676 - lr: 5.0000e-04\n",
      "Epoch 43/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.0626 - out_v_loss: 0.0305 - out_a_loss: 0.0321 - out_v_accuracy: 0.9885 - out_a_accuracy: 0.9886 - val_loss: 0.1892 - val_out_v_loss: 0.0919 - val_out_a_loss: 0.0974 - val_out_v_accuracy: 0.9711 - val_out_a_accuracy: 0.9688 - lr: 5.0000e-04\n",
      "Epoch 44/200\n",
      "320/320 [==============================] - 11s 34ms/step - loss: 0.0594 - out_v_loss: 0.0310 - out_a_loss: 0.0283 - out_v_accuracy: 0.9876 - out_a_accuracy: 0.9894 - val_loss: 0.1918 - val_out_v_loss: 0.0988 - val_out_a_loss: 0.0930 - val_out_v_accuracy: 0.9697 - val_out_a_accuracy: 0.9699 - lr: 5.0000e-04\n",
      "Epoch 45/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0633 - out_v_loss: 0.0296 - out_a_loss: 0.0337 - out_v_accuracy: 0.9891 - out_a_accuracy: 0.9863 - val_loss: 0.1966 - val_out_v_loss: 0.0946 - val_out_a_loss: 0.1020 - val_out_v_accuracy: 0.9689 - val_out_a_accuracy: 0.9666 - lr: 5.0000e-04\n",
      "Epoch 46/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0626 - out_v_loss: 0.0317 - out_a_loss: 0.0309 - out_v_accuracy: 0.9881 - out_a_accuracy: 0.9881 - val_loss: 0.2078 - val_out_v_loss: 0.0979 - val_out_a_loss: 0.1099 - val_out_v_accuracy: 0.9676 - val_out_a_accuracy: 0.9670 - lr: 5.0000e-04\n",
      "Epoch 47/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0582 - out_v_loss: 0.0280 - out_a_loss: 0.0302 - out_v_accuracy: 0.9888 - out_a_accuracy: 0.9892 - val_loss: 0.2116 - val_out_v_loss: 0.1014 - val_out_a_loss: 0.1102 - val_out_v_accuracy: 0.9688 - val_out_a_accuracy: 0.9660 - lr: 5.0000e-04\n",
      "Epoch 48/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0577 - out_v_loss: 0.0287 - out_a_loss: 0.0291 - out_v_accuracy: 0.9891 - out_a_accuracy: 0.9895\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0578 - out_v_loss: 0.0286 - out_a_loss: 0.0292 - out_v_accuracy: 0.9891 - out_a_accuracy: 0.9895 - val_loss: 0.2032 - val_out_v_loss: 0.0947 - val_out_a_loss: 0.1085 - val_out_v_accuracy: 0.9693 - val_out_a_accuracy: 0.9668 - lr: 5.0000e-04\n",
      "Epoch 49/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0482 - out_v_loss: 0.0232 - out_a_loss: 0.0250 - out_v_accuracy: 0.9917 - out_a_accuracy: 0.9902 - val_loss: 0.1757 - val_out_v_loss: 0.0830 - val_out_a_loss: 0.0927 - val_out_v_accuracy: 0.9730 - val_out_a_accuracy: 0.9713 - lr: 2.5000e-04\n",
      "Epoch 50/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0387 - out_v_loss: 0.0203 - out_a_loss: 0.0183 - out_v_accuracy: 0.9930 - out_a_accuracy: 0.9934 - val_loss: 0.1762 - val_out_v_loss: 0.0825 - val_out_a_loss: 0.0937 - val_out_v_accuracy: 0.9740 - val_out_a_accuracy: 0.9713 - lr: 2.5000e-04\n",
      "Epoch 51/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0402 - out_v_loss: 0.0208 - out_a_loss: 0.0194 - out_v_accuracy: 0.9923 - out_a_accuracy: 0.9922 - val_loss: 0.1712 - val_out_v_loss: 0.0808 - val_out_a_loss: 0.0904 - val_out_v_accuracy: 0.9740 - val_out_a_accuracy: 0.9711 - lr: 2.5000e-04\n",
      "Epoch 52/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0393 - out_v_loss: 0.0186 - out_a_loss: 0.0207 - out_v_accuracy: 0.9923 - out_a_accuracy: 0.9918 - val_loss: 0.1727 - val_out_v_loss: 0.0796 - val_out_a_loss: 0.0930 - val_out_v_accuracy: 0.9730 - val_out_a_accuracy: 0.9727 - lr: 2.5000e-04\n",
      "Epoch 53/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0354 - out_v_loss: 0.0172 - out_a_loss: 0.0181 - out_v_accuracy: 0.9938 - out_a_accuracy: 0.9930 - val_loss: 0.1749 - val_out_v_loss: 0.0865 - val_out_a_loss: 0.0883 - val_out_v_accuracy: 0.9742 - val_out_a_accuracy: 0.9746 - lr: 2.5000e-04\n",
      "Epoch 54/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0347 - out_v_loss: 0.0178 - out_a_loss: 0.0169 - out_v_accuracy: 0.9935 - out_a_accuracy: 0.9932 - val_loss: 0.1815 - val_out_v_loss: 0.0884 - val_out_a_loss: 0.0931 - val_out_v_accuracy: 0.9732 - val_out_a_accuracy: 0.9725 - lr: 2.5000e-04\n",
      "Epoch 55/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0356 - out_v_loss: 0.0173 - out_a_loss: 0.0183 - out_v_accuracy: 0.9939 - out_a_accuracy: 0.9926 - val_loss: 0.1797 - val_out_v_loss: 0.0856 - val_out_a_loss: 0.0941 - val_out_v_accuracy: 0.9742 - val_out_a_accuracy: 0.9721 - lr: 2.5000e-04\n",
      "Epoch 56/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0337 - out_v_loss: 0.0176 - out_a_loss: 0.0161 - out_v_accuracy: 0.9933 - out_a_accuracy: 0.9935\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0336 - out_v_loss: 0.0175 - out_a_loss: 0.0160 - out_v_accuracy: 0.9933 - out_a_accuracy: 0.9935 - val_loss: 0.1832 - val_out_v_loss: 0.0841 - val_out_a_loss: 0.0991 - val_out_v_accuracy: 0.9734 - val_out_a_accuracy: 0.9713 - lr: 2.5000e-04\n",
      "Epoch 57/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0286 - out_v_loss: 0.0128 - out_a_loss: 0.0158 - out_v_accuracy: 0.9953 - out_a_accuracy: 0.9936 - val_loss: 0.1624 - val_out_v_loss: 0.0797 - val_out_a_loss: 0.0827 - val_out_v_accuracy: 0.9760 - val_out_a_accuracy: 0.9746 - lr: 1.2500e-04\n",
      "Epoch 58/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0279 - out_v_loss: 0.0129 - out_a_loss: 0.0150 - out_v_accuracy: 0.9950 - out_a_accuracy: 0.9943 - val_loss: 0.1642 - val_out_v_loss: 0.0787 - val_out_a_loss: 0.0856 - val_out_v_accuracy: 0.9781 - val_out_a_accuracy: 0.9754 - lr: 1.2500e-04\n",
      "Epoch 59/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0295 - out_v_loss: 0.0153 - out_a_loss: 0.0142 - out_v_accuracy: 0.9941 - out_a_accuracy: 0.9950 - val_loss: 0.1701 - val_out_v_loss: 0.0804 - val_out_a_loss: 0.0897 - val_out_v_accuracy: 0.9754 - val_out_a_accuracy: 0.9744 - lr: 1.2500e-04\n",
      "Epoch 60/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0266 - out_v_loss: 0.0136 - out_a_loss: 0.0130 - out_v_accuracy: 0.9950 - out_a_accuracy: 0.9951 - val_loss: 0.1674 - val_out_v_loss: 0.0789 - val_out_a_loss: 0.0885 - val_out_v_accuracy: 0.9768 - val_out_a_accuracy: 0.9734 - lr: 1.2500e-04\n",
      "Epoch 61/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0263 - out_v_loss: 0.0127 - out_a_loss: 0.0136 - out_v_accuracy: 0.9958 - out_a_accuracy: 0.9951 - val_loss: 0.1650 - val_out_v_loss: 0.0792 - val_out_a_loss: 0.0859 - val_out_v_accuracy: 0.9756 - val_out_a_accuracy: 0.9746 - lr: 1.2500e-04\n",
      "Epoch 62/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0215 - out_v_loss: 0.0115 - out_a_loss: 0.0100 - out_v_accuracy: 0.9955 - out_a_accuracy: 0.9967\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0214 - out_v_loss: 0.0114 - out_a_loss: 0.0100 - out_v_accuracy: 0.9956 - out_a_accuracy: 0.9967 - val_loss: 0.1660 - val_out_v_loss: 0.0794 - val_out_a_loss: 0.0866 - val_out_v_accuracy: 0.9768 - val_out_a_accuracy: 0.9754 - lr: 1.2500e-04\n",
      "Epoch 63/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0220 - out_v_loss: 0.0110 - out_a_loss: 0.0110 - out_v_accuracy: 0.9962 - out_a_accuracy: 0.9958 - val_loss: 0.1635 - val_out_v_loss: 0.0780 - val_out_a_loss: 0.0855 - val_out_v_accuracy: 0.9779 - val_out_a_accuracy: 0.9760 - lr: 6.2500e-05\n",
      "Epoch 64/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0216 - out_v_loss: 0.0097 - out_a_loss: 0.0119 - out_v_accuracy: 0.9961 - out_a_accuracy: 0.9959 - val_loss: 0.1627 - val_out_v_loss: 0.0805 - val_out_a_loss: 0.0822 - val_out_v_accuracy: 0.9773 - val_out_a_accuracy: 0.9768 - lr: 6.2500e-05\n",
      "Epoch 65/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0232 - out_v_loss: 0.0117 - out_a_loss: 0.0115 - out_v_accuracy: 0.9954 - out_a_accuracy: 0.9956 - val_loss: 0.1705 - val_out_v_loss: 0.0816 - val_out_a_loss: 0.0889 - val_out_v_accuracy: 0.9775 - val_out_a_accuracy: 0.9746 - lr: 6.2500e-05\n",
      "Epoch 66/200\n",
      "320/320 [==============================] - 11s 35ms/step - loss: 0.0220 - out_v_loss: 0.0105 - out_a_loss: 0.0115 - out_v_accuracy: 0.9956 - out_a_accuracy: 0.9964 - val_loss: 0.1621 - val_out_v_loss: 0.0750 - val_out_a_loss: 0.0871 - val_out_v_accuracy: 0.9777 - val_out_a_accuracy: 0.9758 - lr: 6.2500e-05\n",
      "Epoch 67/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0228 - out_v_loss: 0.0117 - out_a_loss: 0.0110 - out_v_accuracy: 0.9955 - out_a_accuracy: 0.9958 - val_loss: 0.1635 - val_out_v_loss: 0.0767 - val_out_a_loss: 0.0868 - val_out_v_accuracy: 0.9775 - val_out_a_accuracy: 0.9770 - lr: 6.2500e-05\n",
      "Epoch 68/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0189 - out_v_loss: 0.0100 - out_a_loss: 0.0089 - out_v_accuracy: 0.9959 - out_a_accuracy: 0.9964 - val_loss: 0.1621 - val_out_v_loss: 0.0749 - val_out_a_loss: 0.0872 - val_out_v_accuracy: 0.9777 - val_out_a_accuracy: 0.9775 - lr: 6.2500e-05\n",
      "Epoch 69/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0222 - out_v_loss: 0.0115 - out_a_loss: 0.0107 - out_v_accuracy: 0.9957 - out_a_accuracy: 0.9961 - val_loss: 0.1639 - val_out_v_loss: 0.0781 - val_out_a_loss: 0.0858 - val_out_v_accuracy: 0.9770 - val_out_a_accuracy: 0.9785 - lr: 6.2500e-05\n",
      "Epoch 70/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0205 - out_v_loss: 0.0111 - out_a_loss: 0.0094 - out_v_accuracy: 0.9959 - out_a_accuracy: 0.9968 - val_loss: 0.1674 - val_out_v_loss: 0.0786 - val_out_a_loss: 0.0887 - val_out_v_accuracy: 0.9770 - val_out_a_accuracy: 0.9766 - lr: 6.2500e-05\n",
      "Epoch 71/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0194 - out_v_loss: 0.0098 - out_a_loss: 0.0096 - out_v_accuracy: 0.9962 - out_a_accuracy: 0.9969\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0194 - out_v_loss: 0.0098 - out_a_loss: 0.0096 - out_v_accuracy: 0.9962 - out_a_accuracy: 0.9968 - val_loss: 0.1707 - val_out_v_loss: 0.0808 - val_out_a_loss: 0.0900 - val_out_v_accuracy: 0.9773 - val_out_a_accuracy: 0.9770 - lr: 6.2500e-05\n",
      "Epoch 72/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0199 - out_v_loss: 0.0107 - out_a_loss: 0.0092 - out_v_accuracy: 0.9958 - out_a_accuracy: 0.9972 - val_loss: 0.1640 - val_out_v_loss: 0.0770 - val_out_a_loss: 0.0870 - val_out_v_accuracy: 0.9791 - val_out_a_accuracy: 0.9768 - lr: 3.1250e-05\n",
      "Epoch 73/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0207 - out_v_loss: 0.0106 - out_a_loss: 0.0101 - out_v_accuracy: 0.9961 - out_a_accuracy: 0.9961 - val_loss: 0.1683 - val_out_v_loss: 0.0814 - val_out_a_loss: 0.0869 - val_out_v_accuracy: 0.9768 - val_out_a_accuracy: 0.9762 - lr: 3.1250e-05\n",
      "Epoch 74/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0204 - out_v_loss: 0.0091 - out_a_loss: 0.0112 - out_v_accuracy: 0.9966 - out_a_accuracy: 0.9958 - val_loss: 0.1660 - val_out_v_loss: 0.0781 - val_out_a_loss: 0.0879 - val_out_v_accuracy: 0.9777 - val_out_a_accuracy: 0.9777 - lr: 3.1250e-05\n",
      "Epoch 75/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0181 - out_v_loss: 0.0092 - out_a_loss: 0.0089 - out_v_accuracy: 0.9969 - out_a_accuracy: 0.9971 - val_loss: 0.1664 - val_out_v_loss: 0.0785 - val_out_a_loss: 0.0879 - val_out_v_accuracy: 0.9779 - val_out_a_accuracy: 0.9771 - lr: 3.1250e-05\n",
      "Epoch 76/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0186 - out_v_loss: 0.0089 - out_a_loss: 0.0097 - out_v_accuracy: 0.9968 - out_a_accuracy: 0.9966\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0187 - out_v_loss: 0.0090 - out_a_loss: 0.0097 - out_v_accuracy: 0.9967 - out_a_accuracy: 0.9966 - val_loss: 0.1660 - val_out_v_loss: 0.0785 - val_out_a_loss: 0.0875 - val_out_v_accuracy: 0.9789 - val_out_a_accuracy: 0.9783 - lr: 3.1250e-05\n",
      "Epoch 77/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0189 - out_v_loss: 0.0082 - out_a_loss: 0.0107 - out_v_accuracy: 0.9970 - out_a_accuracy: 0.9959 - val_loss: 0.1658 - val_out_v_loss: 0.0782 - val_out_a_loss: 0.0876 - val_out_v_accuracy: 0.9787 - val_out_a_accuracy: 0.9783 - lr: 1.5625e-05\n",
      "Epoch 78/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0168 - out_v_loss: 0.0092 - out_a_loss: 0.0076 - out_v_accuracy: 0.9969 - out_a_accuracy: 0.9971 - val_loss: 0.1649 - val_out_v_loss: 0.0771 - val_out_a_loss: 0.0878 - val_out_v_accuracy: 0.9777 - val_out_a_accuracy: 0.9773 - lr: 1.5625e-05\n",
      "Epoch 79/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0184 - out_v_loss: 0.0097 - out_a_loss: 0.0087 - out_v_accuracy: 0.9964 - out_a_accuracy: 0.9969 - val_loss: 0.1670 - val_out_v_loss: 0.0777 - val_out_a_loss: 0.0892 - val_out_v_accuracy: 0.9777 - val_out_a_accuracy: 0.9773 - lr: 1.5625e-05\n",
      "Epoch 80/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0169 - out_v_loss: 0.0084 - out_a_loss: 0.0085 - out_v_accuracy: 0.9965 - out_a_accuracy: 0.9967 - val_loss: 0.1664 - val_out_v_loss: 0.0791 - val_out_a_loss: 0.0873 - val_out_v_accuracy: 0.9781 - val_out_a_accuracy: 0.9779 - lr: 1.5625e-05\n",
      "Epoch 81/200\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.0179 - out_v_loss: 0.0088 - out_a_loss: 0.0091 - out_v_accuracy: 0.9969 - out_a_accuracy: 0.9969\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0179 - out_v_loss: 0.0088 - out_a_loss: 0.0091 - out_v_accuracy: 0.9969 - out_a_accuracy: 0.9968 - val_loss: 0.1648 - val_out_v_loss: 0.0780 - val_out_a_loss: 0.0869 - val_out_v_accuracy: 0.9785 - val_out_a_accuracy: 0.9789 - lr: 1.5625e-05\n",
      "Epoch 82/200\n",
      "320/320 [==============================] - 11s 33ms/step - loss: 0.0169 - out_v_loss: 0.0085 - out_a_loss: 0.0084 - out_v_accuracy: 0.9970 - out_a_accuracy: 0.9972 - val_loss: 0.1653 - val_out_v_loss: 0.0774 - val_out_a_loss: 0.0879 - val_out_v_accuracy: 0.9785 - val_out_a_accuracy: 0.9768 - lr: 7.8125e-06\n",
      "Epoch 00082: early stopping\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.1653 - out_v_loss: 0.0774 - out_a_loss: 0.0879 - out_v_accuracy: 0.9785 - out_a_accuracy: 0.9768\n",
      "\n",
      "processing:  01 ......\n",
      "Before fine-tuning: [('loss', 0.006677), ('valence loss', 0.006139), ('arousal loss', 0.000538), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.005857), ('valence loss', 0.004759), ('arousal loss', 0.001099), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "Epoch 00028: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.002997), ('valence loss', 0.002997), ('arousal loss', 0.005375), ('valence acc', 1.0), ('arousal acc', 0.993631)]\n",
      "Epoch 00032: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.000538), ('valence loss', 0.006139), ('arousal loss', 0.000538), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  02 ......\n",
      "Before fine-tuning: [('loss', 0.393862), ('valence loss', 0.125302), ('arousal loss', 0.26856), ('valence acc', 0.964706), ('arousal acc', 0.917647)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.230536), ('valence loss', 0.09782), ('arousal loss', 0.132717), ('valence acc', 0.976471), ('arousal acc', 0.947059)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.076763), ('valence loss', 0.076763), ('arousal loss', 0.183101), ('valence acc', 0.988235), ('arousal acc', 0.935294)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.123335), ('valence loss', 0.132049), ('arousal loss', 0.123335), ('valence acc', 0.976471), ('arousal acc', 0.947059)]\n",
      "\n",
      "processing:  03 ......\n",
      "Before fine-tuning: [('loss', 0.029738), ('valence loss', 0.029611), ('arousal loss', 0.000128), ('valence acc', 0.987179), ('arousal acc', 1.0)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.029738), ('valence loss', 0.029611), ('arousal loss', 0.000128), ('valence acc', 0.987179), ('arousal acc', 1.0)]\n",
      "Epoch 00075: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.019649), ('valence loss', 0.019649), ('arousal loss', 0.32156), ('valence acc', 0.99359), ('arousal acc', 0.897436)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.000128), ('valence loss', 0.029611), ('arousal loss', 0.000128), ('valence acc', 0.987179), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  04 ......\n",
      "Before fine-tuning: [('loss', 0.358054), ('valence loss', 0.14942), ('arousal loss', 0.208634), ('valence acc', 0.975), ('arousal acc', 0.95)]\n",
      "Epoch 00045: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.358054), ('valence loss', 0.14942), ('arousal loss', 0.208634), ('valence acc', 0.975), ('arousal acc', 0.95)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.14942), ('valence loss', 0.14942), ('arousal loss', 0.208634), ('valence acc', 0.975), ('arousal acc', 0.95)]\n",
      "Epoch 00053: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.208634), ('valence loss', 0.14942), ('arousal loss', 0.208634), ('valence acc', 0.975), ('arousal acc', 0.95)]\n",
      "\n",
      "processing:  05 ......\n",
      "Before fine-tuning: [('loss', 0.101507), ('valence loss', 0.085142), ('arousal loss', 0.016364), ('valence acc', 0.974359), ('arousal acc', 0.99359)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.09419), ('valence loss', 0.069392), ('arousal loss', 0.024799), ('valence acc', 0.974359), ('arousal acc', 0.99359)]\n",
      "Epoch 00021: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.055267), ('valence loss', 0.055267), ('arousal loss', 0.040696), ('valence acc', 0.987179), ('arousal acc', 0.987179)]\n",
      "Epoch 00036: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.010543), ('valence loss', 0.07245), ('arousal loss', 0.010543), ('valence acc', 0.974359), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  06 ......\n",
      "Before fine-tuning: [('loss', 0.06154), ('valence loss', 0.007641), ('arousal loss', 0.053899), ('valence acc', 0.994253), ('arousal acc', 0.977012)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.06154), ('valence loss', 0.007641), ('arousal loss', 0.053899), ('valence acc', 0.994253), ('arousal acc', 0.977012)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.007641), ('valence loss', 0.007641), ('arousal loss', 0.053899), ('valence acc', 0.994253), ('arousal acc', 0.977012)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.053899), ('valence loss', 0.007641), ('arousal loss', 0.053899), ('valence acc', 0.994253), ('arousal acc', 0.977012)]\n",
      "\n",
      "processing:  07 ......\n",
      "Before fine-tuning: [('loss', 0.039027), ('valence loss', 0.015585), ('arousal loss', 0.023442), ('valence acc', 0.994118), ('arousal acc', 0.988235)]\n",
      "Epoch 00069: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.016352), ('valence loss', 0.001187), ('arousal loss', 0.015165), ('valence acc', 1.0), ('arousal acc', 0.994118)]\n",
      "Epoch 00044: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.002047), ('valence loss', 0.002047), ('arousal loss', 0.013697), ('valence acc', 1.0), ('arousal acc', 0.994118)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.010267), ('valence loss', 0.001854), ('arousal loss', 0.010267), ('valence acc', 1.0), ('arousal acc', 0.994118)]\n",
      "\n",
      "processing:  08 ......\n",
      "Before fine-tuning: [('loss', 0.087379), ('valence loss', 0.023318), ('arousal loss', 0.064061), ('valence acc', 0.987578), ('arousal acc', 0.968944)]\n",
      "Epoch 00040: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.087379), ('valence loss', 0.023318), ('arousal loss', 0.064061), ('valence acc', 0.987578), ('arousal acc', 0.968944)]\n",
      "Epoch 00034: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.023318), ('valence loss', 0.023318), ('arousal loss', 0.064061), ('valence acc', 0.987578), ('arousal acc', 0.968944)]\n",
      "Epoch 00045: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.060899), ('valence loss', 0.029295), ('arousal loss', 0.060899), ('valence acc', 0.981366), ('arousal acc', 0.975155)]\n",
      "\n",
      "processing:  09 ......\n",
      "Before fine-tuning: [('loss', 0.287264), ('valence loss', 0.034081), ('arousal loss', 0.253183), ('valence acc', 0.993939), ('arousal acc', 0.933333)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.262087), ('valence loss', 0.028954), ('arousal loss', 0.233133), ('valence acc', 0.975758), ('arousal acc', 0.933333)]\n",
      "Epoch 00035: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.016985), ('valence loss', 0.016985), ('arousal loss', 0.26249), ('valence acc', 0.993939), ('arousal acc', 0.921212)]\n",
      "Epoch 00019: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.240836), ('valence loss', 0.036932), ('arousal loss', 0.240836), ('valence acc', 0.981818), ('arousal acc', 0.933333)]\n",
      "\n",
      "processing:  10 ......\n",
      "Before fine-tuning: [('loss', 0.037695), ('valence loss', 0.017174), ('arousal loss', 0.020521), ('valence acc', 0.993464), ('arousal acc', 0.993464)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.037695), ('valence loss', 0.017174), ('arousal loss', 0.020521), ('valence acc', 0.993464), ('arousal acc', 0.993464)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.017174), ('valence loss', 0.017174), ('arousal loss', 0.020521), ('valence acc', 0.993464), ('arousal acc', 0.993464)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.008844), ('valence loss', 0.02143), ('arousal loss', 0.008844), ('valence acc', 0.993464), ('arousal acc', 0.993464)]\n",
      "\n",
      "processing:  11 ......\n",
      "Before fine-tuning: [('loss', 0.28779), ('valence loss', 0.090442), ('arousal loss', 0.197348), ('valence acc', 0.964912), ('arousal acc', 0.953216)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.266942), ('valence loss', 0.089404), ('arousal loss', 0.177539), ('valence acc', 0.976608), ('arousal acc', 0.959064)]\n",
      "Epoch 00020: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.074501), ('valence loss', 0.074501), ('arousal loss', 0.253652), ('valence acc', 0.964912), ('arousal acc', 0.947368)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.161881), ('valence loss', 0.084139), ('arousal loss', 0.161881), ('valence acc', 0.97076), ('arousal acc', 0.959064)]\n",
      "\n",
      "processing:  12 ......\n",
      "Before fine-tuning: [('loss', 0.147113), ('valence loss', 0.095869), ('arousal loss', 0.051244), ('valence acc', 0.97351), ('arousal acc', 0.993378)]\n",
      "Epoch 00045: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.147113), ('valence loss', 0.095869), ('arousal loss', 0.051244), ('valence acc', 0.97351), ('arousal acc', 0.993378)]\n",
      "Epoch 00028: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.083694), ('valence loss', 0.083694), ('arousal loss', 0.158127), ('valence acc', 0.980132), ('arousal acc', 0.97351)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.051244), ('valence loss', 0.095869), ('arousal loss', 0.051244), ('valence acc', 0.97351), ('arousal acc', 0.993378)]\n",
      "\n",
      "processing:  13 ......\n",
      "Before fine-tuning: [('loss', 0.110986), ('valence loss', 0.105248), ('arousal loss', 0.005738), ('valence acc', 0.964789), ('arousal acc', 1.0)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.110986), ('valence loss', 0.105248), ('arousal loss', 0.005738), ('valence acc', 0.964789), ('arousal acc', 1.0)]\n",
      "Epoch 00038: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.085367), ('valence loss', 0.085367), ('arousal loss', 0.193228), ('valence acc', 0.978873), ('arousal acc', 0.950704)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.005738), ('valence loss', 0.105248), ('arousal loss', 0.005738), ('valence acc', 0.964789), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  14 ......\n",
      "Before fine-tuning: [('loss', 0.038396), ('valence loss', 0.017502), ('arousal loss', 0.020894), ('valence acc', 0.993976), ('arousal acc', 0.993976)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.037225), ('valence loss', 0.010667), ('arousal loss', 0.026558), ('valence acc', 1.0), ('arousal acc', 0.981928)]\n",
      "Epoch 00066: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.010714), ('valence loss', 0.010714), ('arousal loss', 0.049475), ('valence acc', 1.0), ('arousal acc', 0.975904)]\n",
      "Epoch 00044: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.020894), ('valence loss', 0.017502), ('arousal loss', 0.020894), ('valence acc', 0.993976), ('arousal acc', 0.993976)]\n",
      "\n",
      "processing:  15 ......\n",
      "Before fine-tuning: [('loss', 0.133755), ('valence loss', 0.07276), ('arousal loss', 0.060994), ('valence acc', 0.970588), ('arousal acc', 0.988235)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.129901), ('valence loss', 0.068343), ('arousal loss', 0.061558), ('valence acc', 0.970588), ('arousal acc', 0.988235)]\n",
      "Epoch 00034: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.062069), ('valence loss', 0.062069), ('arousal loss', 0.072599), ('valence acc', 0.976471), ('arousal acc', 0.976471)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.060371), ('valence loss', 0.065855), ('arousal loss', 0.060371), ('valence acc', 0.970588), ('arousal acc', 0.988235)]\n",
      "\n",
      "processing:  16 ......\n",
      "Before fine-tuning: [('loss', 0.150588), ('valence loss', 0.103411), ('arousal loss', 0.047176), ('valence acc', 0.976744), ('arousal acc', 0.994186)]\n",
      "Epoch 00028: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.131037), ('valence loss', 0.099663), ('arousal loss', 0.031374), ('valence acc', 0.97093), ('arousal acc', 0.994186)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.102045), ('valence loss', 0.102045), ('arousal loss', 0.038605), ('valence acc', 0.976744), ('arousal acc', 0.994186)]\n",
      "Epoch 00033: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.041153), ('valence loss', 0.139056), ('arousal loss', 0.041153), ('valence acc', 0.976744), ('arousal acc', 0.994186)]\n",
      "\n",
      "processing:  17 ......\n",
      "Before fine-tuning: [('loss', 0.395423), ('valence loss', 0.155018), ('arousal loss', 0.240405), ('valence acc', 0.960526), ('arousal acc', 0.927632)]\n",
      "Epoch 00020: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.381127), ('valence loss', 0.164754), ('arousal loss', 0.216373), ('valence acc', 0.953947), ('arousal acc', 0.934211)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.1515), ('valence loss', 0.1515), ('arousal loss', 0.224089), ('valence acc', 0.953947), ('arousal acc', 0.940789)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.227498), ('valence loss', 0.178459), ('arousal loss', 0.227498), ('valence acc', 0.953947), ('arousal acc', 0.940789)]\n",
      "\n",
      "processing:  18 ......\n",
      "Before fine-tuning: [('loss', 0.105957), ('valence loss', 0.102426), ('arousal loss', 0.00353), ('valence acc', 0.976608), ('arousal acc', 1.0)]\n",
      "Epoch 00057: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.082427), ('valence loss', 0.078631), ('arousal loss', 0.003797), ('valence acc', 0.982456), ('arousal acc', 1.0)]\n",
      "Epoch 00026: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.096636), ('valence loss', 0.096636), ('arousal loss', 0.006421), ('valence acc', 0.97076), ('arousal acc', 1.0)]\n",
      "Epoch 00086: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.001514), ('valence loss', 0.090771), ('arousal loss', 0.001514), ('valence acc', 0.976608), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  19 ......\n",
      "Before fine-tuning: [('loss', 0.015787), ('valence loss', 0.00869), ('arousal loss', 0.007097), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.015787), ('valence loss', 0.00869), ('arousal loss', 0.007097), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "Epoch 00034: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.001527), ('valence loss', 0.001527), ('arousal loss', 0.03359), ('valence acc', 1.0), ('arousal acc', 0.98)]\n",
      "Epoch 00058: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.006805), ('valence loss', 0.009394), ('arousal loss', 0.006805), ('valence acc', 0.993333), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  20 ......\n",
      "Before fine-tuning: [('loss', 0.131738), ('valence loss', 0.087004), ('arousal loss', 0.044734), ('valence acc', 0.987421), ('arousal acc', 0.981132)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.131738), ('valence loss', 0.087004), ('arousal loss', 0.044734), ('valence acc', 0.987421), ('arousal acc', 0.981132)]\n",
      "Epoch 00041: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.04621), ('valence loss', 0.04621), ('arousal loss', 0.182245), ('valence acc', 0.987421), ('arousal acc', 0.949686)]\n",
      "Epoch 00050: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.044734), ('valence loss', 0.087004), ('arousal loss', 0.044734), ('valence acc', 0.987421), ('arousal acc', 0.981132)]\n",
      "\n",
      "processing:  21 ......\n",
      "Before fine-tuning: [('loss', 0.033301), ('valence loss', 0.012675), ('arousal loss', 0.020626), ('valence acc', 0.99375), ('arousal acc', 0.99375)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.033301), ('valence loss', 0.012675), ('arousal loss', 0.020626), ('valence acc', 0.99375), ('arousal acc', 0.99375)]\n",
      "Epoch 00021: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.010902), ('valence loss', 0.010902), ('arousal loss', 0.17547), ('valence acc', 1.0), ('arousal acc', 0.9375)]\n",
      "Epoch 00029: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.020626), ('valence loss', 0.012675), ('arousal loss', 0.020626), ('valence acc', 0.99375), ('arousal acc', 0.99375)]\n",
      "\n",
      "processing:  22 ......\n",
      "Before fine-tuning: [('loss', 0.660316), ('valence loss', 0.279656), ('arousal loss', 0.38066), ('valence acc', 0.895425), ('arousal acc', 0.882353)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.658694), ('valence loss', 0.247858), ('arousal loss', 0.410837), ('valence acc', 0.934641), ('arousal acc', 0.875817)]\n",
      "Epoch 00028: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.251056), ('valence loss', 0.251056), ('arousal loss', 0.414855), ('valence acc', 0.915033), ('arousal acc', 0.869281)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.38066), ('valence loss', 0.279656), ('arousal loss', 0.38066), ('valence acc', 0.895425), ('arousal acc', 0.882353)]\n",
      "\n",
      "processing:  23 ......\n",
      "Before fine-tuning: [('loss', 0.02926), ('valence loss', 0.010899), ('arousal loss', 0.018362), ('valence acc', 0.993671), ('arousal acc', 0.993671)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.02926), ('valence loss', 0.010899), ('arousal loss', 0.018362), ('valence acc', 0.993671), ('arousal acc', 0.993671)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.010899), ('valence loss', 0.010899), ('arousal loss', 0.018362), ('valence acc', 0.993671), ('arousal acc', 0.993671)]\n",
      "Epoch 00052: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.018362), ('valence loss', 0.010899), ('arousal loss', 0.018362), ('valence acc', 0.993671), ('arousal acc', 0.993671)]\n",
      "\n",
      "processing:  24 ......\n",
      "Before fine-tuning: [('loss', 0.118517), ('valence loss', 0.078518), ('arousal loss', 0.039999), ('valence acc', 0.958084), ('arousal acc', 0.976048)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.118517), ('valence loss', 0.078518), ('arousal loss', 0.039999), ('valence acc', 0.958084), ('arousal acc', 0.976048)]\n",
      "Epoch 00018: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.070168), ('valence loss', 0.070168), ('arousal loss', 0.107415), ('valence acc', 0.97006), ('arousal acc', 0.964072)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.039999), ('valence loss', 0.078518), ('arousal loss', 0.039999), ('valence acc', 0.958084), ('arousal acc', 0.976048)]\n",
      "\n",
      "processing:  25 ......\n",
      "Before fine-tuning: [('loss', 0.013363), ('valence loss', 0.013203), ('arousal loss', 0.000159), ('valence acc', 0.993421), ('arousal acc', 1.0)]\n",
      "Epoch 00025: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.006451), ('valence loss', 0.005117), ('arousal loss', 0.001335), ('valence acc', 1.0), ('arousal acc', 1.0)]\n",
      "Epoch 00064: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.00287), ('valence loss', 0.00287), ('arousal loss', 0.008626), ('valence acc', 1.0), ('arousal acc', 0.993421)]\n",
      "Epoch 00036: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.000159), ('valence loss', 0.013203), ('arousal loss', 0.000159), ('valence acc', 0.993421), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  26 ......\n",
      "Before fine-tuning: [('loss', 0.582525), ('valence loss', 0.335742), ('arousal loss', 0.246783), ('valence acc', 0.928994), ('arousal acc', 0.940828)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.571437), ('valence loss', 0.271821), ('arousal loss', 0.299616), ('valence acc', 0.940828), ('arousal acc', 0.940828)]\n",
      "Epoch 00083: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.244452), ('valence loss', 0.244452), ('arousal loss', 0.421596), ('valence acc', 0.934911), ('arousal acc', 0.91716)]\n",
      "Epoch 00043: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.246783), ('valence loss', 0.335742), ('arousal loss', 0.246783), ('valence acc', 0.928994), ('arousal acc', 0.940828)]\n",
      "\n",
      "processing:  27 ......\n",
      "Before fine-tuning: [('loss', 0.022295), ('valence loss', 0.004766), ('arousal loss', 0.017529), ('valence acc', 1.0), ('arousal acc', 0.994012)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.022295), ('valence loss', 0.004766), ('arousal loss', 0.017529), ('valence acc', 1.0), ('arousal acc', 0.994012)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.004766), ('valence loss', 0.004766), ('arousal loss', 0.017529), ('valence acc', 1.0), ('arousal acc', 0.994012)]\n",
      "Epoch 00028: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.014006), ('valence loss', 0.108314), ('arousal loss', 0.014006), ('valence acc', 0.952096), ('arousal acc', 0.994012)]\n",
      "\n",
      "processing:  28 ......\n",
      "Before fine-tuning: [('loss', 0.236634), ('valence loss', 0.136574), ('arousal loss', 0.100059), ('valence acc', 0.97037), ('arousal acc', 0.977778)]\n",
      "Epoch 00020: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.195836), ('valence loss', 0.128114), ('arousal loss', 0.067722), ('valence acc', 0.962963), ('arousal acc', 0.97037)]\n",
      "Epoch 00102: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.094955), ('valence loss', 0.094955), ('arousal loss', 0.11552), ('valence acc', 0.977778), ('arousal acc', 0.940741)]\n",
      "Epoch 00021: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.09379), ('valence loss', 0.156198), ('arousal loss', 0.09379), ('valence acc', 0.948148), ('arousal acc', 0.962963)]\n",
      "\n",
      "processing:  29 ......\n",
      "Before fine-tuning: [('loss', 0.317725), ('valence loss', 0.065558), ('arousal loss', 0.252167), ('valence acc', 0.962963), ('arousal acc', 0.95679)]\n",
      "Epoch 00028: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.312014), ('valence loss', 0.058323), ('arousal loss', 0.253692), ('valence acc', 0.969136), ('arousal acc', 0.95679)]\n",
      "Epoch 00031: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.046256), ('valence loss', 0.046256), ('arousal loss', 0.256913), ('valence acc', 0.969136), ('arousal acc', 0.95679)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.21204), ('valence loss', 0.060123), ('arousal loss', 0.21204), ('valence acc', 0.969136), ('arousal acc', 0.962963)]\n",
      "\n",
      "processing:  30 ......\n",
      "Before fine-tuning: [('loss', 0.031429), ('valence loss', 0.025041), ('arousal loss', 0.006388), ('valence acc', 0.993548), ('arousal acc', 1.0)]\n",
      "Epoch 00023: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.028994), ('valence loss', 0.01941), ('arousal loss', 0.009584), ('valence acc', 0.987097), ('arousal acc', 1.0)]\n",
      "Epoch 00034: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.022481), ('valence loss', 0.022481), ('arousal loss', 0.00959), ('valence acc', 0.987097), ('arousal acc', 1.0)]\n",
      "Epoch 00037: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.006388), ('valence loss', 0.025041), ('arousal loss', 0.006388), ('valence acc', 0.993548), ('arousal acc', 1.0)]\n",
      "\n",
      "processing:  31 ......\n",
      "Before fine-tuning: [('loss', 0.100682), ('valence loss', 0.066334), ('arousal loss', 0.034348), ('valence acc', 0.974026), ('arousal acc', 0.987013)]\n",
      "Epoch 00020: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.100682), ('valence loss', 0.066334), ('arousal loss', 0.034348), ('valence acc', 0.974026), ('arousal acc', 0.987013)]\n",
      "Epoch 00024: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.066334), ('valence loss', 0.066334), ('arousal loss', 0.034348), ('valence acc', 0.974026), ('arousal acc', 0.987013)]\n",
      "Epoch 00039: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.021024), ('valence loss', 0.114256), ('arousal loss', 0.021024), ('valence acc', 0.961039), ('arousal acc', 0.993506)]\n",
      "\n",
      "processing:  32 ......\n",
      "Before fine-tuning: [('loss', 0.10563), ('valence loss', 0.045231), ('arousal loss', 0.060399), ('valence acc', 0.987654), ('arousal acc', 0.975309)]\n",
      "Epoch 00017: early stopping\n",
      "After fine-tuning on Multi-Task [('loss', 0.087475), ('valence loss', 0.03773), ('arousal loss', 0.049746), ('valence acc', 0.987654), ('arousal acc', 0.981481)]\n",
      "Epoch 00045: early stopping\n",
      "After fine-tuning on VALENCE [('loss', 0.024019), ('valence loss', 0.024019), ('arousal loss', 0.073443), ('valence acc', 0.993827), ('arousal acc', 0.969136)]\n",
      "Epoch 00027: early stopping\n",
      "After fine-tuning on AROUSAL [('loss', 0.053904), ('valence loss', 0.039381), ('arousal loss', 0.053904), ('valence acc', 0.975309), ('arousal acc', 0.975309)]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Show results"
   ],
   "metadata": {
    "id": "KQ6cKxptROlC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if fine_tuning:\n",
    "    scores_dict =\\\n",
    "        {\n",
    "         'before_fine_tuning':     f'{metrics_dir}/{model_name}_scores_SD_before.pkl',\n",
    "         'valence_task_fine_tuning': f'{metrics_dir}/{model_name}_valence_scores_SD.pkl',\n",
    "         'arousal_fine_tuning':    f'{metrics_dir}/{model_name}_arousal_scores_SD.pkl',\n",
    "         'multi_fine_tuning':    f'{metrics_dir}/{model_name}_multi_scores_SD.pkl'\n",
    "        } # dict with paths of scores files\n",
    "    \n",
    "    print_results(scores_dict, fine_tuning)\n",
    "else:\n",
    "    scores_dict = {'results': f'{metrics_dir}/{model_name}_scores_SI.pkl'} # dict with path of scores file\n",
    "    print_results(scores_dict, fine_tuning)"
   ],
   "metadata": {
    "id": "cgniEqIEVPYj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3156bbad-c1f1-43fa-8824-fb33f45860b9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "before_fine_tuning\n",
      "\tArousal\n",
      "\t\tmean: 97.5985 %\n",
      "\t\tstd: 2.8609 %\n",
      "\tValence\n",
      "\t\tmean: 97.7674 %\n",
      "\t\tstd: 2.1577 %\n",
      "\n",
      "valence_task_fine_tuning\n",
      "\tArousal\n",
      "\t\tmean: 96.3428 %\n",
      "\t\tstd: 3.1456 %\n",
      "\tValence\n",
      "\t\tmean: 98.1814 %\n",
      "\t\tstd: 1.9097 %\n",
      "\n",
      "arousal_fine_tuning\n",
      "\tArousal\n",
      "\t\tmean: 97.8010 %\n",
      "\t\tstd: 2.6641 %\n",
      "\tValence\n",
      "\t\tmean: 97.4632 %\n",
      "\t\tstd: 2.1817 %\n",
      "\n",
      "multi_fine_tuning\n",
      "\tArousal\n",
      "\t\tmean: 97.6857 %\n",
      "\t\tstd: 2.7381 %\n",
      "\tValence\n",
      "\t\tmean: 97.9568 %\n",
      "\t\tstd: 1.6930 %\n"
     ]
    }
   ]
  }
 ]
}